{"meta":{"title":"Jerry的小站","subtitle":"博客/工具分享","description":"一个集博客和工具咨询分享于一体的平台","author":"Jerry Gao","url":"http://jerryblogs.com","root":"/"},"pages":[{"title":"","date":"2023-04-20T16:31:23.664Z","updated":"2023-04-20T16:31:23.664Z","comments":true,"path":"about/index.html","permalink":"http://jerryblogs.com/about/index.html","excerpt":"","text":"Jerry Gao 98年生，是一名SRE（运维开发）工程师，曾就职于网易互娱 网易就职期间拿过运维质量优化奖，贡献过最佳实践文档 5+年Python经验，2+年Go经验，对Linux&#x2F;云原生领域有浓厚的兴趣 自学编程，大学时期不好好学习，比较喜欢折腾，玩儿过前端(Vue,React)&#x2F;后端(Python,Go)&#x2F;爬虫，拿过奖，卖过软件&#x2F;数据，甚至玩儿过React Native，感觉自己啥都会 大学时期开设币圈自媒体，撰写技术博客 大一开始学编程开始就用Github，上面是自己成长的印记 爱好 旅游，去和陌生人交流，去体验他们的人生，他们的生活 足球，2013年广州恒大夺冠开始热爱足球，足球是我的生命 做饭，把一堆活蹦乱跳的小鱼小虾都变成可口的红烧鱼，黄焖大虾，那是满满的成就感 民谣，一边听李志&#x2F;宋冬野&#x2F;赵雷，一边写代码 家乡现居深圳，家乡漳州。 漳州，位于中国福建省东南沿海，是一座历史悠久、文化灿烂的古城。漳州地处于山海交汇之间，山清水秀，自然风光美不胜收。 首先，漳州的自然景观令人陶醉。这里有蜿蜒曲折的河流，如林溪、诏安溪等，水清澈见底，悠然流淌。还有峡谷、瀑布、溶洞等奇峰异石，如仙都、雁洋、霞阳等，形态各异，美不胜收。尤其是世界文化自然双遗产——土楼，作为漳州独特的民居建筑，更是引人入胜。这些土楼如同宛如童话般的梦幻世界，有着千姿百态的外形，深受游客喜爱。 其次，漳州的气候四季如春，温暖湿润，充满了生机和活力。春天，山花烂漫，野草如茵，百花争艳；夏天，绿树成荫，溪水潺潺，清凉宜人；秋天，硕果累累，丹桂飘香，金黄一片；冬天，阳光温暖，气候宜人，是避寒胜地。这里的自然景色犹如一幅幅画卷，令人陶醉其中。 此外，漳州还有丰富的生物资源，拥有众多珍稀植物和动物。如南靖土楼周边的茶园、竹林等，以及漳浦国家级自然保护区和天台山国家级自然保护区，都是得天独厚的生态环境。这里的自然生态保护得当，生物多样性丰富，是一个理想的生态旅游胜地。 漳州是一个融合了自然、历史、文化的城市，其独特的自然风光和丰富的生物资源，使其成为一处旅游胜地，吸引了众多的游客前来观光、度假和探索。无论是欣赏山水之美，还是领略古城之韵，漳州都是一个让人陶醉的地方，仿佛是一幅神奇的画卷，让人流连忘返。 (其实以上来自ChatGpt)"},{"title":"Page","date":"2013-12-26T14:52:56.000Z","updated":"2023-04-19T19:09:41.010Z","comments":true,"path":"page/index.html","permalink":"http://jerryblogs.com/page/index.html","excerpt":"","text":"This is a page test."},{"title":"所有分类","date":"2023-04-20T15:13:33.291Z","updated":"2023-04-20T15:13:33.291Z","comments":true,"path":"categories/index.html","permalink":"http://jerryblogs.com/categories/index.html","excerpt":"","text":""},{"title":"所有标签","date":"2023-04-20T12:16:46.212Z","updated":"2023-04-20T12:16:46.212Z","comments":true,"path":"tag/index.html","permalink":"http://jerryblogs.com/tag/index.html","excerpt":"","text":""},{"title":"","date":"2023-04-20T12:28:29.841Z","updated":"2023-04-20T12:28:29.841Z","comments":true,"path":"404.html","permalink":"http://jerryblogs.com/404.html","excerpt":"","text":"404 很抱歉，您访问的页面不存在 可能是输入地址有误或该地址已被删除"}],"posts":[{"title":"用于弹性容器编排的Kubernetes中的水平Pod自动扩缩容","slug":"k8s-pod-scale","date":"2022-05-26T11:31:22.000Z","updated":"2023-04-20T13:39:35.334Z","comments":true,"path":"2022/05/26/k8s-pod-scale/","link":"","permalink":"http://jerryblogs.com/2022/05/26/k8s-pod-scale/","excerpt":"","text":"文章信息作者：Thanh-Tung Nguyen , Yu-Jin Yeom 1 , Taehong Kim 1,* , Dae-Heon Park 2,* and Sehan Kim原文：https://pdfs.semanticscholar.org/a6e5/53a4f59d737d802cebdce7e114c5d62f728c.pdf 摘要Kubernetes是一个开源的容器编排平台，通过自动扩缩容机制（水平、垂直和集群自动扩缩容）实现高可用性和可拓展性。其中HPA通过动态拓展和所见资源单元（Pod）的数量来帮助提供不间断的服务，而无需通过重新启动整个系统。Kubernetes监控默认的资源指标，包括主机及其Pod的CPU和内存使用情况。另一方面，有Prometheus等外部软件提供的自定义指标是可定制的，可以监控各种指标。在本文中，我们通过各种实验来研究HPA，以提供有关操作行为的关键知识。我们还讨论了Kubernetes资源指标（KRM）和Prometheus自定义指标（PCM）之间的本质区别，以及他们如何影响HPA的性能。最后，我们为将来使用Kubernetes的研究人员，开发人员和系统管理员提供了有关如何优化HPA性能的更深入的见解和经验教训。 前言近年来，随着云计算和后期边缘计算的快速出现，虚拟化技术已经成为学术研究和工业发展的轰动话题，因为他们使用Amazon Web Services、GCP、Microsoft Azure等云平台能实现大规模的弹性。新兴的虚拟化技术之一是容器化技术，其中配备随时可部署的应用程序组件的轻量级操作系统被打包到一个自给自足的容器中，准备在支持多租户的主机上运行。在主机系统中，不同的容器在同一主机操作系统和同一内核中一起运行，这有助于降低存储要求，并允许他们主机操作系统相比实现接近本机的性能。 由于容器可以大规模部署，因此对在部署、拓展和管理方面高度自动化的容器编排平台有着巨大的需求。在各种编排平台中，包括Docker Swarm、Amazon Elastic Container Service、Red Hat OpenShift Container Platform，Kubernetes已成为其受欢迎的事实上的标准。他是一个开源平台，可以轻松打包和运行容器化应用程序、工作负载和服务，并提供用于操作可拓展分部署系统的框架，此外，容器化应用程序具有在任何类型的操作系统和云基础架构上运行的可移植性。Kubernetes使用Docker作为基础环境来运行可以指且自给自足的容器，其实例化称为Docker镜像，它提供了一个控制平面，用于管理和安排这些容器在其主机集群（节点）上运行，具体取决于其可用资源和每个容器的特定要求。 在Kubernetes中，最重要的功能之一是自动伸缩，因为它允许容器化应用程序和服务在没有人干预的情况下弹性运行。Kubernetes提供了三种类型的自动伸缩器： HPA，根据各种要求调整执行和资源单元（Pod）的数量来支持高可用性。触发后HPA会出创建新的Pod来共享工作负载，而不会影响当前在集群内运行的现有Pod。 VPA，直接更改Pod的规格，例如请求的资源，并维护工作Pod的数量。因此，他需要重新启动这些Pod，从而中断应用程序和服务的连续性。 集群自动伸缩（CA），增加了不再能够在现有POD上调度POD的节点数。目前CA仅适用于GCP和AWS等商业云平台。 为了支持自动扩缩容，Kubernetes会监控Pod、应用程序、主机和集群统计信息（指标）。当这些指标达到特定阈值时，将自动触发扩缩容程序。虽然Kubernetes默认提供资源指标，但是他们的监控目标仅限于Pod和主机的CPU和内存的使用情况。因此，可以在外部软件的帮助下添加可定制（自定义）指标，提高HPA的性能和灵活性。在本文中，我们研究了Prometheus提供的自定义指标，Prometheus是由云原生计算基金会运行的开源项目。 已经有几项工作旨在提高Kubernetes自动扩缩容的性能。 例如，参考文献15，16中提出了改善CA和VPA的技术，参考文献17，18中则侧重于提高HPA和资源检测的性能。然而他们还没有深入基础知识。例如，需要解决诸如“Kubernetes HPA如何对不同的类型做出反应？”、“不同指标抓取周期对Kubernetes有何影响？”或“仅监视CPU和内存使用率是否满足HPA？”之类的问题。此外，有关Kubernetes及其自动扩缩器的文档可以在官方文档、网站和互联网的其他几个来源上找到。但是他们是从功能角度编写的，或者只是提供有关如何安装和运行Kubernetes的教程。对Kubernetes的运营行为缺乏全面的基本分析。因此，在本文中，我们专注于HPA，并寻求通过以下贡献来提高关于有关该主题的知识： 首先，我们通过测试平台上的各种实现，从拓展趋势，指标搜集，请求处理，集群大小，抓取时间和延迟等方面评估HPA。我们对结果的全面分析提供了官方网站和其他来源所没有的知识和见解。 其次，除了Kubernetes的默认资源指标外，我们还使用Prometheus Custom Metric评估HPA。通过了解这两种指标之间的差异，读者可以更牢固掌握HPA的运行行为。 最后，我们提供从实验和分析中获得的实践经验。它们可以作为基础知识，以便研究人员，开发人员和系统管理员可以做出明智的决策，以优化HPA的性能以及Kubernetes集群中的服务质量。 本文的其余部分组织如下。第 2 部分讨论了有关 Kubernetes 和自扩展研究的现有文献。第 3 部分分析了 Kubernetes 的体系结构，而第 4 部分则彻底讨论了 HPA、不同的指标以及 Reiness Probe 的使用，这有助于读者在进行性能分析之前更好地了解 Kubernetes 和 HPA。第5节从各种实验场景的结果中得出了有关HPA性能的经验教训。最后，第6节总结了本文。 相关工作Kubernetes最初由Google开发，后来转移到CNCF [14]，作为在云数据中心高效部署，管理和扩展容器化应用程序的解决方案。但是，由于它是一个开源项目，因此可以配置和修改Kubernetes作为坚实的基础，可以在其上构建和开发满足特定需求的其他平台。Reference [19] 的作者认为，当前版本的 Kubernetes 调度程序只考虑了可虚拟化的物理基础设施，包括 CPU 和内存使用率，这才有逻辑意义。但是，从公司的角度来看，为了提高数据中心的效率，还需要考虑其他条件，例如地理位置，电力基础设施和业务流程。因此，作者提出了一种名为Edgetic的增强调度程序，它可以在性能和功耗方面预测Pod的最佳位置。 在参考文献[15]中，瑟古德和列侬讨论了许多场景，包括智能家居环境，其中有大量的输入设备，而家庭成员等用户数量经常波动。这就意味着当所有现有节点都繁忙时，需要 HPA 和更高版本的 CA。然而，目前，默认的Kubernetes CA仅由GCP [2]等云平台提供商提供，因此他们为Kubernetes提出了一种称为自由&#x2F;开源软件（FOSS）的弹性CA解决方案。此解决方案采用 VMware ESXi 主机作为节点和虚拟机工具，包括用于 CA 操作的 vCenter、Foreman。具体而言，当任何节点达到 CPU 或内存阈值时，vCenter 服务器都会创建虚拟机警报，然后执行 bash 脚本，通过 Foreman 创建新的虚拟机节点。 在参考文献[16]中，作者提出了一种名为基于资源利用率的自动缩放系统（RUBAS）的无中断VPA解决方案，其中包含容器迁移。他们认为，资源可能被高估，导致利用率低下。因此，RUBAS 计算 VPA 的实际所需资源。此外，作者试图通过使用用户空间中的检查点还原（CRIU）创建检查点映像来解决必须在VPA中重新启动容器和容器的问题。Rossi [20]提出了一种用于水平和垂直自动缩放的强化学习模型。它旨在确保应用程序所需的响应时间。Reference [21]的作者开发了一种混合自适应自动缩放器Libra。它还考虑了采用传统HPA的应用的最佳资源分配。Libra本质上是VPA和HPA的控制回路。在第一阶段，Libra 使用金丝雀应用程序计算适当的 CPU 限制，并根据此新的 CPU 限制调整生产应用程序的 Pod 数量。之后，如果负载达到极限，则重复循环。 在参考文献[22，23]中，作者认为Kubernetes目前正在使用通过cAdvisor从&#x2F;cgroup虚拟文件系统收集的相对指标。这些指标可能与处理器中的实际 CPU 使用率不同，后者可以从 &#x2F;proc 文件系统中收集。这种差异可能导致低估所需资源。因此，作者提出了CPU密集型应用程序的相对指标和绝对指标之间的相关模型，用于纠正Kubernetes收集的相对指标，以提高HPA的性能。考虑到类似的目标，参考文献[24]的作者提出了几个影响因素，例如保守常数，它实际上为度量波动创造了一个缓冲区。仅当指标值超出此区域时，才会执行 HPA 操作。另一个因素是连续缩放操作之间的适应间隔。这样可以减少指标值波动时不必要的缩放。 在参考文献 [25，26] 中，作者将 Kubernetes 应用于容器化雾计算应用程序的资源配置中。提出一种网络感知调度算法，该算法考虑了节点的CPU和RAM容量、设备类型和地理位置等网络基础设施，以做出配置决策。例如，此算法在调度时间关键型应用程序的实例时可以考虑往返延迟。另一个基于 Kubernetes 的雾计算平台，用于管理地理上分散的容器，在参考文献 [27] 中提出。在本文中，作者设计了一种名为Autoscaling Broker（AS Broker）的服务，以获取原始指标并根据CPU和内存使用情况计算HPA的最佳副本数，同时减少应用程序的响应时间。在参考文献 [12] 中，Chang 等人介绍了一个基于 Kubernetes 的云监控平台，该平台提供基于资源利用率和应用程序 QoS 指标的动态资源配置算法。在此平台上，使用Heapster v.0.19.1 [28]，InfluxDB v.0.9.4.1 [29]和Grafana v.2.1.0 [30]收集并显示资源指标，而应用程序的响应时间则使用Apache JMeter计算[31]。此数据被聚合并输入到一个配置算法中，该算法实质上是计算和获取 Pod 的数量。Jin-Gang等人在参考文献中提出了一种用于统一通信服务器的预测HPA算法[18]。该算法的反应式扩展部分与 Kubernetes 的当前算法相同。另一方面，该算法还采用自动回归集成和移动平均（ARIMA）模型来预测未来的工作负载，或HTTP请求的数量[32]来触发HPA和升级。 Kubernetes HPA的另一个用例是在API网关系统上，如参考文献[17]中所述。该系统旨在简化与后端服务的内部连接。由于前端和后端服务的 Pod 在必要时都会进行水平自动缩放，因此它们的互连也会显著增加，从而导致对网关系统进行扩展的需求。在这项工作中，作者使用Prometheus自定义指标进行HPA操作。但是，没有提到哪些指标以及如何使用它们。以同样的方式，Dickel，Podolskiy和Gerndt [33]建议在有状态物联网网关上应用Kubernetes HPA。虽然无状态应用程序（如HTTP）可以很容易地水平扩展，但有状态的应用程序（包括WebSocket和MQTT）需要更多的关注。例如，在 HPA 之后，群集中将有多个网关实例。在以信息为中心的 IoT 网络上的发布-订阅模型中，客户端（订户）和服务器（发布者）需要通过同一网关进行连接。因此，作者使用WebSocket和MQTT协议为物联网网关设计了一个框架，专注于在客户端和服务器之间建立和监控活动连接。该论文还提到利用Prometheus Operator的HPA自定义指标[34]。但是，与前面提到的工作类似，它没有提供有关如何收集，计算和获取这些指标的具体信息。 值得注意的是，虽然上述大多数作品都研究Kubernetes及其功能HPA，但它们都未能详细描述HPA的工作原理及其在与不同类型的指标一起使用或在各种扩展配置（如抓取时间）下使用时的行为。这对于在 Kubernetes 中高效开发和管理容器化应用程序非常重要。因此，在本文中，我们首先讨论Kubernetes的架构，其组件及其内部通信，以建立坚实的主题基础，这将有助于读者牢牢掌握HPA相关概念，例如收集不同类型指标的方法，这将在随后进行解释。据我们所知，我们的论文是第一个完成这一任务的论文。最后，我们在各种场景中进行了严格的实验，以评估和分析HPA性能的各个方面。基于分析，我们就如何优化 Kubernetes HPA 提供深入的见解和建议，以帮助研究人员、开发人员和系统管理员做出明智的决策。 Kubernetes 的架构在本节中，我们首先介绍 Kubernetes 集群的架构 — 主要组件及其在集群内的相互通信。然后，我们仔细研究 Kubernetes 的服务如何使当前在集群内的 pod 上运行的应用程序能够作为网络服务工作。 Kubernetes集群如图 1a 所示，每个 Kubernetes 集群至少由一个主节点和几个工作节点组成。在实践中，可以有一个具有多个主节点的集群 [11]，通过复制主节点来确保集群的高可用性，因此在其中一个主节点发生故障的情况下，仍然存在一个仲裁来运行集群。 Figure 1. (a) The architecture of Kubernetes. (b,c) Examples of YAML code for application deployment in Kubernetes. Kubernetes 中最基本的执行和资源单元称为 pod，它包含一个容器或一组容器，以及有关如何操作这些容器的说明。每个 Pod 代表应用程序的一个实例，并且始终属于一个命名空间。此外，属于同一应用程序的 Pod 是相同的，并且具有相同的规格。从这个意义上说，Pod 也可以称为副本。部署应用程序时，需要指定所需的副本数以及请求的资源量。图 1b 显示了在 Namespace-1 中以 Application-A 的名义创建应用程序，并请求其每个 Pod 250Mi 和 250m，内存和 CPU 可用。“Mi”表示“兆字节”，“m”表示“毫核”，即等于 CPU 内核 1&#x2F;1000 的唯一单位。它被 Kubernetes 定义为一种测量 CPU 资源的细粒度方法，以便多个 pod 可以共享一个 CPU 核心。 此外，每个 Pod 在集群中都分配有一个唯一的 IP 地址 [11]，如图 1a 所示。这种设计允许 Kubernetes 水平扩展应用程序。例如，当应用程序需要更多计算资源时，用户无需调整现有 Pod 的规格，只需创建另一个相同的 Pod 即可共享负载。然后，这个额外的 Pod 的 IP 地址将包含在应用程序的服务中，该服务将传入流量路由到新 Pod 以及现有 Pod。这将再次更详细地讨论。 主节点主节点通过控制平面的四个主要组件（即 kube-apiserver、kube-controller-manager、kube-scheduler 和 etcd [11]）对集群进行全面控制，如图 1a 所示。 kube-controller-manager 监视并确保集群在所需状态下运行。例如，一个应用程序正在运行4个pod;但是，其中一个被逐出或丢失，kube-controller-manager 必须确保创建新的副本。 kube-scheduler 查找新创建和未调度的 Pod 以将它们分配给节点。它必须考虑几个因素，包括节点的资源可用性和亲和力规范。在前面的示例中，当新 pod 已创建且当前未调度时，kube 调度程序会在集群内搜索满足要求的节点，并分配 pod 在该节点上运行。 etcd 是包含集群所有配置数据的后备存储。 kube-apiserver是可以与所有其他组件通信的基础管理组件，对集群状态的每次更改都必须经过它。kube-apiserver还能够通过 kubelet 与工作节点交互，这将在后面讨论。此外，用户可以通过将 kubectl 命令传递给 kube-apiserver 来通过主服务器管理集群。在图 1 中，在运行命令 kubectl apply -f examA.yaml 之后，此文件中的规范通过 kube-apiserver 传递到 kube-controller-manager 以进行副本控制，并传递到 kube-scheduler 以在特定节点上调度 Pod。他们将回复 kube-apiserver，然后 kube-apiserver 会向这些节点发出信号，要求创建和运行 pod。这些配置也存储在 etcd 中。 工作节点工作节点以 Pod 的形式分配计算资源，并根据主节点的指令运行它们。 kubelet 是一个本地代理，它按照主节点的 kube-apiserver 的指示操作 Pod，并保持它们的健康和活动状态。 kube-proxy （KP） 允许与集群的 Pod 进行外部和内部通信。如前所述，每个 Pod 在创建时都会分配一个唯一的 IP 地址。KP 使用此 IP 地址将来自集群内部和外部的流量转发到 Pod。 容器运行时：Kubernetes 可以被认为是容器化应用程序的专用编排平台，因此需要所有节点（包括主节点）中的容器运行时才能实际运行容器。它可以在各种运行时上运行，包括Docker，CRI-O和Containerd。其中，Docker [8]被认为是Kubernetes最常见的一个。通过将容器打包到轻量级映像中，Docker 允许用户自动部署容器化应用程序。 CAdvisor（或容器顾问）[35]是一种工具，可提供本地主机或容器的统计运行数据，例如资源使用情况。这些数据可以导出到kubelet或管理工具，如Prometheus，用于监控目的。CAdvisor 对 Docker 具有本机支持，并与 Docker 一起安装在所有节点中，以便能够监视群集内的所有节点。 Kubernetes 服务在 Kubernetes 中，可以在内部访问每个 pod，因为它具有可在集群内访问的唯一 IP 地址。但是，由于 Pod 可以随时创建和死亡，因此使用单个 Pod 的 IP 地址并不是一个合理的解决方案。此外，无法从群集外部访问这些 IP 地址，这使得用户请求或部署在不同群集中的应用程序之间的通信变得不可能。 这些情况的解决方案是Kubernetes Service [11]，它是一个抽象对象，它公开了一组可以在内部和外部轻松访问的Pod。有三种类型的 Kubernetes 服务： ClusterIP 在创建时分配给服务，并在此服务的整个生存期内保持不变。只能在内部访问群集 IP。在图 1a 中，服务 A、B 和 C 分配了三个不同的内部 IP 地址，并分别公开了三个服务端口 9897、9898 和 9899。例如，当地址 10.98.32.199：9899（由 Service-C 的集群 IP 和公开的端口组成）在集群内被命中时，流量会自动重定向到 Application-C Pod 容器上的 targetPort 9899，如图 1c 中的关键字选择器在 YAML 文件中指定的那样。根据所选策略选择确切的目标 Pod。 NodePort 是每个节点上的服务的保留端口，该节点正在运行属于该服务的 Pod。在图 1b 的示例中，NodePort 31197 和服务端口 9897 几乎耦合在一起。当流量到达节点 A 上的 NodePort 31197 时，它将路由到端口 9897 上的服务 A。然后，与前面的示例类似，流量依次路由到 targetPort 9897 上的 Pod A-1 和 A-2。这使得甚至可以从集群外部访问 Pod。例如，如果节点A的外部IP地址130.211.11.131可以从互联网访问，则通过点击地址130.211.11.131：31197，用户实际上是在向Pod A-1和A-2发送请求。但是，很明显，直接访问节点的IP地址并不是一种有效的策略。 负载平衡由特定云服务提供商提供。当集群部署在 GCP [2]、Azure [3] 或 AWS [1] 等云平台上时，会为其提供负载均衡器，该负载均衡器可通过 URL 在外部轻松访问（www.my-example-app.com）。发往此 URL 的所有流量都将以与上一个示例类似的方式转发到 NodePort 31198 上的群集节点，如图 1a 所示。 水平自动伸缩在 Kubernetes 中，HPA 是一项强大的功能，它可以自动提高 Pod 的数量，从而提高应用程序的整体计算和处理能力，而无需停止应用程序当前正在运行的实例 [11]。成功创建后，这些新 Pod 能够与现有 Pod 共享传入负载。从技术角度来看，HPA是由kube-controller-manager实现的控制回路。默认情况下，每隔 15 秒（也称为同步周期）kube-controller-manager 都会将收集的指标与 HPA 配置中指定的阈值进行比较。图 2a 显示了 HPA 的配置。’’minReplicas’ 和 ‘’maxReplicas’ 是指应该在集群中运行的 pod 的最小和最大数量。在此示例中，minReplicas 和 maxReplicas 分别为 2 和 4。复制控制器是 kube-controller-manager 的一个组件，它跟踪副本集，并确保此应用程序的 Pod 始终在群集中运行不少于 2 个且不超过 4 个。用于此 HPA 的指标是 CPU 使用率。一旦 CPU 利用率的平均值达到预设阈值，HPA 就会通过计算以下内容自动增加 Pod 数： 1desiredReplicas = currentReplicas * currentMetricValue / desiredMetricValue 其中，desiredReplicas是扩展后的 Pod 数，currentReplicas 是当前运行的 Pod 数，currentMetricValue 是最新收集的指标值，所需的 desiredMetricValue 是目标阈值。desiredMetricValue实际上是图2a中的阈值目标AverageValue。 在此示例中，当currentMetricValue（在本例中为CPU使用率）达到150 m（高于阈值desiredMetricValue 60 m）时，desiredReplicas等于d2×（150&#x2F;60）e或5。但是，由于副本的最大数量只有 4 个，因此 kube-controller-manager 仅向 kube-apiserver 发出信号，要求再增加 2 个副本。在此之后，如果平均 CPU 使用率下降到 40 m，则desiredReplicas 为 d4 ×（40&#x2F;60）e 或 3。因此，将删除其中一个新创建的 Pod。但是，值得注意的是，为了避免重复创建和删除 Pod，因为指标可能会大幅波动，每个新创建的 Pod 在从集群中删除之前，至少会运行一段时间的降级延迟期。这段时间定为5分钟[11]。此外，HPA 可以使用多个指标，每个指标都有自己的阈值。当这些指标中的任何一个达到其阈值时，HPA 将以上述方式扩展群集。但是，对于缩减操作，所有这些指标都必须低于其阈值。 kube-controller-manager用于水平自动缩放的上述指标是Kubernetes的默认资源指标或Prometheus [13]，Microsoft Azure [3]等提供的外部自定义指标。在本文的范围内，我们只讨论Kubernetes资源指标和Prometheus Custom Metrics，这是HPA最受欢迎的。 Figure 2. (a) Examples of YAML code for configuring HPA and Readiness Probe in Kubernetes. (b) Horizontal Pod Autoscaling’s (HPA) architecture. Kubernetes 资源指标如图 2b 所示，cAdvisor 充当监控代理，收集主机和正在运行的 Pod 的 CPU、内存使用情况等核心指标，并通过 HTTP 端口发布这些指标。例如，在图 2b 中，cAdvisor 当前正在监视 4 个现有的 Pod A-1、A-2、B-1 和 B-2。在 kubelet 的帮助下，Metrics-Server 会定期抓取这些指标。默认抓取周期为 60 秒，可以通过更改 Metrics-Server 的部署配置进行调整。然后，Metrics-Server将它们作为各个Pod和节点的CPU和内存使用情况公开给kube-apiserver中的指标聚合器，其平均值将被计算并提取到HPA。这称为资源指标管道 [11]。Kubernetes Resource Metrics可以通过将命令kubectl top pod和kubectl top node传递给kube-apiserver来手动检查。 Prometheus 自定义指标Prometheus [13]允许灵活的监控，因为它将监控的目标公开为端点，并通过HTTP服务器定期提取其指标。它可以监视各种目标，包括节点，pod，服务甚至自身。其中每个目标的监视操作称为作业。虽然Prometheus的默认全局获取周期为60秒，但其工作可以有自己的周期。对于寿命太短而无法抓取的工作，Prometheus有一个名为Pushgateway的组件，这些工作可以在退出后直接推送其指标。然后，通过将Pushgateway也公开为端点，Prometheus可以在以后抓取这些指标，即使在作业终止之后也是如此。如图 2b 所示，Prometheus 使用一个名为 kubernetes-pods 的作业从现有的 pod A-1、A-2、B-1 和 B-2 中抓取指标。 然后，抓取的数据以时间序列的形式存储在时间序列数据库（TSDB）中，该数据库暴露给Prometheus Adapter[36]，该适配器是用PromQL（Prometheus Query Language）编写的 - 一种功能查询语言[13]，并且有几个查询来实际处理原始时间序列指标。例如，查询速率（http_requests_total[1m]）返回1分钟内收集的时间序列的每秒平均速率。时间序列的编号取决于抓取周期。 对于上述查询，15 秒的抓取周期总共会产生 4 个时间序列。 查询完成后，生成的指标将被发送到 kube-apiserver 中的 Metrics Aggregator，并通过该服务器提取到 HPA。虽然指标服务器只能监控CPU和内存使用情况，但Prometheus可以提供各种自定义指标。上一示例中的指标可用作 HTTP 请求的平均到达率，该比率可添加到图 2a 中的 HPA 中。因此，在本例中，如果有两个输入指标，则 HPA 会在任一指标达到其阈值时纵向扩展，并在两个指标都低于其阈值时缩减。 就绪性探头在大多数情况下，Pod 在创建后尚未准备好立即提供流量，因为它们可能必须加载数据或配置。因此，如果在这些新创建的 Pod 启动期间将流量发送到它们，则请求显然会失败。作为一种解决方案，Kubernetes 提供了一个名为 Readiness Probe [11] 的功能，该功能检查新 Pod 的状态，并且仅在准备就绪后才允许流量流向它们。在图 2a 中，初始的DelaySeconds定义了从创建 Pod 到首次检查 Pod 的就绪时间。如果在检查之后，Pod 仍未准备就绪，Kubernetes 将每隔一段时间定期检查一次。在这个例子中，一旦 Pod B-3 和 B-4 被创建，Kubernetes 就会给它 5 个时间来启动和准备就绪。第一次准备情况检查后，如果容器的状态为就绪，它将立即开始为传入流量提供服务。另一方面，如果不是，Kubernetes 每 10 秒检查一次 3 次，这由 failureThreshold 定义，然后放弃并决定根据预设配置重置或将 pod 视为未就绪。 性能评估在本节中，我们将详细展示和讨论评估结果，以确认我们对 Kubernetes 及其 HPA 功能的理解之前，我们将介绍我们的实验设置。我们还就如何优化HPA提供分析和深入见解。 实验设置我们在一台在英特尔（R） 酷睿 （TM） i7-8700 @ 3.20Ghz * 12 上运行的物理机内，设置了一个由 5 个节点组成的 Kubernetes 集群，该节点由 1 个主节点和 4 个工作节点组成。集群的每个节点都运行一个虚拟机，其中包含 Ubuntu 18.04.3 LTS 操作系统、Docker 版本 18.09.7、Kubernetes 版本 1.15.1。在计算能力方面，主节点分配了4个核心处理器和8GB RAM，而每个工作节点则分配了2个核心处理器和2GB RAM。此外，Gatling开源版本3.3.1 [37]被用作负载生成器，通过每个工作节点上的指定NodePort向我们的应用程序发送HTTP请求。 我们的应用程序被设计为 CPU 扩展。换句话说，一旦它成功接收到HTTP请求，它就会使用CPU资源，直到将响应发送回源。每个副本的 CPU 请求和限制分别为 100 m 和 200 m。副本数的范围从最小值 4（平均 1 个副本&#x2F;节点）到最大 24 个（平均每个节点 6 个副本）。 所有实验运行300秒。在前 100秒，Gatling 发送的平均传入请求速率约为 1800 个请求&#x2F;秒，而在接下来的 100 个请求中，它大约为 600 个请求&#x2F;秒，总共生成 240，000 个请求。我们将这两个时间段分别定义为高流量周期 （HTP） 和低流量周期 （LTP）。其余的模拟时间用于在没有到达请求的情况下观察指标的减少。在本文中，我们在7个不同的实验中测试了Kubernetes HPA的性能。每个实验重复10次，以确保其准确性。 实验结果默认 Kubernetes 资源指标的 HPA 性能设置。指标的抓取周期设置为默认值 60 秒。 目标。我们的目标是使用默认的 Kubernetes 资源指标 （KRM） 在默认抓取期内的 CPU 使用率、副本数和失败请求数方面评估 HPA 的性能。 如图 3a 所示，由于请求速率高，平均 CPU 使用率增加到极限。之后，随着副本数量的增加，它也会减少。可以观察到，最引人注目的一点是CPU使用率的指标值在每个抓取周期（60 s）都会发生变化。这是因为 Kubelet 只在抓取周期开始时从 cAdvisor 中抓取原始指标。然后，通过指标服务器报告指标，而不对指标聚合器进行任何修改。换句话说，指标的报告值与抓取的值完全相等。 图 3.HPA 使用默认的 Kubernetes Resource Metrics （KRM）。（a） 平均 CPU 使用率和副本集的缩放比例。（b） 失败请求的总数。（c） 失败请求的时间表。 将副本集扩展到 8 的第一个扩容操作发生在第 50 秒左右，因为指标值在第 35s和第 40 个秒之间增加。之后，当指标值达到最大值 200% 时，副本集将在第 65 秒再次扩展到 15。但是，第三个缩放操作发生在第 110s，即 55s 之后，即使 CPU 使用率仍然很高。这是由于HPA的一个非常重要的特征。默认情况下，HPA 每 15 秒检查一次指标值。在检查时，如果与上次检查相比，该值没有变化，则认为没有必要调整副本数。我们可以看到，从第 40 秒到第 100 秒，正好是一个抓取周期，CPU 使用率不会改变。在此之后，它下降到约180%，这实际上触发了对 21 个副本的第三次缩放操作。在这里，指标值再次保持稳定，直到第 160 秒，它开始下降，并且不会导致任何进一步的扩展。复制副本的数量在实验结束之前保持稳定，因为 HPA 必须等待 5 分钟，从上次放大到缩小。此设计旨在避免由连续缩放操作引起的抖动。 图3b，c显示了失败请求的数量和失败的时间。这些请求在扩展操作期间被拒绝，因为新创建的 Pod 尚未准备好为流量提供服务。在此期间路由到它们的任何请求都将失败。此外，我们可以看到，第一次扩展会导致大多数请求失败，因为它发生在 HTP 期间。高请求速率会导致更多请求被拒绝。与此相反，第三次纵向扩展仅会导致少量故障。 总之，需要注意的是，Kubernetes HPA 旨在定期检查指标值，有关扩展的决策取决于指标值是否与上次检查相比发生了变化。此外，由于 KRM 的值仅在每个抓取周期内更改，因此创建的副本数取决于此因素。因此，下一个实验分析抓取周期的长度对HPA性能的影响。 使用默认 Kubernetes 资源指标和不同抓取周期的 HPA 性能设置。KRM的抓取周期分别调整为15 s，30 s和60 s。 目标。我们的目标是研究不同抓取期对HPA性能的影响。 图 4 显示了使用 KRM 的 HPA 在 15 秒、30 秒和 60 秒三个不同抓取周期内的性能。在图 4a–c 中，我们可以看到指标值的趋势仍然遵循前面描述的模式。这些值会更改所有三种情况的每个抓取周期。换句话说，随着周期的延长，指标值将保持在同一水平更长的时间内。 Figure 4. HPA using default Kubernetes Resource Metrics (KRM). (a–c) The average CPU usage and the scaling of the replica set for scraping periods of 15 s, 30 s, and 60 s, respectively. (d–f) The total number of the failed requests for scraping periods of 15 s, 30 s, and 60 s, respectively. 但是，副本的最大数量往往会随着抓取周期的增加而减少。在图 4a、b 中，副本的最大数量分别为 24 和 23。另一方面，对于60秒抓取期的情况，它只有21。这是因为如果指标值没有变化，则不会触发扩容操作，如前所述。在这种情况下，由于值在较短的抓取周期内更改得更频繁，因此副本数也会更频繁地增加。此外，图 4d–f 显示了三种情况下失败的请求数。与副本的最大数量类似，失败请求的数量往往会随着爬取周期的延长而减少。这是因为未准备好的 Pod 获得的传入流量越多，被拒绝的请求就越多。请注意，进入未就绪 Pod 的请求的失败可以使用就绪情况调查来解决，其影响将在第 5.2.7 节中分析。 通过上述解释，可以得出结论，当在相同的负载下相同时间时，较长的抓取期会导致HPA产生较少数量的复制品。这有优点和缺点。一方面，较长的抓取期可能会通过触发相对缓慢的扩展操作来添加较少数量的副本，从而实现高效的资源分配。另一方面，如果传入负载变得过高，则可能导致所需资源不足。 HPA性能与Prometheus 自定义指标设置。Prometheus Custom Metrics 的抓取周期设置为默认值 60 s。 目标。我们的目标是使用 Prometheus 自定义指标评估 HPA 的性能，包括 CPU 使用率、副本数量和失败请求，以便与使用 KRM 的 HPA 进行比较。 从图 5a 可以看出，每次查询 PCM 的指标值时，都会非常频繁地变化。这与KRM的情况完全相反。其原因在于PCM的收集方式。尽管 Prometheus 也根据抓取周期来抓取 pod 的指标，但这些指标必须通过 Prometheus Adapter 的 rate（） 函数来计算它们每秒平均增加的次数 [13]。此外，重要的是要注意，rate（） 函数还在必要时根据指标的当前趋势执行外推，例如在缺少时间序列数据点的情况下。因此，在这种情况下，CPU 使用率在每个查询周期都会更改。反过来，这些频繁的变化导致HPA迅速将副本数量增加到最多24个，而KRM只有21个。因此，PCM 具有对指标值频繁变化的响应能力优势。快速增加副本数量或整体计算能力，使 HPA 能够处理传入负载的激增。但是，缺点是失败请求数较多，如图 5b，c 所示。 Figure 5. HPA using Prometheus Custom Metrics (PCM). (a) The average CPU usage and the scaling of the replica set. (b) The total number of the failed requests. (c) The timeline of the failed requests. HPA性能与普罗米修斯自定义指标和不同的抓取周期设置。指标抓取周期为15s，30s和60s。 目标。我们的目标是研究使用PCM的不同抓取周期对HPA性能的影响。 图 6 显示了 HPA 在 CPU 使用率方面的性能，以及具有三个不同抓取周期（分别为 15 秒、30 秒和 60 秒）的副本数。可以看出，在所有这三种情况下，图表的趋势都非常相似。如前所述，原因是Prometheus Adapter（负责将原始指标转换为自定义指标的实体）可以执行外推以在原始数据点丢失时提供指标。如果查询时刻处于抓取周期的中间，Prometheus Adapter（尤其是 rate（） 会根据之前收集的数据点计算指标。由于指标值的相似性，在同一时期内，三个事例的副本数以类似的模式增加。 Figure 6. HPA using Prometheus Custom Metrics (PCM). (a–c) The average CPU usage and the scaling of the replica set for scraping periods of 15 s, 30 s, and 60 s, respectively. (d–f) The total number of the failed requests for scraping periods of 15 s, 30 s, and 60 s, respectively. 我们可以得出结论，PCM不像KRM那样受到抓取期调整的强烈影响。可以选择更长的抓取期，因为它将减少收集和提取指标所需的计算和内部通信资源量。但是，值得注意的是，较长的抓取周期意味着数据点较少，这可能会降低速率函数的精度。 2-worker 集群和 4 Worker 集群中的 HPA 性能比较设置。设置了两个由 2 个和 4 个工作节点组成的集群。工作线程节点是相同的，并且两种情况下副本的最大和最小数量是相同的。 目标。我们的目标是调查和比较HPA在这两种情况下的表现。 图 7 显示了在 2 工作线程群集和 4 工作群集（PCM-2W 和 PCM-4W）中使用 PCM 的 HPA 之间的性能比较。从图 7a 中可以得出结论，两个 CPU 使用率值的总体趋势大致相似。CPU 使用率中唯一明显的差异出现在 30 和 70 秒之间。这是由于副本增加的差异造成的，如图 7b 所示，其中 PCM-2W 增加副本数量的速度比 PCM-4W 慢。这是因为，虽然两个 HPA 的最大副本数均为 24，但对于 4 个工作线程和 2 个工作线程集群，每个节点上的平均 Pod 数分别为 6 个和 12 个。它表明，与 4 工作线程群集中的节点相比，2 工作线程群集中的节点在扩展时必须处理更高的计算负载，这使得为其他 Pod 创建和分配资源的速度更慢。 Figure 7. Comparisons of HPA performances in 2-worker and 4-worker clusters. (a) The average CPU usage. (b) The scaling of the replica set. (c) The total number of the failed requests. (d) Response time of the successful requests. 不同自定义指标的HPA性能比较设置。与仅监视 CPU 和内存使用情况的 KRM 相反，PCM 可以监视其他指标，例如传入 HTTP 请求的速率。在第一种情况下，仅请求速率用于 HPA。另一方面，在第二种情况下，它与CPU使用率相结合。 目标。我们的目标是研究使用不同自定义指标对HPA性能的影响。 图 8 显示了使用 HTTP 请求 （PCM-H） 的 HPA 与使用 CPU 和 HTTP 请求 （PCM-CH） 的 HPA 之间的性能比较。PCM-H 提供基于传入请求速率的自动缩放。另一方面，PCM-CH 结合了速率和 CPU 使用率。指定多个指标时，如果任一指标达到其阈值，HPA 将纵向扩展。从图8a的CPU使用率比较中可以看出，一般来说，PCM-H的CPU使用率明显高于PCM-CH。在图 8b 中，PCM-CH 的平均请求速率远高于 PCM-H 从第 30 秒到第 60 秒的平均请求速率。之后，它一直保持在较低水平，直到第250秒。这是因为 PCM-CH 的请求速率急剧上升，从而触发了第一次扩展反应，如图 8c 所示，之后 CPU 使用率仍高于其阈值，并导致后续副本数增加到最大数量。这里，由于 PCM-CH 具有更多副本，因此平均请求速率较低。另一方面，PCM-H 仅使用请求速率。在第一个拓展之后，耗时降低并保持在阈值以下，这不会触发任何进一步的拓展。此外，由于现在它只有13个副本，因此CPU使用率上升并保持在较高水平。此外，由于 Pod 的增加较少，PCM-H 产生的失败请求数仅为 PCM-CH 的一半左右。 Figure 8. Comparison of HPA Performances with different custom metrics. (a) The average CPU usage. (b) The average rate of HTTP requests. (c) The scaling of the replica set. (d) The total number of the failed requests. 有和没有就绪探头的HPA性能比较设置。在一种情况下设置常规HPA，而在另一种情况下则伴随着准备情况调查。 目标。我们的目标是调查Readiness Probe的使用及其对失败请求数和响应时间的影响。 图 9a 显示了 HPA 与使用就绪度探测器 （RP） 和常规 HPA 在失败请求数方面的比较。很明显，在使用RP的情况下，没有失败的请求，因为当额外的 pod 准备就绪时，Kubernetes 服务不会将任何流量路由到它们。只有在它们被认为准备就绪后，它们才能接收和处理传入的请求。因此，我们可以预期，通过使用 RP 可以避免从前面的实验中观察到的失败请求。但是，这是一种权衡，因为一般响应时间明显高于图 9b 中所示的不使用 RP 的情况，因为更多的流量被路由到现有 Pod。 Figure 9. Comparison of HPA performances with and without Readiness Probe. (a) The total number of the failed requests. (b) Response time of the successful requests. 讨论为了总结之前的实验和分析，我们列出了Kubernetes HPA行为的几个关键点。 关于 KRM 和 PCM：KRM 仅报告指标值，并且每个抓取周期只能更改一次，而 PCM 则即使在抓取周期的中间或缺少数据点时，PCM 也能够保持指标值的趋势。直接结果是，与 PCM 相比，KRM 扩展副本集的速度较慢，并且主要扩展到较少数量的副本。这种行为的优点显然是更少的资源消耗。另一方面，当在高负载下时，Pod可能会崩溃或变得不可用。因此，我们建议将 KRM 用于负载更稳定的应用，例如视频处理服务。在这种情况下，来自观看者的请求数量通常很小，因为视频至少需要几分钟到几个小时。相反，PCM 更适合指标频繁变化的应用程序。例如，电子商务网站可能会在销售活动期间在几个小时内经历持续的激增，因此需要快速的系统反应。 关于抓取周期：对PCM抓取周期的调整不会对HPA的性能产生强烈影响。因此，可以设置更长的时间段以减少用于提取指标的资源量。但是，值得注意的是，过长的时间段可能会导致计算指标不精确。关于KRM，抓取周期对HPA的性能有重大影响。较长的时间段可以减少为新 Pod 分配的资源量，但会导致服务质量下降。因此，在考虑了服务类型和群集功能的情况下，应仔细选择抓取期。 关于集群大小：很明显，4个工作线程的集群具有更多的计算能力，这使得它能够比2个工作线程集群更快地执行HPA操作，假设两个集群的工作线程在计算能力方面是相同的。此外，4 工作线程群集的通信能力优于 2 工作线程群集。这会导致两个集群的请求响应时间不同。但是，即使 2 工作线程群集具有相同的计算能力和通信能力，将 Pod 扩展到更宽的群集也更安全，因为当节点崩溃时，与 4 工作线程群集中的四分之一 Pod 相比，一半的 Pod 可能变得不可用。 在具有不同自定义指标的 HPA 上：Prometheus 允许使用自定义指标（如 HTTP 请求速率）来满足特定需求。特别是将多个指标组合在一起也可以提高HPA的有效性，因为任何单个指标的变化都会导致缩放反应。但是，作为缺点，这可能会导致资源浪费。因此，应根据应用程序的类型选择指标或指标组合。例如，游戏应用程序可能具有各种请求大小。在地图上移动字符的请求很小，但数量可能很多。因此，应考虑请求速率，以便可以快速满足每个请求，从而减少“滞后”效应并改善整体游戏体验。另一方面，加载新位置地图的请求很重，但数量很少。在这里，计算要求明显增长，这表明HPA应该根据CPU和内存使用情况进行扩展。简而言之，自定义指标使应用程序能够考虑各种因素，例如请求数量，延迟和带宽，以实现有效的水平自动缩放。 关于Readiness Probe：Kubernetes 的一项强大功能是防止请求被路由到未准备好的 Pod，这将拒绝请求。但是，将许多请求路由到现有 Pod 可能会导致其余请求的响应时间明显延长。因此，在保持传入请求处于活动状态或让它们失败和期望重新请求之间，应根据系统资源和 QoS 要求之间的平衡仔细选择一个请求。 结论Kubernetes 是一个功能强大的容器化应用程序和服务编排平台，可应用于包括云&#x2F;边缘计算和物联网网关在内的重要未来技术。其功能 HPA 可为应用程序提供动态有效的扩展，而无需人工干预。在本文中，我们给出了 Kubernetes 和 HPA 的第一个全面的架构级视图。还彻底解释了每种类型的指标（包括 Kubernetes 资源指标和 Prometheus 自定义指标）是如何收集、计算和提取到 HPA 的。此外，我们还进行了多次实验，涵盖了各种场景，并对Kubernetes HPA的行为进行了清晰的分析。 本文应作为进一步研究和开发Kubernetes和HPA的基础研究。未来，我们的目标是通过更多的HPA场景扩展我们的实验，并为Kubernetes开发更有效的扩展算法。 作者贡献：Conceptualization, T.-T.N., Y.-J.Y. and T.K.; Experiment, T.-T.N. and Y.-J.Y.; Writing—Original Draft Preparation, T.-T.N.; Review &amp; Editing, T.K.; Supervision, D.-H.P. and T.K.; Funding Acquisition, D.-H.P. and S.K. All authors have read and agreed to the published version of the manuscript. 资助：这项工作得到了韩国政府（MSIT）资助的信息通信技术规划与评估研究所（IITP）资助（No.2018-0-00387，开发基于ICT的智能智能福利住房系统，用于预防和控制牲畜疾病）。 致谢：作者对Linh-An Phan在整个研究中的许多宝贵评论表示高度赞赏，并感谢Dinh-Nguyen Nguyen在早期阶段的支持。 利益冲突：作者声明没有利益冲突。 引用 Amazon Web Services. Available online: https://aws.amazon.com (accessed on 23 June 2020). Google Cloud Platform. Available online: https://cloud.google.com (accessed on 23 June 2020). Microsoft Azure. Available online: https://azure.microsoft.com (accessed on 23 June 2020). Pahl, C.; Brogi, A.; Soldani, J.; Jamshidi, P. Cloud Container Technologies: A State-of-the-Art Review. IEEE Trans. Cloud Comput. 2017, 7, 677–692. [CrossRef] He, S.; Guo, L.; Guo, Y.; Wu, C.; Ghanem, M.; Han, R. Elastic application container: A lightweight approach for cloud resource provisioning. In Proceedings of the 2012 IEEE 26th International Conference on Advanced Information Networking and Applications, Fukuoka, Japan, 26–29 March 2012; pp. 15–22. [CrossRef] Dua, R.; Raja, A.R.; Kakadia, D. Virtualization vs containerization to support PaaS. In Proceedings of the2014 IEEE International Conference on Cloud Engineering, Boston, MA, USA, 11–14 March 2014; pp. 610–614. [CrossRef] Pahl, C. Containerization and the PaaS Cloud. IEEE Cloud Comput. 2015, 2, 24–31. [CrossRef] Docker. Available online: https://www.docker.com (accessed on 23 June 2020). Amazon Elastic Container Service. Available online: https://aws.amazon.com/ecs (accessed on 23 June 2020). Red Hat OpenShift Container Platform. Available online: https://www.openshift.com/products/containerplatform (accessed on 23 June 2020). Kubernetes. Available online: www.kubernetes.io (accessed on 23 June 2020). Chang, C.C.; Yang, S.R.; Yeh, E.H.; Lin, P.; Jeng, J.Y. A Kubernetes-based monitoring platform for dynamic cloud resource provisioning. In Proceedings of the GLOBECOM 2017—2017 IEEE Global Communications Conference, Singapore, 4–8 December 2017; pp. 1–6. [CrossRef] Prometheus. Available online: https://prometheus.io (accessed on 23 June 2020). Cloud Native Computing Foundation. Available online: https://www.cncf.io (accessed on 23 June 2020). Thurgood, B.; Lennon, R.G. Cloud computing with Kubernetes cluster elastic scaling. ICPS Proc. 2019, 1–7, doi:10.1145&#x2F;3341325.3341995. [CrossRef] Rattihalli, G.; Govindaraju, M.; Lu, H.; Tiwari, D. Exploring potential for non-disruptive vertical auto scaling and resource estimation in kubernetes. In Proceedings of the IEEE International Conference on Cloud Computing (CLOUD), Milan, Italy, 8–13 July 2019; pp. 33–40. [CrossRef] Song, M.; Zhang, C.; Haihong, E. An auto scaling system for API gateway based on Kubernetes. In Proceedings of the 2018 IEEE 9th International Conference on Software Engineering and Service Science (ICSESS), Beijing, China, 23–25 November 2018; pp. 109–112. [CrossRef] Jin-Gang, Y.; Ya-Rong, Z.; Bo, Y.; Shu, L. Research and application of auto-scaling unified communication server based on Docker. In Proceedings of the 2017 10th International Conference on Intelligent Computation Technology and Automation (ICICTA), Changsha, China, 9–10 October 2017; pp. 152–156. [CrossRef] Townend, P.; Clement, S.; Burdett, D.; Yang, R.; Shaw, J.; Slater, B.; Xu, J. Improving data center efficiency through holistic scheduling in kubernetes. In Proceedings of the 2019 IEEE International Conference on Service-Oriented System Engineering (SOSE), Newark, CA, USA, 4–9 April 2019; pp. 156–166. [CrossRef] 20. Rossi, F. Auto-scaling policies to adapt the application deployment in Kubernetes. CEUR Workshop Proc. 2020, 2575, 30–38. Balla, D.; Simon, C.; Maliosz, M. Adaptive scaling of Kubernetes pods. In Proceedings of the IEEE&#x2F;IFIPNetwork Operations and Management Symposium 2020: Management in the Age of Softwarization andArtificial Intelligence, NOMS 2020, Budapest, Hungary, 20–24 April 2020; pp. 8–12. [CrossRef] Casalicchio, E.; Perciballi, V. Auto-scaling of containers: The impact of relative and absolute metrics.In Proceedings of the 2017 IEEE 2nd International Workshops on Foundations and Applications of Self*Systems (FAS*W), Tucson, AZ, USA, 18–22 September 2017; pp. 207–214. [CrossRef] Casalicchio, E. A study on performance measures for auto-scaling CPU-intensive containerized applications. Clust. Comput. 2019, 22, 995–1006. [CrossRef] Taherizadeh, S.; Grobelnik, M. Key influencing factors of the Kubernetes auto-scaler for computing-intensive microservice-native cloud-based applications. Adv. Eng. Softw. 2020, 140, 102734. [CrossRef] Santos, J.; Wauters, T.; Volckaert, B.; De Turck, F. Towards network-Aware resource provisioning in kubernetes for fog computing applications. In Proceedings of the 2019 IEEE Conference on Network Softwarization (NetSoft), Paris, France, 24–28 June 2019; pp. 351–359. [CrossRef] Santos, J.; Wauters, T.; Volckaert, B.; Turck, F.D. Resource provisioning in fog computing: From theory to practice. Sensors 2019, 19, 1–25. [CrossRef] [PubMed] Zheng, W.S.; Yen, L.H. Auto-scaling in Kubernetes-based Fog Computing platform. In International Computer Symposium; Springer: Singapore, 2018; pp. 338–345. [CrossRef] Heapster. Available online: https://github.com/kubernetes-retired/heapster (accessed on 23 June 2020). InfluxDB. Available online: https://www.influxdata.com (accessed on 23 June 2020). Grafana. Available online: https://grafana.com (accessed on 23 June 2020). Apache JMeter. Available online: https://jmeter.apache.org (accessed on 23 June 2020). Syu, Y.; Wang, C.M. modeling and forecasting http requests-based Cloud workloads using autoregressive artificial Neural Networks. In Proceedings of the 2018 3rd International Conference on Computer and Communication Systems (ICCCS), Nagoya, Japan, 27–30 April 2018; pp. 21–24. [CrossRef] Dickel, H.; Podolskiy, V.; Gerndt, M. Evaluation of autoscaling metrics for (stateful) IoT gateways.In Proceedings of the 2019 IEEE 12th Conference on Service-Oriented Computing and Applications (SOCA), Kaohsiung, Taiwan, 18–21 November 2019; pp. 17–24. [CrossRef] Prometheus Operator. Available online: https://github.com/coreos/prometheus-operator (accessed on 23 June 2020). CAdvisor. Available online: https://github.com/google/cadvisor (accessed on 23 June 2020). Prometheus Adapter. Available online: https://github.com/DirectXMan12/k8s-prometheus-adapter (accessed on 23 June 2020). Gatling. Available online: https://gatling.io/open-source (accessed on 23 June 2020).","categories":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"http://jerryblogs.com/categories/Kubernetes/"}],"tags":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"http://jerryblogs.com/tags/Kubernetes/"}]},{"title":"限制内核模块自动加载","slug":"linux-core-module-load","date":"2022-05-25T14:06:37.000Z","updated":"2023-04-20T13:39:45.664Z","comments":true,"path":"2022/05/25/linux-core-module-load/","link":"","permalink":"http://jerryblogs.com/2022/05/25/linux-core-module-load/","excerpt":"","text":"概述内核的模块机制允许构建具有广泛软硬件支持的内核，而无需将所有的代码实际加载到任何给定的运行系统中。在一个典型的分配器内核中，所有这些模块的可用性意味着非常多的功能是可用的，但是也可能存在许多可以被利用的bug。在许多情况下，内核的自动模块加载器已被用于将错误代码带入正在运行的系统当中。减少内核暴露于有缺陷的模块的尝试表明，某些类型的加固工作是多么困难。 模块自动加载两种方法可以将模块加载到Linux内核中，无需管理员进行明确的操作。在大多数的现有系统上，它发生在硬件被发现时，无论是通过总线驱动程序（在受支持被发现的总线上）还是通过设备树等外部描述。这个发现将导致一个事件被发送到用户空间像udev这样的已经被配置的守护进程应用和被加载的适当的模块。这种机制由可用的硬件驱动，并且相对难以受攻击者影响。 然而在内核中，潜伏着一种较旧的机制，即request_module()函数的形式。当内核函数确定缺少所需的模块时，它可以调用request_module()函数向用户空间发送请求以加载相关的模块。例如，如果应用程序使用给定的主要和次要编号打开一个char设备，并且这些数字不存在驱动程序，则char设备代码将尝试通过调用来定位驱动程序： 1request_module(&quot;char-major-%d-%d&quot;, MAJOR(dev), MINOR(dev)); 如果驱动模块声明了一个具有匹配编号的别名，他将自动加载到内核中以处理打开请求。 内核中有数百个request_module()调用。有些是非常具体的；如果用户不幸拥有这样一个设备，就会自动加载ide-tape模块。其他的更为普遍；例如，在网络子系统中有许多调用，以找到实现特定网络协议或数据包过滤机制的模块。虽然特定设备的调用已经大部分被udev机制所取代，但是像网络协议这样的模块仍然依赖于request_module()来实现用户透明的自动加载。 自动加载方便系统管理，但是它也可以方便于系统的开发。如果DCCP模块未被加载到内核中，则2月份发布的DCCP协议漏洞是不可利用的。通常情况下，因为DCCP的用户很少。但是自动加载机制允许任何用户通过创建一个DCCP套接字来强制加载该模块。因此，自动加载扩大了内核的攻击面，将非特权用户可能导致加载的任何模块包含在模块中——典型的分发器内核中有很多模块。 收紧系统Djalal Harouni一直在开发一个补丁集，旨在减少自动加载的风险；最新版本是在11月27日发布的。Harouni的工作从grseurity补丁集的加固中获得了灵感，但没有从那里获得代码。在这个版本中（随着时间的推移，它有一些变化），它增加了一个新的sysctl的按钮（&#x2F;proc&#x2F;sys&#x2F;kernel&#x2F;module_autoload_mode），用来限制内核的自动加载机制。如果这个按钮被设置为零（默认值），自动加载就会像在当前内核一样工作。把它设置为1，就可以把自动加载限制在具有特定功能的进程中：具有CAP_SYS_MODULE的进程就可以使任何模块被加载，而具有CAP_NET_ADMIN的进程可以加载任何以netdev-开头的模块。将这个按钮设置为2可以完全禁用自动加载功能。一旦这个值被提高到0以上，在系统的生命周期内就不能再降低了。 补丁集还实现了可以使用prctl()系统调用设置每个进程的标志。该标志（与全局标志具有相同的值）可以限制特定进程及其所有后代的自动加载，而不会改变整个系统中的模块加载行为。 可以肯定地说，这个补丁集不会以其当前形式合并，原因很简单：Linus Torvalds非常不喜欢它。禁用自动加载可能破坏许多系统，这意味着分销商将不愿意启动这个选项，他也不会有太多的用途。“人们在不破坏系统的情况下无法使用的安全选项毫无意义”，他说。讨论有时候会变得激烈，但是Linus并不反对减少内核暴露于自动加载漏洞的想法。这只是找到正确解决方案的问题。 每个进程标志看起来是解决该问题方案的一部分。例如，它可以用于限制在容器内运行的代码的自动加载，同时保持整个系统不变。在容器中创建具有CAP_NET_ADMIN功能的进程来配置该容器的网络并希望容器中运行的大多数代码无法强制加载模块的情况并不少见。 但是Linus说，一个标志永远无法正确控制自动加载发挥作用的所有情况。一些模块可能始终是可加载的，而其他模块可能需要特定的功能。因此他建议保留Harouni的补丁集添加的request_module_cap()函数（仅在存在特定功能时才执行加载）并更广泛使用它。但他确实有一些更改要求。 首先是request_module_cap()不应该在所需的功能不存在的情况下实际阻止模块的加载-至少在开始时不应该。相反，它应该记录一条信息。这将允许对实际需要模块自动加载的地方进行研究，如果幸运的话，将指出可以限制自动加载而不破坏现有系统的地方。他还建议，能力检查过于简单。例如，上面描述的“char-major-”自动加载在进程能够打开具有给定的主要和次要编号的设备节点时发生。在这种情况下，权限测试（打开该特殊文件的能力）已经通过，模块应该无条件加载。因此，可能需要request_module()的其他条件来描述功能不实用的设置。 最后Linus有另一个想法，即最糟糕的错误往往潜伏在维护不善的模块中。例如，上面提到的DCCP模块很少被使用并且无人维护。如果维护良好的模块标有特殊标志，则可以将非特权自动模块限制为仅对这些模块。这将阻止一些更复杂的模块的自动加载。不过这个想法确实提出了一个没人问的问题：当一个模块不在被维护时，谁会很好的维护它以去除“维护良好”的标志。 无论如何，如果Kees Cook提出的计划成立，该标志可能不会立即添加。他建议从启用警告的request_module_cap()方法开始。将为那些可以使用它的人添加每个进程的标志，但限制自动加载的全局按钮不会。最终有可能摆脱非特权模块加载，但这将是未来的目标。短期利益有关如何实际自动加载的更好信息，以及现在想要收紧的管理员提供每个进程的选项。 这段话强调了围绕内核强化工作的基本张力之一。很少有人反对更安全的内核但是一旦加固工作可以破坏现有系统，事情就会变得困难——而且情况通常如此。面向安全的开发人员经常对内核社区抵制对用户可见的影响的加固工作而感到沮丧，而内核开发者对那些会导致错误报告和用户不高兴的改动却没有什么同情心。这些挫折感在这次讨论的开发者都对达成一个所有人都有效的解决方案感兴趣。","categories":[{"name":"Linux","slug":"Linux","permalink":"http://jerryblogs.com/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://jerryblogs.com/tags/Linux/"}]},{"title":"Docker容器和虚拟机的性能评估","slug":"docker-vm-compare","date":"2022-05-25T10:35:17.000Z","updated":"2023-04-20T13:39:18.675Z","comments":true,"path":"2022/05/25/docker-vm-compare/","link":"","permalink":"http://jerryblogs.com/2022/05/25/docker-vm-compare/","excerpt":"","text":"文章信息作者：Amit M Potdara, Narayan D Gb, Shivaraj Kengondc, Mohammed Moin Mulla原文链接：https://reader.elsevier.com/reader/sd/pii/S1877050920311315 摘要服务器虚拟化是IT企业广泛使用的技术革新。虚拟化提供了一个在云上运行不同操作系统的服务的平台。它有利于在单个基本的物理机甚至于形式为hypervisor或容器里构建多个虚拟机。为了承载许多微服务应用，这个新兴的技术提出了一个模型，该模型由较小的单个部署服务执行的不同操作组成。因此，用于低开销的虚拟化技术的需求正在迅速发展。有许多轻量级的虚拟化技术，docker是其中之一，它是一个开源的平台。这项技术允许开发者和系统管理员使用docker引擎构建、创建和运行应用。本论文使用标准基准测试工具如Sysbench、Phoronix和Apache benchmark，这写工具包含CPU性能，内存吞吐量，存储读写性能，负载测试和运行速度测试。 前言在近些年，对云计算的关注正在增加。It行业在Xen、HyperV、VMware vSphere，KVM等等的出现上开发了许多技术，这些技术统称为虚拟化技术。要在同一虚拟机上部署多个应用程序，需要组织和隔离应用程序依赖。由于虚拟化，多个应用程序可以在同一个物理硬件上运行。虚拟化技术的缺点是：虚拟机体积大，由于运行多个虚拟机导致性能不稳定，启动过程需要很长时间才能运行，虚拟机无法解决可管理性，软件更新和持续集成持续交付等难题。这些问题导致了一种称为容器化的新进程的出现，进一步导致了操作系统级别的虚拟化，而虚拟化则将吸收带到了硬件级别。容器使用共享相关库和资源的宿主机操作系统。它更有效，因为它没有guest Os。在宿主机内核上，可以处理特定于应用程序的二进制文件和库，从而使执行速度非常快。容器是在Docker平台的帮助下形成的，该平台结合了应用程序及其依赖。这些应用总是在隔离空间中，在操作系统内核之上运行。Docker这种容器化功能可确保环境支持任何相关应用程序。在这项工作中，为了量化和对比虚拟机管理程序的虚拟机和Docker容器上的应用程序，进行了一系列实验。这些测试有助于我们了解两种主要的虚拟化技术（容器和虚拟机管理程序）的性能影响。本文组织如下：第2节给出了背景相关研究和关于技术和平台的简要说明。第3节介绍了用于了解性能比较的方法。第4节中，介绍了基准测试结果。最后，在第5节中提供了结论和未来的工作。 背景研究Docker容器化是一种技术，它将应用程序、相关依赖项和组织起来以容器的形式构建的系统库。构建和组织的应用程序可以作为容器执行和部署。这个平台被称为Docker，它确保应用可以工作在每个环境。它也自动执行将部署到容器中的应用程序。Docker在容器环境中附加了一个额外的部署引擎层，应用程序在其中执行和虚拟化。为了有效的运行代码，Docker有四个主要部分：Docker容器，Docker Client-Server、Docker镜像和Docker引擎。以下各节将对这些组件进行详细说明。 Docker引擎Docker系统的基本部分是Docker引擎，这是一个客户端-服务器工作模式的应用程序，它安装在主机上，具有以下组件： Docker Daemon：一种长时间运行的程序（Docker命令）有助于创建，构建和运行应用程序； RestApi被用来与docker daemon进行通信； 客户端通过终端发送请求到docker daemon来访问操作。 Docker客户端-服务器Docker技术主要是指客户端-服务器架构。客户端主要与Docker守护进程通信，Docker守护进程充当主机中存在的服务器。守护进程作为运行，构建和分发容器的三个主要进程。Docker容器和守护程序都可以放在一台机器中。 Docker镜像Docker镜像通过两个方法被构建。主要的方式是在一个只读模板的帮助下构建镜像。该模板包含基本镜像，甚至它可以是操作系统比如centos，Ubuntu16.04或者fedroa或者任何其他基本的轻量级的操作系统镜像。通常，基本镜像是每个镜像的基础。每当从头开始构建新镜像时都需要一个基本镜像。这种类型的创建新镜像成为“提交更改”。下一种方法是创建一个Dockerfile，其中包含创建Docker镜像的所有说明。当从终端执行Docker的构建命令时，将使用Dockerfile中声明的所有依赖项创建镜像，此过程称为构建镜像的自动化方法。 Docker容器Docker容器由Docker镜像创建。要以受限方式运行应用程序，应用程序所需的每个套件都将由容器保存。可以依据应用程序或软件的服务要求创建容器镜像。假设一个包含Ubuntu和Nginx服务器的应用程序已经被添加到到Dockerfile中，使用命令docker run，创建包含Nginx服务器的Ubuntu Os镜像的容器并开始运行。 虚拟机和Docker容器之间的比较Docker有时被称为轻量级虚拟机，但是它不是虚拟机。如下表1所述，它们的底层技术在虚拟化技术方面的差异。图2显示了虚拟机和Docker容器的体系结构。 表1 虚拟机 Docker 容器 隔离进程级别 硬件 操作系统 操作系统 Separated 共享 启动时间 长 短 资源使用情况 多 少 预构建的镜像 难以寻找和管理 已可用于主服务器 自定义配置镜像 更大，因为包含整个操作系统 更小，只有主机操作系统上的Docker引擎 流动性 易于迁移到新的主机操作系统 销毁和重建替代迁移 创建时间 更长 秒级 相关工作正在开发的虚拟机的使用在组织中很常见。虚拟机广泛用于执行复杂的任务，例如Hadoop。但是，用户甚至使用虚拟机来启动小型的应用程序，这使得系统效率低下。需要启动一个轻量级的应用程序，它更快，使系统更加高效。Docker容器时提供轻量级虚拟化的技术之一，这激励我们执行后台工作。[3]作者从CPU性能，内存吞吐量，磁盘IO和操作速度角度概述了虚拟机和Docker容器的性能评估。[4]作者专注于再HPC集群中实现Docker容器。在本文的后半部分，作者解释了选择容器模型的不同实现方法以及LNPACK和BLAS的使用。在[5]中，作者讨论了轻量级虚拟化的方法，其中解决了容器和unikernel的问题。此外本文还讨论了方差分析检验的统计评估及使用Tukey方法对所收集的数据进行事后比较。作者还讨论了用于比较单核和容器不同基准测试工具。静态HTTP服务器和键值存储参数用于应用程序性能的实现分析，这些分析部署在云上。Nginx服务器用于HTTP性能，为了测量获取和设置操作，使用redis基准测试。[6]中的作者讨论了使用KVM、Docker和OSv的基准测试应用程序的评估。在[7]中，作者讨论了关于虚拟机和容器化技术的简短调查。他还讨论了DOCKER和Docker性能以及各种参数CPU、内存吞吐量、磁盘IO。在[8]中，作者讨论了Docker架构的基本概念，Docker的组件，Docker镜像，Docker register，docker客户端-服务器架构。讨论了Docker和虚拟机的区别。[9]中，作者解释了基于容器的云技术与一组不同参数的性能比较。本文提供了有关基于OpenStack的云部署的使用情况的信息，并考虑进行比较。用于性能比较的平台时docker，LXC和flockport。[10]中的作者讨论了虚拟机和Docker在各种参数（如CPU、网络、磁盘和两个真正的服务器应用程序redis和Mysql）方面的性能比较。 方法论在本节中，使用基准测试工具执行KVM和Docker的评估。以下用于性能评估的基准测试工具时Sysbench，Phoronix，Apache benchmark。这些基准测试工具可以测量CPU性能、内存吞吐量、存储读取和写入、负载测试和操作速度测量。两台HP服务器用于有关各种参数的性能评估，因为其中一台服务器用作安装在主机操作系统之上的虚拟机，而除了此docker引擎之外，guest Os安装在虚拟机之上，另一台服务器作为裸机，主机操作系统Ubuntu 16.04和docker引擎都安装在其上。所有测试均在配备两个因特尔E5-2620 v3处理器的惠普服务器上执行，频率为2.4GHz，共12个内核和64GB的RAM。Ubuntu16.04 64位Linux内核3.10.0用于执行所有测试。为了保持一致性和统一性，使用相同的操作系统。Ubuntu16.04作为Docker容器的基础镜像。为虚拟机配置了12 vCPU和足够的RAM。图3介绍了使用各种基准测试工具不同虚拟化技术的评估方法。 结果和讨论本节讨论虚拟化技术的性能分析。结果分为四个小节。4.1描述所有CPU测量、内展示在4.2和4.3节的存吞吐量和存储读写测量值。4.4节描述负载测试分析。4.5节描述操作速度测量，包括两个测试：八皇后问题和八个谜题问题。最后再4.6节中，执行了统计t检验分析。 CPU性能计算性能可以通过系统在给定时间执行的操作数或特定任务的完成时间来衡量。结果主要取决于分配给服务器的虚拟CPU内核数。测试CPU性能比较是通过以下工具sysbench、phoronix和apache benchmark。 最大质数运算在Sysbench工具测试找出执行最大素数所需的时间，操作的最大质数位50000，时间为60秒，4个线程操作。从下图观察，与VM相比，docker容器执行操作所需时间要少的多。这是由于虚拟机中存在虚拟机管理程序，因此执行需要更多的时间。 7 ZIP 压缩测试7 zip是一款开源的文档存档器，用于将一组文件压缩到称为压缩包的容器中。LZMA基准测试的两个测试，压缩和解压LZMA方法。此测试测量使用7 zip压缩文件所需的时间。用于压缩测试的文件大小为10GB。根据获取的结果，当使用大量文件执行压缩时，Docker容器的性能要比VM好很多。 内存性能RAM speed&#x2F;SMP（对称多处理）是一种缓存和内存基准测试工具，用于测量虚拟化技术（即 Docker 和虚拟机）的 RAM 速度。图 6.表示虚拟化技术之间的 RAM 速度比较。测试RAM速度时，考虑了以下两个主要参数。INTmark 和 FLOATmark 组件用于 RAM 速度 SMP 基准测试工具，该工具可在读取和写入单个数据块时测量最大可能的缓存和内存性能。INTmem和FLOATmem，它们是合成模拟，但与计算的现实世界紧密平衡。每个子测试都由四个子测试（复制，缩放，添加，三元组）组成，以测量内存性能的不同方面。数据从一个内存位置到另一个内存位置的传输是通过复制命令完成的，即（X &#x3D; Y）。写入前的数据修改乘以一定的常量值是通过scale命令完成的，即（X &#x3D; n * Y）。数据从第一个内存位置读取，然后在调用命令 ADD 时从第二个内存位置读取。然后将结果数据放在第三位（X &#x3D; Y + Z）。三元组是添加和缩放的组合。从第一个内存位置读取数据以进行缩放，然后从第二个位置相加以将其写入第三个位置（X &#x3D; n*Y + Z）。图 6.显示相对于 RAM 速度 SMP 测试的内存性能。 磁盘IO性能测试硬盘性能，使用 IOzone 基准测试工具进行性能分析。为了测试系统的写入和读取等操作，使用了1MB的记录大小和4GB的文件大小。从图 7 中可以推断出，与虚拟机相比，Docker 的性能要好得多。VM 的磁盘写入和读取操作减少了 Docker 容器的一半以上（约 54%）。图 7.显示了虚拟机和 Docker 的磁盘性能。 负载测试对于负载测试性能比较，使用Apache基准测试工具，其中它测量给定系统每秒可以容忍的请求数。执行python程序以使用Apache Benchmarking工具测试负载。图 8.表明 VM 的吞吐量分析比 Docker 的吞吐量分析要少得多。这是因为虚拟机中的网络延迟高于 Docker 中的网络延迟。分析表明，Docker容器在每秒处理请求数方面优于虚拟机。 运行速度测量八位女王的问题将八位女王放在8×8的棋盘上，这样他们都没有互相攻击。该测试测量解决问题所需的时间。Eight queen程序是用python编写的，决定了系统的计算性能。图 9.显示了 Docker 和虚拟机的计算性能。根据执行时间，docker 容器解决问题所需的时间更少，而虚拟机需要更长的时间。八拼图测试：拿4×4板，8块瓷砖和一个空的空间。使用空白空间，排列与最终配置相匹配的图块数量是主要目标。它可以将四个相邻的操作（右，左，下和上）磁贴滑入空白区域 - 该测试测量解决问题所需的时间。Eight拼图程序是用python编写的，决定了系统的计算性能。图 10.显示了 Docker 和虚拟机的计算性能。根据执行时间，docker 容器解决问题所需的时间更少，而虚拟机需要更长的时间。 t-test分析t 检验统计测量用于确定两个组的方法之间是否存在关键区别，这两个组可能在相关特征中连接。在下面的结果演示中，统计推理技术 t-test 已被用于证明 docker 容器的性能显著优于虚拟机。此外，阈值的置信水平α为 0.05。两组数据的概率可以通过取 t 统计量、t 分布值和自由度来确定。在分析部分使用原假设 （H0） 和备择假设 （H1） 约定。在进行检验分析时，考虑原假设为真，这是进一步统计分析的假设。然后，测试的结果可以证明，如果 H0 为真，则假设可能是错误的。需要测试 Docker 容器执行操作所需的时间是否比虚拟机少。样本检验包括 10 个关于查找素数操作的实验。表3显示了为分析部分采集的10个样本。发现执行虚拟机操作所花费的平均时间为 153.5 秒。现在需要检查 Docker 容器的平均时间是否小于虚拟机。t 统计量 （t） 的等式为 。 结论Docker 容器是一种新兴的轻量级虚拟化技术。这项工作评估了两种虚拟化技术，即 Docker 容器和虚拟机。在虚拟机和基于 Docker 容器的主机上，从 CPU 性能、内存吞吐量、磁盘 I&#x2F;O、负载测试和操作速度测量等方面进行性能评估。据观察，Docker容器在每次测试中的表现都优于VM，因为虚拟机中存在QEMU层使其效率低于Docker容器。容器和虚拟机的性能评估是使用Sysbench，Phoronix和apache基准测试等基准测试工具执行的。作为未来的工作，我们计划在Docker中调度容器，同时开发更安全的容器变体，这将减少安全约束。 References[1] Docker. https://docs.docker.com/ 2019 [Online; Accessed 24-03-2019][2] Shivaraj Kengond, DG Narayan and Mohammed Moin Mulla (2018) “Hadoop as a Service in OpenStack” in Emerging Research in Electronics, Computer Science and Technology , pp 223-233.[3] C. G. Kominos, N. Seyvet and K. Vandikas, (2017) “Bare-metal, virtual machines and containers in OpenStack” 20th Conference on Innovations in Clouds, Internet and Networks (ICIN), Paris, pp. 36-43.[4] Higgins J., Holmes V and Venters C. (2015) “Orchestrating Docker Containers in the HPC Environment”. In: Kunkel J., Ludwig T. (eds) High Performance Computing. Lecture Notes in Computer Science, vol 9137.pp 506-513[5] Max Plauth, Lena Feinbube and Andreas Polze, (2017) “A Performance Evaluation of Lightweight Approaches to Virtualization”, CLOUD COMPUTING: The Eighth International Conference on Cloud Computing, GRIDs, and Virtualization.[6] Kyoung-Taek Seo, Hyun-Seo Hwang, Il-Young Moon, Oh-Young Kwon and Byeong-Jun Kim “Performance Comparison Analysis of Linux Container and Virtual Machine for Building Cloud,” Advanced Science and Technology Letters Vol.66, pp.105-111.[7] R. Morabito, J. Kjällman and M. Komu, (2015) “Hypervisors vs. Lightweight Virtualization: A Performance Comparison”, IEEE International Conference on Cloud Engineering, Tempe, AZ, pp. 386-393.[8] Babak Bashari Rad, Harrison John Bhatti and Mohammad Ahmadi (2017) “An Introduction to Docker and Analysis of its Performance” IJCSNS International Journal of Computer Science and Network Security, VOL.17 No.3, pp 228-229.[9] Kozhirbayev, Zhanibek, and Richard O. Sinnott. (2017) “A performance comparison of container-based technologies for the cloud”, Future Generation Computer Systems, pp: 175-182.[10] Felter, Wes, et al. (2015) “An updated performance comparison of virtual machines and linux containers”, IEEE international symposium on performance analysis of systems and software (ISPASS). pp:171-172.[11] Sysbench. https://wiki.gentoo.org/wiki/Sysbench 2019 [Online; Accessed 24-03-2019][12] Phoronix Benchmark tool. https://www.phoronix-test-suite.com/ 2019 [Online; Accessed 24-03-2019][13] Apache Benchmark tool. https://www.tutorialspoint.com/apache_bench/ 2019 [Online; Accessed 24-03-2019]","categories":[{"name":"Docker","slug":"Docker","permalink":"http://jerryblogs.com/categories/Docker/"}],"tags":[{"name":"Docker","slug":"Docker","permalink":"http://jerryblogs.com/tags/Docker/"}]},{"title":"kubernetes中的网络策略：性能评估和安全分析","slug":"k8s-network-policy","date":"2022-05-25T10:31:00.000Z","updated":"2023-04-20T13:39:26.155Z","comments":true,"path":"2022/05/25/k8s-network-policy/","link":"","permalink":"http://jerryblogs.com/2022/05/25/k8s-network-policy/","excerpt":"","text":"文章信息作者：Gerald Budigiri; Christoph Baumann; Jan Tobias Mühlberg; Eddy Truyen; Wouter Joosen原文：https://ieeexplore.ieee.org/document/9482526 摘要具有超高可靠性要求和低延迟要求的5G应用需要在移动网络中采用边缘计算解决方案。根据终端用户和第三方公司的需求，像k8s这样的容器编排框架已经进一步成为动态部署边缘应用的首选标准。不幸的是，复杂的网络和安全问题被强调为阻碍行业成功采用容器技术的挑战。安全挑战因（错误）概念而加剧，即安全的荣期间通信以牺牲性能为代价，但是这样中需求对于5G边缘计算用例来说都至关重要。为了追求低开销的安全方案，本论文研究网络策略，即k8s用于控制租户之间网络隔离的概念。我们评估Calico和Cilium基于ebpf的解决方案的性能开销，分析网络策略的安全性，突出网络策略的安全威胁，并概述相应的先进解决方案。我们的评估表明，网络策略时适合低延迟容器间通信的低开销安全解决方案。 I.前言5G的出现为几个新的超可靠低延迟通信（URLLC）应用和用例打开了大门，如虚拟&#x2F;增强现实（VR &#x2F; AR），车辆到一切（V2X）和远程手术（RS），有望显着增加对计算和通信资源的需求。即使硬件功能最近有所进步，这些应用的严格性能要求仍然无法与实际的设备和网络功能相匹配[1]。为了解决这种不匹配问题，5G提供商采用了边缘计算（边缘计算是指在用户或数据源的物理位置或附近进行的计算，这样可以降低延迟，节省带宽。）和网络功能虚拟化（NFV）等几种技术概念。借助NFV，边缘计算平台虚拟化了网络功能模块，并将云计算能力扩展到终端用户附近的边缘设备，从而在多租户边缘生态系统中提供灵活性和敏捷性。云原生计算趋势满足了这一发展，其中应用程序由使用无状态API相互交互的微服务组成。Kubernetes等灵活的编排框架之上使用轻量级和可以指的容器，而不是更资源密集型和启动缓慢基于VM的解决方案，使得云原生网络功能（CNF）成为5G边缘计算URLLC应用的更好选择。现在微服务架构正在被研究为边缘NFV的可能解决方案。然而，复杂的网络和安全问题被强调为工业中容器采用的挑战。在多租户边缘计算云环境中，潜在的安全漏洞具有更高的严重性，在这些环境中，需要隔离属于不同房的微服务，以便他们只能在必要时进行交互。此外，对于RS和V2X等任务关键性5G应用，通信性能和安全性都不应受到影响。因此，必须为容器间通信找到低开销的安全解决方案。虽然行业和研究界已经做出了许多努力，来提高容器安全性，但其中大多数工作都集中在容器镜像，运行时，内核操作系统和k8s配置上，很少或根本没有考虑低延迟应用程序中的网络安全问题。k8s提供允许限制容器间通信的网络策略。虽然缺乏入侵检测等现代防火墙的高级功能，但不同网络策略仍然提供了合理的网络安全水平。在多租户环境中，它们通过将流量限制为仅允许互相通信的微服务，在租户之间提供可配置的网络隔离。虽然通过k8s网络策略API进行定义，但是策略的实施由自定义的容器网络接口（CNI）插件处理。之前的工作比较了不同CNI插件，并讨论了他们在多租户中的使用。相比之下，本文首次专门研究了k8s网络策略的性能开销和安全影响，并探讨了网络策略在边缘保护URLLC应用程序的适用性。我们做出以下贡献： 性能：我们评估了合适的CNI插件并且表明CNI插件的网络策略产生的性能开销可以忽略不计，该开销仅随策略数量和不同策略配置方式而略有不同。 安全：我们定义一个针对网络策略的攻击模型并且分析相应的威胁和漏洞。此外我们提出了合适的最先进的低延迟解决方案，以应对这些威胁。 II.背景与动机在介绍这些结果之前，我们概述了k8s的网络策略的技术基础，功能和挑战。本节介绍k8s、k8s中的网络策略，并且为什么它们在多租户边缘平台中的重要性。此外，我们就CNI插件和网络策略支持进行比较，并解释Calico中的策略实施。 A.Kubernetes(k8s)k8s是容器编排的事实标准，它支持部署、拓展和管理容器化微服务应用程序并提供网络概念和对大规模互联微服务的支持。单个微服务应用程序在容器中运行，一个或多个紧密耦合的容器在Pod中运行，而pod在节点上运行，通过k8s主节点上运行的API服务进行控制。由于Pod是临时的，在拓展过程中动态启动或终止，因此服务对象用于为pod提供稳定的端点。此类和其他配置对象与控制k8s的编排过程的状态信息一起存储在分布式etcd数据库中。 B.kubernetes 网络策略在多租户平台，保护边缘用户的隐私是重要的。尽管k8s提供集群范围的命名空间用来提供隔离以进行管理和资源配额管理，单词支持不足以避免网络遍历。然而缺乏足够的网络分段是是一个一个经常被引用的高风险漏洞，该漏洞已被Equifax数据泄露等大型网络犯罪所利用。网络策略通过显式声明允许和拒绝的连接，帮助提供限制pod之间（在和&#x2F;或跨命名空间）以及pod与外部网络之间的流量所需的护栏。网络策略规范由podSelector组成，用于指定将受策略约束的pod，以及用于指定策略类型（入口或出口）的策略类型。入口规则指定允许入站的流量到目标Pod，出口规则指定允许的来自目标pod的出站流量。每个规则都由一个NetworkPolicyPeer组成，用于通过无类域间路由（CIDR）表示法选择连接另一端允许流量的pod，该表示法指定IP地址块，命名空间或pod标签选择一个NetworkPolicyPort，它允许显示指定可能与Pod通信的端口或网络协议。网络策略可能是累加的，如果多个策略选择一个Pod，则流量将限制在这些策略的入口&#x2F;出口规则并集所允许的范围内。 CNI插件和网络模式由于没有内置功能来实施网络策略，k8s依靠CNI插件来实施。CNI插件作为附加组件安装，该附加组件可以作为容器运行，也可以依赖开源k8s组件。虽然存在许多CNI插件，但只有Calico和Cilium支持使用ebpf实施网络策略，ebpf是iptables的高性能替代方案。事实上，再将ebpf与iptables进行比较时，我们观察到延迟减轻了0.7到0.8倍，节点间和节点内场景的吞吐量分别提高了3.5倍和1.2倍。测量是在封闭实验室的OpenStack测试平台上使用Calico进行的（参见III-A和III-B部分，了解测试平台的实验设置和规格）。Calico和Cilium都使用流量控制钩子，在Pod的虚拟以太网接口的入口和出口处附加ebpf程序。根据主机模式评估Calico ebpf和Cilium ebpf性能，在该模式下，基准测试在OpenStack VM实例上运行。结果表明，尽管ebpf数据平面有优势，但是CNI插件（可能还有整个K8s架构）的开销仍然很大，特别是对于节点间的通信。结果进一步表明，Calico ebpf在节点间场景中的表现明显优于Cilium ebpf。这是因为Cilium默认以隧道模式运行，这会引发封装标头，而Calico在每个节点上使用Linux内核路由支持来提供纯第三层网络解决方案，从而降低开销。因此，Calico ebpf被用于本文其余性能评估。Calico和主机节点内测量在裸机上重复，其中主机结果在延迟和吞吐量方面分别高出8.7%和10.7%，而OpenStack则为33%和31%。OpenStack上较高的开销表示VM的基于OpenStack的网络配置与Pod基于CNI的网络配置之间可能存在负面干扰。 D.Calico eBPF 策略实现Calico CNI插件将Felix守护程序部署到每个节点，该守护进程将内核路由编程到本地Pod并且使用边界网关协议分发路由信息。Felix通过API服务器从K8s etcd获取网络策略定义，并且将他们转换为BPF程序，这些程序被加载到内核中，并附加到每个Pod的veth接口的tc钩子上。只有来自Pod标签匹配的网络策略规则才会被附加，从而实现可拓展的实现，Linux的conntrack（Linux内核网络栈的一个核心功能，允许内核跟踪所有的逻辑网络连接或流）对此进行了强化（见本目录下另一篇关于conntrack的文章）。Calico还以自定义的资源定义（CRD）的形式实现自己的网络策略模型，该模型提供了拓展的操作范围，例如根据其顺序确定规则的优先级。Felix进一步使用快速数据路径（xDP）在节点上实现数据包过滤，以防止拒绝服务（DoS）攻击。 E.多租户网络平台中的网络策略基于微服务的应用程序可以包含数百个微服务，像netfilter这样的公司使用500多个微服务来运行其电影流系统。在多租户边缘平台中，不同的应用程序可能来自不同的提供商，需要强大网络隔离，因此必须遵循最小权限原则，为微服务之间每个预期的租户件交互显式指定网络策略，该原则旨在减少攻击者的安全攻击接口。同样所有租户内容器通信都需要显式允许策略。因此多租户平台中的策略数量可能会随着租户应用程序的数量和大小急剧增加。因此，对于5G边缘部署，不仅要对k8s的网路性能进行基准测试，还要评估网络策略的性能开销。 III.间接费用考核本节总结了各种条件下节点内和节点间场景下Calico网络策略的网络时延开销的评估结果。 A.基准和架构评估我们使用netperf的TCP流和请求-响应模式分别进行吞吐量和端到端的延迟测量。我们将netperf配置为测试长度为120秒，目标是99%的置信度，即测量的平均值在实际平均值的正负2.5%以内。我们不报告吞吐量和CPU利用率结果，因为他们遵循与延迟相同的模式。 B.实验设置用于运行所有实验的测试平台是私有openStack云（Liberty版本）的隔离部分，如图3所示。我们从公共边缘云中使用OpenStack开始，通常最好在VM中运行容器以保护云提供商的资产。OpenStack云由主从架构组成，具有两台控制器机器，以及可调度虚拟机的droplets，这些droplets具有Inter（R）Xeon（R）CPUE5-2650 2.00GHz 处理器和带有Ubuntu xenial的64GBDIMM DDR3内存，每个droplets都有两个10Gbit网络接口。这些droplets有16个CPU内核，其中2个是为OpenStack云的操作保留的。有一个主节点和两个工作节点组成的k8s集群是使用kubeadm部署的，运行k8s版本1.19.2。所有节点都有4个vCPU和8GBRAM，并且部署在同一个物理droplets上，以消除网络延迟的变化。此外，每个节点的vCPU内核专门固定在属于droplets同一主板插槽的物理内核上。 增加策略数量的效果：我们创建了20个命名空间，每个命名空间包含5个微服务（Pod）。三个Pod用于性能测量；一个本地pod和两个远程Pod（每个场景一个，即节点内和节点间）并且所有的pod都被分配了相同数量的策略。Calico提供了一个policyOrder的功能，用于确定策略实施的优先级，较低的顺序优先。最低顺序是不允许任何Pod间流量的策略，然后是允许与不相关的Pod进行通信的策略，直到达到所需数量的策略。然后将最高顺序分配给允许三个选定Pod之间通信的策略。然后，网络策略的数量从零到2000不等，结果如图四所示（accross 100 Pod）。对于所有已评估数量的网络策略，我们观察到对性能的显著影响。 网络策略：表1涵盖了各种k8s网络策略功能的不同策略配方。虽然对表中编号5、11、13和14的每个配方都进行了单独评估，但是仅对具有相似结构的配方（即2、8、9和12）进行了一次测试。对于其余的配方，即1、3、4、6、7和10，拥有许多的策略是不合理的，因为对于GlobalNetworkPolicy，这些策略中所需的流量隔离功能可以通过命名空间中的一个策略甚至整个集群中的一个策略来实现。评估提供了与图四（100个Pod）中观察到的结果是类似结果。 # Deny traffic # Allow traffic 1 to a pod 8 to a pod 2 limiting traffic to a pod 9 to a pod from all NSs 3 to a NS 10 from a NS 4 from other NSs 11 from pods in another NS 5 from a pod 12 from external clients 6 non-whitelisted from a NS 13 only to a port 7 external egress 14 using multiple selectors C.网络策略的可拓展性从上述所有评估中可以看出，在执行网络策略方面没有发现明显的开销。我们可以将此归因于分散的Calico实现，其中仅在每个pod的veth接口上评估相关策略。即使集群中可能有2000个策略，也只会在那些选择器与通信pod匹配的策略才会在这些pod上进行评估。此外Calico的order功能是在策略创建或修改时而不是数据包时强制执行的。为了评估网络策略的可伸缩性，我们在集群中部署了三个pod，并对其进行了标记，使其与集群中所有策略相匹配。如图四的结果（accross 3 pod）仍未显示显著的策略开销，至少对于节点内流量而言。这表明Calico使用ebpf来实施网络策略是可拓展的。其原因可能是Calico进检查新流中第一个数据包的策略之后，conntrack会自动允许同一个流的后续数据包而不是重新检查每个数据包。 IV.网络策略安全分析网络策略限制跨不同pod的容器之间的入站和出站网络连接，拒绝任何未明确允许的连接。下面我们将讨论在同一k8s集群中托管不同边缘应用程序的微服务边缘平台提供商所感知的容器通信网络策略的安全隐患与挑战。 A.攻击模型我们认为攻击者已破坏其中一个边缘应用程序命名空间的微服务。这可能是由恶意或受损的边缘应用程序提供商、通过破坏在平台上部署服务的软件供应链或利用外部可访问微服务中的漏洞来实现的。假设受感染的应用程序在非特权（非root）容器中运行，则攻击者会受到适用于该容器及其相应Pod的功能和资源限制的限制。攻击者仍可以执行任意代码并访问容器的所有允许的内存区域、文件和网络连接。因此，攻击者可能会连接到集群中其他可访问的微服务，并利用任何存在的漏洞来破坏他们。另一方面，我们要求集群虚拟化基础架构值得信赖且经过充分强化，以便攻击者无法逃离其容器沙箱并获得其托管节点上的root权限。攻击者的目标是通过破坏其他服务（尤其是那些有权访问敏感信息或任务关键性服务以破坏其功能的服务）在集群中传播。次要目标是不被发现。然后，了解任何Pod的网络策略以及相应的可访问子网可以帮助攻击者避免触发警报。从这个意义上说，适用于集群的网络策略可以被视为敏感信息。 B.网络策略的安全影响如果根据最小权限原则定义的策略，仅允许必要的连接，则网络策略是阻止上述攻击者的有效工具。由此产生的网络分段减少了攻击面，保护了可能易受攻击的共同托管的应用程序，并防止任何横向攻击者移动。然而，网络策略并不是灵丹妙药。攻击者仍可以滥用允许的连接，并以相应的微服务为目标，以实现远程执行代码或泄露敏感信息。虽然策略允许阻止与某些端口的连接，但表1中的简单规则不对允许的通信节点之间的流量提供任何细粒度的控制。其他解决方案，如网络监控和异常检测，可用于检测和阻止此类攻击。识别出可疑参与者后，可以使用网络策略将其隔离在集群的隔离部分中。但是新颁布的网络策略仅对新连接强制执行，不会切换现有的连接。 C.网络策略安全漏洞和威胁使用网络策略本身有几个陷阱。下面我们报告了常见的漏洞和潜在攻击以及文献中可用的补救措施。 1）：配置错误：StackRox的一项调查显示，91%的K8s用户遇到过安全问题，其中67%归因于配置错误。此外配置错误仍被认为是云中数据泄露的主要原因。配置网络策略时，存在大量机会，这些配置村在大量薄弱或错误的机会，这些配置无疑会威胁到容器网络隔离。这些包括错误的Pod标签规范、拼写错误等手动错误，甚至在编写后完全忘记强制实施网络策略。如果禁用了网络策略，k8s不会发出警告，而只是接受并静默忽略配置。 最先进的解决方案：开放策略代理（OPA）通过在Pod创建期间通过自定义准入控制器对网络策略强制实施适当的OPA要求，有助于减低错误配置风险，例如网络策略中的拼写错误或疏忽、忘记强制执行甚至错误删除他们。但是，OPA不会验证配置的策略要求是否建立更高级别的安全目标，如网络隔离。 2）弱策略设计：所有安全策略（包括但不限于网络策略）都需要根据最小权限和零信任等原则进行设计和配置。应明确允许容器之间的通信，并将其保持在应用程序完成其工作所需的最低限度。例如，不应无意中向只需要从数据库读取的应用程序授予写入权限。指定网络策略时，应为应用程序容器提供一组权限，每组权限都是最小化为服务所需最小权限集。允许策略应允许最低限度，例如通过NerworkPolicyPort，而拒绝策略应该是最大程度地阻止所有未经授权的流量。但是，编写最低特权策略是一项艰巨地任务，因为k8s中典型应用程序需要服务之间、从外部（入口）到外部端点（出口）的数百个连接。 最先进的解决方案：实施网络策略时规则中，OPA可以将受保护的容器允许入口规则限制为特定值，以避免授予其他容器访问权限。BASTION通过确保容器的连接仅限于自身与组成服务所需的容器之间的相互依赖关系，对容器强制实施最低特权网络访问。 3）租户管理员：虽然边缘基础结构通常是受信任的，但是租户及其管理员可能会提出某些风险。具有足够权限的流氓管理员可能会与攻击者串通，以削弱网络集群策略和隔离，从而悄悄地允许非法网络连接。特别是，常规k8s网络策略绑定到租户空间，并且可以由租户管理员更改。 最先进的解决方案：在多租户环境中，必须遵循控制平面中的身份验证、访问控制和审核地最佳实践，以限制恶意租户和内部攻击的影响。租户管理员更改集群策略的问题在Calico中通过全局策略和优先级的概念得到解决。与配置错误一样，OPA还提供了防止使用非法网络策略启动容器的保护。 4）特权网络：不幸的是，容器通常以不安全的设置运行，授予不必要或无意的权限，从而给节点操作系统和节点上其他容器带来风险。网络特权容器直接在LAN网络上公开，绕过Pod网络接口和网络策略。这样容器可以通过分配给他们的网关IP地址或通过猜测相应的IP地址来访问其他节点，从而损害所需的网络隔离。 最先进的解决方案：[4]中介绍了一个高性能的安全实施网络堆栈，它通过对共享主机网络命名空间的启用了网络特权的容器实施细粒度访问控制来解决此漏洞。此外尽管Calico不提供安全机制来防止主机网络命名空间滥用，但它可以防止特权容器滥用虚拟网关IP地址。Pod安全策略可确保不会生成具有无意功能或访问权限的容器。他们也可以由OPA强制执行。 5）易受攻击的实现：在k8s中，网络策略的实施取决于CNI插件组件。这些组件及其依赖项本身可能包含漏洞，这些漏洞可能会危及策略实施或允许攻击者拦截和重定向网络流量，例如CVE-2019-9946、CVE-2020-10749 和 CVE-2020-13597。 最先进的解决方案：使软件保持最新状态可以减轻漏洞的影响，但不能解决根本问题。形式验证技术和安全的编程语言有助于提高软件的安全性。 6）ebpf漏洞利用：在我们的设置中，Calico&#x2F;node代理将网络策略转换为ebpf程序以进行实施。因此，需要在节点上启用ebpf内核功能才能使用网络策略，这可能会增加节点的攻击面。在Linux4.4之前，所有bpf()命令都要求调用方具有CAP SYS ADMIN功能，然而，目前非特权用户可以通过将有限的ebpf程序附加到他们拥有的套接字来创建和运行这些程序。如 CVE-2020-8835 中所示，此功能不仅被利用来在内核内存中执行越界读取和写入，而且还被利用来实现权限提升 [31]。 最先进的解决方案：通过将内核完全禁用bpf()系统调用的非特权访问，将内核无特权bpf禁用的系统设置为1，可以缓解此类威胁。ebpf内核验证器可以限制从cbpf程序访问哪些内核函数和数据结构。Seccomp-BPF限制了用户空间程序可用的系统调用和参数集。 7）泄露网络策略：如上所述，对于热衷于避免检测的攻击者来说，网络策略本身可能是最有价值的信息。获取它们的最简单的方法是从k8s API服务器或etcd请求他们，但用户平面攻击者通常无法访问这些API。具有监视控制平面流量能力的对手可以获取配置信息，例如网络策略（如果以明文形式发送）。攻击者还需要通过诸如[31]之类的漏洞利用来提供额外的内存读取功能以便从本地Calico&#x2F;node代理或ebpf内核工具泄露策略。无特权攻击者可能会探测网络以推断允许的连接，但这可能会破坏保持隐身的最初目标。 最先进的解决方案：如上所述，泄露策略信息的原始尝试很容易受到最佳实践的阻碍，例如为控制平面启用相互传输层安全加密，身份验证和RBAC。尽管Calico支持TLS来保护Calico组件和控制平面通信，但默认情况下，大多数提供的清单中都未配置安全性，以使Calico部署变得更容易。对于Calico，建议使用TLS与其数据存储进行通信，同时使用客户端整数和JSON Web令牌身份验证模块。出于机密性和完整性的原因，应考虑在可信执行环境（TEE）（如英特尔SGX）中运行任务关键型应用和控制平面组件，以便在每个容器应用之间的节点中创建隔离的安全区环境。英特尔SGX安全区通过加密内存隔离驻留在容器中的应用，从而保护应用内存免受恶意或特权容器以及受损节点操作系统的影响。虽然SGX在执行安全区代码时会产生性能开销，但其应用于控制平面组件以保护网络策略的机密性和完整性不会给用户平面流量增加任何开销。 V.结语边缘计算和容器化是多租户5G云环境中无疑重要的技术。然而，安全要求仍然阻碍其广泛采用，特别是在具有高性能和安全要求的5G URLLC边缘应用中，因为大多数安全解决方案都会损害性能。这使得找到一个低开销的容器安全解决方案变得至关重要。在评估了网络策略（k8s的可配置网络隔离安全解决方案）并且没有观察到明显的性能开销之后，我们证明了他们是此类应用程序的合适的安全解决方案。作为一般关注点，交互微服务应部署在同一节点上以确保最高性能，但是这会增加对不同租户的适当隔离的需求。 Acknowledgment这项研究的部分资金来自KU Leuven研究基金和弗兰德研究计划网络安全。这项研究已获得欧盟H2020 MSCA-ITN行动5GhOSTS的资助，赠款协议号814035。 引用[1] Q.-V. Pham et al., “A survey of multi-access edge computing in 5Gand beyond: Fundamentals, technology integration, and state-of-the-art,”IEEE Access, 2020.[2] H. Hawilo et al., “Exploring microservices as the architecture of choicefor network function virtualization platforms,” IEEE Network, 2019.[3] J. Watada, A. Roy, R. Kadikar, H. Pham, and B. Xu, “Emerging trends,techniques and open issues of containerization,” IEEE Access, 2019.[4] J. Nam, S. Lee, H. Seo, P. Porras, V. Yegneswaran, and S. Shin, “BASTION: A security enforcement network stack for container networks,”in 2020 USENIX Annual Technical Conf. (USENIXATC 20), 2020.[5] S. Sultan, I. Ahmad, and T. Dimitriou, “Container security: Issues,challenges, and the road ahead,” IEEE Access, 2019.[6] M. Souppaya, J. Morello, and K. Scarfone, “Application containersecurity guide (2nd draft),” NIST, Tech. Rep., 2017.[7] E. Reshetova, J. Karhunen, T. Nyman, and N. Asokan, “Security of oslevel virtualization technologies: Tech. report,” arXiv:1407.4245, 2014.[8] A. Grattafiori, “Understanding and hardening linux containers,”Whitepaper, NCC Group, 2016.[9] S. Vaucher, R. Pires, P. Felber, M. Pasin, V. Schiavoni, and C. Fetzer,“SGX-aware container orchestration for heterogeneous clusters,” in 2018IEEE 38th Conf. on Dist. Computing Syst. (ICDCS). IEEE, 2018.[10] S. Arnautov et al., “SCONE: Secure linux containers with intel SGX,”in 12th USENIX OSDI 16, 2016.[11] M. S. I. Shamim, F. A. Bhuiyan, and A. Rahman, “XI commandmentsof Kubernetes security: A systematization of knowledge related toKubernetes security practices,” in 2020 IEEE SecDev. IEEE, 2020.[12] A. Balkan, “Kubernetes network policy recipes,” https://github.com/ahmetb&#x2F;kubernetes-network-policy-recipes&#x2F;, [accessed 2021-01-28].[13] S. Qi et al., “Understanding container network interface plugins: designconsiderations and performance,” in 2020 IEEE Int. Symp. on Local andMetropolitan Area Networks (LANMAN). IEEE, 2020.[14] X. Nguyen, “Network isolation for K8s hard multi-tenancy,” 2020.[Online]. Available: https://aaltodoc.aalto.fi/handle/123456789/46078[15] “Kubernetes namespaces,” https://kubernetes.io/blog/2016/08/kubernetes-namespaces-use-cases-insights&#x2F;, [accessed 2020-11-24].[16] “How to stop the next equifax-style megabreach,” https://www.wired.com&#x2F;story&#x2F;how-to-stop-breaches-equifax&#x2F;, 2017, [accessed 2021-01-14].[17] “About eBPF,” https://docs.projectcalico.org/about/about-ebpf/, [accessed 2020-12-18].[18] “eBPF datapath,” https://docs.cilium.io/en/v1.9/concepts/ebpf/, 2020,[accessed 2020-12-18].[19] CED, “How it’s built,” https://medium.com/the-andela-way/how-itsbuilt-netflix-8f62ab329011/, 2019, [accessed 2020-12-18].[20] “Care and Feeding of Netperf 2.7.X,” https://hewlettpackard.github.io/netperf&#x2F;doc&#x2F;netperf.html&#x2F;, [accessed 2020-10-23].[21] https://github.com/falcosecurity/falco/, 2020, [accessed 2021-01-27].[22] C.-W. Tien et al., “KubAnomaly: Anomaly detection for Docker orchestration platform with neural network approaches,” Eng. Reports, 2019.[23] “Kubernetes adoption, security, and market share trends report,”https://www.stackrox.com/kubernetes-adoption-security-and-marketshare-for-containers/, 2020, [accessed 2020-12-18].[24] F. Truta, “Misconfig. remains #1 cause of data breaches in the cloud,”https://securityboulevard.com/2020/04/misconfiguration-remains-the-1-cause-of-data-breaches-in-the-cloud&#x2F;, [accessed 2021-01-14].[25] M. Ahmed, “How to enforce Kubernetes network security policiesusing OPA,” https://www.magalix.com/blog/how-to-enforce-kubernetesnetwork-security-policies-using-opa/, 2020, [accessed 2020-11-02].[26] D. Monica, “Least priv. cont. orchestration,” https://www.docker.com/ ´blog&#x2F;least-privilege-container-orchestration&#x2F;, [accessed 2020-12-17].[27] “Zero-trust networks in Kubernetes, cloud-native applications,”https://www.stackrox.com/wiki/zero-trust-networks-in-kubernetescloud-native-applications/, 2020, [accessed 2020-12-17].[28] “Pod security policies,” https://kubernetes.io/docs/concepts/policy/podsecurity-policy/#host-namespaces/, 2020, [accessed 2021-01-27].[29] M. Ahmed, “Enforce pod security policies in Kubernetes usingcommit d72943c30b7562b19cc450936892fedf7adbdc98Author: dreamcoin1998 &#103;&#97;&#111;&#106;&#117;&#110;&#x62;&#x69;&#x6e;&#57;&#x38;&#64;&#103;&#x6d;&#x61;&#x69;&#x6c;&#x2e;&#x63;&#111;&#109;Date: Wed May 25 18:31:00 2022 +0800 增加kubernetes网络策略：性能评估和安全分析翻译 …skipping…OPA,” https://www.magalix.com/blog/enforce-pod-security-policies-inkubernetes-using-opa/, 2020, [accessed 2021-01-27].[30] “bpf(2) - linux manual page,” https://man7.org/linux/man-pages/man2/bpf.2.html&#x2F;, 2020, [accessed 2020-12-21].[31] P. Manfred, “Kernel privilege escalation via improper eBPF programverification,” https://www.thezdi.com/, 2020, [accessed 2020-12-17].[32] M. Fleming, “A thorough introduction to ebpf,” https://lwn.net/Articles/740157&#x2F;, 2017, [accessed 2020-12-21].[33] “Using eBPF in Kubernetes,” https://kubernetes.io/blog/2017/12/usingebpf-in-kubernetes/, 2017, [accessed 2021-01-28].[34] “Customize the manifests,” https://docs.projectcalico.org/getting-started/kubernetes&#x2F;installation&#x2F;config-options&#x2F;, 2020, [accessed 2020-12-01].[35] “Configure encryption and authentication,” https://docs.projectcalico.org&#x2F;security&#x2F;comms&#x2F;crypto-auth&#x2F;, 2020, [accessed 2020-12-01].[36] A. Gowda and D. Coulter, “Microsoft Docs:Enclave aware containers onAzure ,” https://docs.microsoft.com/en-us/azure/confidential-computing/enclave-aware-containers&#x2F;, 2020, [accessed 2020-10-30].[37] P.-L. Aublin et al., “TaLoS: Secure and transparent TLS terminationinside SGX enclaves,” Imperial College London, Tech. Rep, vol. 5, 2017.","categories":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"http://jerryblogs.com/categories/Kubernetes/"}],"tags":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"http://jerryblogs.com/tags/Kubernetes/"}]},{"title":"Linux Conntrack为什么会崩溃并避免问题","slug":"linux-conntrack","date":"2022-05-25T10:31:00.000Z","updated":"2023-04-20T13:39:41.461Z","comments":true,"path":"2022/05/25/linux-conntrack/","link":"","permalink":"http://jerryblogs.com/2022/05/25/linux-conntrack/","excerpt":"","text":"文章信息作者：Alex Pollitt原文：https://www.tigera.io/blog/when-linux-conntrack-is-no-longer-your-friend/ 连接跟踪（“conntrack”）是Linux内核网络堆栈的核心功能。它允许内核跟踪所有逻辑网络连接或流，从而识别构成每个流的所有数据包，以便可以一致的处理它们。 Conntrack是一个重要的内核特性，它支持一些关键的主线用例： NAT依赖于连接跟踪信息，因此它可以以相同的方式转换流中的所有数据包。例如当pod访问kubernetes服务时，kube-proxy的负载均衡使用NAT将连接重定向到特定的后端pod。Conntrack记录对于特定连接，发往服务IP的数据包应该全部发送到同一个后端Pod，并且从后端pod返回的数据包未经NAT回到源pod。 有状态的防火墙，例如Calico，依靠连接跟踪信息来精确将“响应”流量列入白名单。这允许您编写一个网络策略，上面写着“允许我的pod连接到任何远程IP”，而无需编写策略来明确允许响应流量。（否则，将不得不提娜佳更不安全的规则“允许任何IP到我的pod的数据包”） 此外conntrack通常会提高性能（减少CPU并减少数据包延迟），因为只有流中第一个数据包需要经过完整网络对战处理才能确定如何处理它。有关此操作的一个示例，请参阅“comparing kube-proxy modes”博客。 可是，conntrack有它自己的限制… 它在哪里崩溃conntrack表有一个可配置的最大大小，如果它被填满，连接通常会开始拒绝或丢弃。对于大多数工作负载，表有足够的空间，这永远不会成为问题。但是在某些情况下，需要对conntrack表进行更多的考虑： 最明显的情况是，服务器处理极高数量的同时活动的连接。例如，如果你的conntrack表配置为128k条目，但是有超过128k的同时连接，肯定会遇到问题。 稍微不那么明显的情况是，服务器每秒处理非常多的连接数。即使连接是短暂的，Linux也会在很短的超时时间内（默认为120秒）继续跟踪链接。例如如果您的conntrack表配置为128k条目，并且您尝试每秒处理1100个连接，那么即使连接时间非常短暂（128k &#x2F; 120s &#x3D; 1092个连接&#x2F;s） 有一些小众的工作负载属于这种类型。此外如果你出在一个敌对的环境中，那么用大量的半开放连接来淹没你的服务器，可以作为一种拒绝服务的攻击。在这里两种情况下，通过增加conntrack表的大小或减少conntrack超时，调整conntrack可能足以满足你的需求，但是如果你把调整弄错了，会导致很多痛苦。对于其他情况，你需要绕过conntrack来处理违规的流量。 一个真实的例子与我们合作的一个大型Saas供应商有一组运行在裸机服务器上的memcached服务器（没有虚拟化或容器化），每个服务器每秒处理5万多个短时连接。这远远超过了标准Linux配置所能应付的范围。他们曾尝试调整conntrack配置以增加表大小并减少超时，但调整很脆弱，增加的RAM使用是一个重大损失（想想GBytes），而且连接是如此的短暂，以至于conntrack没有提供其通常的性能优势（减少CPU或数据包延迟）。相反，他们转向了Calico。Calico的网络策略允许绕过特定流量的conntrack（使用doNotTrack的标志）。为他们提供了所需的性能，以及Calico带来的额外安全优势。 绕过conntrack的取舍是什么？ 不跟踪网络策略通常是对称的。在saas提供商的案例中，他们的工作负载是内部的，因此使用网络策略，他们可以非常狭隘地将允许访问memcached服务的所有工作负载的往来流量列为白名单 不跟踪策略对连接的方向是无视的。所以在memcached服务器被攻击的情况下，理论上可以尝试连接到任何memcached客户端，只要它使用正确的源端口。然而，假设你为你的memcached客户端正确定义了网络策略，那么这些链接尝试仍在客户端被拒绝。 不跟踪网络策略适用于每个数据包，而正常的网络策略只适用于流中的第一个数据包。这可能会增加每个数据包的CPU成本，因为每个数据包都需要被网络策略处理。但是对于短暂的连接，这种额外的处理会被连通性处理的减少所抵消。例如，在saas供应上的案例中，每个连接中的数据包数量非常少，因此将策略应用于每个数据包的额外开销是一个合理的权衡。 进行测试我们测试了单个memcached服务器pod和在远程节点上运行的多个客户端pod，因此我们可以每秒驱动非常高的连接。memcached服务器pod主机有8个内核和一个512k的conntrack表。我们测量了以下之间的性能差异：没有网络策略、Calico正常网络策略和Calico不跟踪网络策略。在第一次测试中，我们将连接数限制在每秒4000个，这样我们就可以关注CPU的差异。没有策略和正常策略在性能上没有可衡量的差异，但不跟踪策略使得CPU使用率降低了约20%。 在第二个测试中，我们推送了我们的客户所能容纳的尽可能多的连接，并测量了memcached服务器每秒能够处理的最大连接数。正如预期那样，无策略和正常策略都在每秒4000多个连接数上达到conntrack表的限制(512k &#x2F; 120s &#x3D; 4396个连接&#x2F;s)。有了不跟踪策略，我们的客户端每秒推送了60000个连接而没有遇到任何问题。我们相信我们可以通过吸引更多的客户来超越这个数字，但是我们觉得这些数字足以说明这篇博客的意义了。 结论Conntrack是一个重要的内核特性。它做的很出色。许多主线用例都依赖它。然而对于某些小众的场景，conntrack的开销超过了它带来的正常好处。在这些场景中，Calico的网络策略可以用来有选择的绕过conntrack，同时仍然执行网络安全。对于所有的其他流量，conntrack仍然是你的朋友。","categories":[{"name":"Linux","slug":"Linux","permalink":"http://jerryblogs.com/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://jerryblogs.com/tags/Linux/"}]},{"title":"docker内的init进程是否能被SIGKILL或SIGTERM杀死","slug":"docker-signal","date":"2021-08-13T15:41:47.000Z","updated":"2023-04-20T13:39:16.232Z","comments":true,"path":"2021/08/13/docker-signal/","link":"","permalink":"http://jerryblogs.com/2021/08/13/docker-signal/","excerpt":"","text":"问题在容器内给容器内的init进程发送SIGTERM信号，init进程会不会被杀死，发送SIGKILL信号呢？在容器外发送信号呢？为什么？ 问题来源：极客时间 相关理论知识信号相关 SIGKILL和SIGTERM信号都是Linux信号，SIGKILL和SIGSTOP属于不能被忽略和handle的内核信号，主要提供给内核进程和root用户进行特权操作。 使用kill命令的时候，缺省状态下，kill 1 向init进程发送SIGTREM信号 Ctrl+C发送的是SIGING信号 编程语言的signal很多编程语言对不同的信号做了handle处理，使用C语言需要自己做处理（除了SIGKILL和SIGSTOP之外，他们有内核提供的默认的handle）。下列是Python和Go语言对部分signal的处理： Python Golang SIGQUIT 执行默认操作，进程退出并将内存中的信息转存到硬盘（核心转储） 以堆栈转储方式退出 SIGCHLD 终止子进程 子进程退出 SIGHUP 在控制终端上检测到挂起或控制进程的终止 程序退出 SIGTERM 终结 程序退出 SIGPIPE 写入到没有读取器的管道，默认忽略 在标准输出或标准错误上写入损坏的管道将导致程序退出，而在其他的文件描述符上写入不会采取任何操作，但是会抛出EPIPE错误 SIGING 引发 KeyboardInterrupt 关于signal信号的处理，可以查看下列文档： Python：https://docs.python.org/zh-cn/3/library/signal.html?highlight=signal#signal.SIGPIPE Golang：https://pkg.go.dev/os/signal 容器中的init进程 容器中的init进程是pid为1的进程 当容器以单进程运行时，init进程为容器内执行的进程 当容器以多进程运行时，会启动一个管理进程，该进程即为容器的init进程 在容器内发送SIGKILL命令，内核将会屏蔽掉该信号（与在跟命名空间中一致），防止其他进程杀掉init进程 在容器外杀掉init进程后，再向容器内添加进程将会导致ENOMEN错误，因为进程已经终止 init进程是容器内所有孤儿进程的父进程 当使用kill命令时，内核做了什么（源码分析）寻找kill系统调用Linux系统调用由原本的sys_XXXX换成了SYSCALL_DEFINE，故在&#x2F;kernel&#x2F;signal.c文件下，寻找SYSCALL_DEFINE，找到SYSCALL_DEFINE2(kill, pid_t, pid, int, sig)，代码如下： 12345678910111213/** * sys_kill - send a signal to a process * @pid: the PID of the process * @sig: signal to be sent */SYSCALL_DEFINE2(kill, pid_t, pid, int, sig)&#123; struct kernel_siginfo info; // info是signal的结构体 prepare_kill_siginfo(sig, &amp;info); return kill_something_info(sig, &amp;info, pid);&#125; 关于SYSCALL_DEFINEx，可以在https://blog.csdn.net/hxmhyp/article/details/22699669文章里面看到。 关于kernel_siginfo，可以看如下代码： 123typedef struct kernel_siginfo &#123; __SIGINFO;&#125; kernel_siginfo_t; 关于__SIGINFO： 123456struct &#123; \\ int si_signo; /* Signal number */ int si_errno；/* An errno value */ int si_code; // 产生信号的原因 union __sifields _sifields;&#125; 简而言之，__SIGINFO就是标识signal的一些信息。 接下来我们来看看prepare_kill_siginfo(sig, &amp;info);方法做了什么： 123456789static inline void prepare_kill_siginfo(int sig, struct kernel_siginfo *info)&#123; clear_siginfo(info); info-&gt;si_signo = sig; // 传入的信号 info-&gt;si_errno = 0; info-&gt;si_code = SI_USER; // 产生自用户空间 info-&gt;si_pid = task_tgid_vnr(current); // 主要是从当前namespace 中找到对应的pid号。 info-&gt;si_uid = from_kuid_munged(current_user_ns(), current_uid()); // 返回0，也就是root用户&#125; 关于info-&gt;si_code = SI_USER; 中的SI_USER，在内核源码\\include\\uapi\\asm-generic\\siginfo.h可以看到Linux定义的一些信号相关的信息。下列贴出部分： 1234567891011121314151617/* * si_code values * Digital reserves positive values for kernel-generated signals. */#define SI_USER 0 /* sent by kill, sigsend, raise 由kill、raise等发送的*/#define SI_KERNEL 0x80 /* sent by the kernel from somewhere 从内核某处发送 */#define SI_QUEUE -1 /* sent by sigqueue */#define SI_TIMER -2 /* sent by timer expiration */#define SI_MESGQ -3 /* sent by real time mesq state change */#define SI_ASYNCIO -4 /* sent by AIO completion */#define SI_SIGIO -5 /* sent by queued SIGIO */#define SI_TKILL -6 /* sent by tkill system call */#define SI_DETHREAD -7 /* sent by execve() killing subsidiary threads */#define SI_ASYNCNL -60 /* sent by glibc async name lookup completion */#define SI_FROMUSER(siptr) ((siptr)-&gt;si_code &lt;= 0)#define SI_FROMKERNEL(siptr) ((siptr)-&gt;si_code &gt; 0) 由此可以看到，这里的标识表示SI_USER是由kill发出的。 接下来可以看task_tgid_vnr(current)；首先是current： 1234567891011121314/* * We don&#x27;t use read_sysreg() as we want the compiler to cache the value where * possible. */static __always_inline struct task_struct *get_current(void)&#123; unsigned long sp_el0; asm (&quot;mrs %0, sp_el0&quot; : &quot;=r&quot; (sp_el0)); return (struct task_struct *)sp_el0;&#125;#define current get_current() // 获取当前运行的task_struct 它是获取当前运行的task_struct，我们都知道，在Linux里面task_struct表示一个进程描述符，存储了进程相关的一些信息。 然后我们看一下task_tgid_vnr方法，一路追踪到底，可以看到： 12345678910111213pid_t __task_pid_nr_ns(struct task_struct *task, enum pid_type type, struct pid_namespace *ns)&#123; pid_t nr = 0; // pid_t 宏定义 int 变量 rcu_read_lock(); // 申请rcu锁 if (!ns) ns = task_active_pid_ns(current); nr = pid_nr_ns(rcu_dereference(*task_pid_ptr(task, type)), ns); rcu_read_unlock(); // 释放rcu锁 return nr;&#125; task_active_pid_ns方法：\\linux\\kernel\\pid.c 1234struct pid_namespace *task_active_pid_ns(struct task_struct *tsk)&#123; return ns_of_pid(task_pid(tsk));&#125; 接着继续ns_of_pid方法：\\include\\linux\\pid.h 1234567static inline struct pid_namespace *ns_of_pid(struct pid *pid)&#123; struct pid_namespace *ns = NULL; if (pid) ns = pid-&gt;numbers[pid-&gt;level].ns; return ns;&#125; 这里返回pid所在的命名空间。 然后是pid_nr_ns：\\include\\linux\\pid.h 123456789101112pid_t pid_nr_ns(struct pid *pid, struct pid_namespace *ns)&#123; struct upid *upid; pid_t nr = 0; if (pid &amp;&amp; ns-&gt;level &lt;= pid-&gt;level) &#123; upid = &amp;pid-&gt;numbers[ns-&gt;level]; if (upid-&gt;ns == ns) nr = upid-&gt;nr; &#125; return nr;&#125; 主要是从当前namespace 中找到对应的pid号。 OK，所以prepare_kill_siginfo方法获取kernel_siginfo相关的信息并进行初始化。 接下来我们来看最关键的kill_something_info： 12345678static int kill_something_info(int sig, struct kernel_siginfo *info, pid_t pid)&#123; int ret; if (pid &gt; 0) return kill_proc_info(sig, info, pid); ...&#125; kill_proc_info: 12345678static int kill_proc_info(int sig, struct kernel_siginfo *info, pid_t pid)&#123; int error; rcu_read_lock(); error = kill_pid_info(sig, info, find_vpid(pid)); rcu_read_unlock(); return error;&#125; 首先是find_vpid: 1234struct pid *find_vpid(int nr)&#123; return find_pid_ns(nr, task_active_pid_ns(current));&#125; task_active_pid_ns刚刚我们已经讲过，所以这里是返回当前进程所在的命名空间。 而find_pid_ns作用是找到其所属的struct pid。 然后我们进入kill_pid_info： 123456789101112131415161718192021int kill_pid_info(int sig, struct kernel_siginfo *info, struct pid *pid)&#123; int error = -ESRCH; struct task_struct *p; for (;;) &#123; rcu_read_lock(); p = pid_task(pid, PIDTYPE_PID); if (p) error = group_send_sig_info(sig, info, p, PIDTYPE_TGID); rcu_read_unlock(); if (likely(!p || error != -ESRCH)) return error; /* * The task was unhashed in between, try again. If it * is dead, pid_task() will return NULL, if we race with * de_thread() it will find the new leader. */ &#125;&#125; pid_task： 123456789101112struct task_struct *pid_task(struct pid *pid, enum pid_type type)&#123; struct task_struct *result = NULL; if (pid) &#123; struct hlist_node *first; first = rcu_dereference_check(hlist_first_rcu(&amp;pid-&gt;tasks[type]), lockdep_tasklist_lock_is_held()); if (first) result = hlist_entry(first, struct task_struct, pid_links[(type)]); &#125; return result;&#125; 根据一致的struct pid找到struct task_struct。 group_send_sig_info： 12345678910111213141516int group_send_sig_info(int sig, struct kernel_siginfo *info, struct task_struct *p, enum pid_type type)&#123; int ret; rcu_read_lock(); ret = check_kill_permission(sig, info, p); // 检查是否是有效信号，检测是否来自用户或内核 // 来自用户需要记录审计日志后返回0，来自内核直接返回0 rcu_read_unlock(); if (!ret &amp;&amp; sig) // 如果是有效信号，调用do_send_sig_info ret = do_send_sig_info(sig, info, p, type); return ret;&#125; 我们来看看check_kill_permission的调用链： 123456789101112131415161718192021222324252627282930313233343536373839/* * Bad permissions for sending the signal * - the caller must hold the RCU read lock */static int check_kill_permission(int sig, struct kernel_siginfo *info, struct task_struct *t)&#123; struct pid *sid; int error; if (!valid_signal(sig)) // return sig &lt;= _NSIG ? 1 : 0; _NSIG == 64 这里检测是否是一个有效的信号 return -EINVAL; // EINVAL 宏定义 22 if (!si_fromuser(info)) // si_fromuser判断是否是来自用户，是返回True return 0; error = audit_signal_info(sig, t); // 记录审计日志，这里始终返回0 if (error) return error; if (!same_thread_group(current, t) &amp;&amp; // current和当前进程信号是否一致 !kill_ok_by_cred(t)) &#123; switch (sig) &#123; case SIGCONT: sid = task_session(t); /* * We don&#x27;t return the error if sid == NULL. The * task was unhashed, the caller must notice this. */ if (!sid || sid == task_session(current)) break; fallthrough; default: return -EPERM; &#125; &#125; return security_task_kill(t, info, sig, NULL); // 返回0&#125; si_fromuser： 1234567891011static inline bool si_fromuser(const struct kernel_siginfo *info)&#123; return info == SEND_SIG_NOINFO || // SEND_SIG_NOINFO为SI_USER，这里info.si_code为SI_USER故为True (!is_si_special(info) &amp;&amp; SI_FROMUSER(info)); // is_si_special函数关键处为info &lt;= SEND_SIG_PRIV; SEND_SIG_PRIV即SI_KERNEL表示由内核发送 // 故is_si_special(info)为True，即!is_si_special(info)为false // SI_FROMUSER为下列宏定义 // #define SI_FROMUSER(siptr) ((siptr)-&gt;si_code &lt;= 0) // 故这里SI_FROMUSER(info)为True // 综上所述，si_fromuser判断sig是否来自用户&#125; 进入audit_signal_info(sig, t)调用： 1234567891011121314151617181920212223242526/** * audit_signal_info - record signal info for shutting down audit subsystem * @sig: signal value * @t: task being signaled * * If the audit subsystem is being terminated, record the task (pid) * and uid that is doing that. */int audit_signal_info(int sig, struct task_struct *t)&#123; kuid_t uid = current_uid(), auid; if (auditd_test_task(t) &amp;&amp; // auditd_test_task判断是否是注册的审计进程，是返回1，否返回0 (sig == SIGTERM || sig == SIGHUP || sig == SIGUSR1 || sig == SIGUSR2)) &#123; // 如果sig是SIGTERM、SIGHUP或用户自定义的SIGUSR1和SIGUSR2 audit_sig_pid = task_tgid_nr(current); auid = audit_get_loginuid(current); if (uid_valid(auid)) audit_sig_uid = auid; else audit_sig_uid = uid; security_task_getsecid(current, &amp;audit_sig_sid); &#125; return audit_signal_info_syscall(t); // 始终返回0&#125; auditd_test_task： 1234567891011121314151617181920/** * auditd_test_task - Check to see if a given task is an audit daemon * @task: the task to check * * Description: * Return 1 if the task is a registered audit daemon, 0 otherwise. */int auditd_test_task(struct task_struct *task)&#123; int rc; struct auditd_connection *ac; rcu_read_lock(); ac = rcu_dereference(auditd_conn); // rc = (ac &amp;&amp; ac-&gt;pid == task_tgid(task) ? 1 : 0); // task_tgid通过task_struct获取pid结构体，而ac是Linux audit守护进程，负责将审计记录写入磁盘，使用ausearch或aureport实用程序查看日志。 rcu_read_unlock(); return rc;&#125; 然后回到group_send_sig_info函数，我们聚焦到do_send_sig_info函数： 12345678910111213int do_send_sig_info(int sig, struct kernel_siginfo *info, struct task_struct *p, enum pid_type type)&#123; unsigned long flags; int ret = -ESRCH; if (lock_task_sighand(p, &amp;flags)) &#123; // 获取信号处理函数锁 ret = send_signal(sig, info, p, type); unlock_task_sighand(p, &amp;flags); &#125; return ret;&#125; 调用send_signal： 1234567891011121314151617181920212223242526272829303132333435363738static int send_signal(int sig, struct kernel_siginfo *info, struct task_struct *t, enum pid_type type)&#123; /* Should SIGKILL or SIGSTOP be received by a pid namespace init? */ // pid namespace init是否需要接收SIGKILL or SIGSTOP bool force = false; if (info == SEND_SIG_NOINFO) &#123; /* Force if sent from an ancestor pid namespace */ // 如果来自祖先用户空间，则强制发送 force = !task_pid_nr_ns(current, task_active_pid_ns(t)); &#125; else if (info == SEND_SIG_PRIV) &#123; /* Don&#x27;t ignore kernel generated signals */ // 来自内核的信号不会被忽略 force = true; &#125; else if (has_si_pid_and_uid(info)) &#123; /* SIGKILL and SIGSTOP is special or has ids */ struct user_namespace *t_user_ns; rcu_read_lock(); t_user_ns = task_cred_xxx(t, user_ns); if (current_user_ns() != t_user_ns) &#123; kuid_t uid = make_kuid(current_user_ns(), info-&gt;si_uid); info-&gt;si_uid = from_kuid_munged(t_user_ns, uid); &#125; rcu_read_unlock(); /* A kernel generated signal? */ force = (info-&gt;si_code == SI_KERNEL); /* From an ancestor pid namespace? */ if (!task_pid_nr_ns(current, task_active_pid_ns(t))) &#123; info-&gt;si_pid = 0; force = true; &#125; &#125; return __send_signal(sig, info, t, type, force);&#125; task_pid_nr_ns我们刚刚已经讲过了，让我们再讲一遍： 123456789101112131415pid_t __task_pid_nr_ns(struct task_struct *task, enum pid_type type, struct pid_namespace *ns)&#123; //参数中的 ns = task_active_pid_ns(t)是返回目标进程的namespace // 这里的task是current，也就是当前进程的task_struct pid_t nr = 0; // pid_t 宏定义 int 变量 rcu_read_lock(); // 申请rcu锁 if (!ns) ns = task_active_pid_ns(current); // 返回pid也就是调用kill的进程所在的命名空间 nr = pid_nr_ns(rcu_dereference(*task_pid_ptr(task, type)), ns); // 从当前namespace 中找到对应的pid号 rcu_read_unlock(); // 释放rcu锁 return nr;&#125; 然后来到pid_nr_ns： 123456789101112pid_t pid_nr_ns(struct pid *pid, struct pid_namespace *ns)&#123; struct upid *upid; pid_t nr = 0; if (pid &amp;&amp; ns-&gt;level &lt;= pid-&gt;level) &#123; upid = &amp;pid-&gt;numbers[ns-&gt;level]; // 获取和目标进程处于同一level的namespace if (upid-&gt;ns == ns) // 当前进程和目标进程是不是同一个命名空间 nr = upid-&gt;nr; &#125; return nr;&#125; 在这里，我们使用kill命令发送signal的进程和目标进程之间的关系存在3种情况： 处于同一个namespace： 那么也就是&amp;ns.level&#x3D;&#x3D;&amp;pid.level，而且&amp;pid-&gt;numbers[ns-&gt;level] &#x3D;&#x3D; ns，最终结果返回当前进程pid 当前进程处于目标进程的父namespace： 则&amp;pid.level &lt; &amp;ns.level，最终返回0 所以由于pid &gt; 0，结果!task_pid_nr_ns(current, task_active_pid_ns(t))情况如下： 处于同一个namespace：False 当前进程处于目标进程的父namespace：True 所以，如果当前进程处于目标进程的祖先namespace，那这个信号一定会被传递给init进程。 1234567891011121314151617181920212223242526stateDiagram [*] --&gt; SYSCALL_DEFINE2 SYSCALL_DEFINE2 --&gt; prepare_kill_siginfo prepare_kill_siginfo --&gt; task_tgid_vnr task_tgid_vnr --&gt; pid_nr_ns prepare_kill_siginfo --&gt; kill_something_info kill_something_info --&gt; kill_proc_info kill_proc_info --&gt; kill_pid_info kill_pid_info --&gt; group_send_sig_info group_send_sig_info --&gt; kill_pid_info group_send_sig_info --&gt; check_kill_permission group_send_sig_info --&gt; do_send_sig_info check_kill_permission --&gt; security_task_kill security_task_kill --&gt; do_send_sig_info do_send_sig_info --&gt; send_signal send_signal --&gt; task_pid_nr_ns send_signal --&gt; has_si_pid_and_uid has_si_pid_and_uid --&gt; current_user_ns has_si_pid_and_uid --&gt; task_pid_nr_ns current_user_ns --&gt; make_kuid make_kuid --&gt; from_kuid_munged task_pid_nr_ns --&gt; task_active_pid_ns send_signal --&gt; __send_signal from_kuid_munged --&gt; __send_signal task_active_pid_ns --&gt; __send_signal __send_signal --&gt; [*] Linux忽略SIGKILL和SIGSTOP发送到init进程123456789101112131415static int __send_signal(int sig, struct kernel_siginfo *info, struct task_struct *t, enum pid_type type, bool force)&#123; struct sigpending *pending; // sig的链表，保存了进程上尚未处理的信号 struct sigqueue *q; // 实时信号可能会排队，使用队列实现 int override_rlimit; int ret = 0, result; assert_spin_locked(&amp;t-&gt;sighand-&gt;siglock); // 自旋锁 result = TRACE_SIGNAL_IGNORED; // 判断该信号SIGKILL或者SIGSTOP是否是发送给init，如果是则忽略 if (!prepare_signal(sig, t, force)) goto ret;&#125; 追踪到prepare_signal函数一直往下： 1234567891011121314151617181920212223242526272829static bool sig_task_ignored(struct task_struct *t, int sig, bool force)&#123; void __user *handler; handler = sig_handler(t, sig); /* SIGKILL and SIGSTOP may not be sent to the global init */ // SIGKILL and SIGSTOP不会被发送到global init进程 // 如果是SIGKILL and SIGSTOP目标进程是init进程 if (unlikely(is_global_init(t) &amp;&amp; sig_kernel_only(sig))) return true; // 这里的flag标志是task_struct的标志，见下面 // SIGNAL_UNKILLABLE为进程刚被fork，但是还没被执行，所以这里是如果进程刚被创建，但是还没被执行，返回true // 如果是默认的handle // 不是SIGKILL或者是SIGSTOP并且可以被发送到进程 // 所以这里标识如果进程刚被fork，发送的信号不是必须被处理的信号，且handle是默认的handle，则不能响应信号 if (unlikely(t-&gt;signal-&gt;flags &amp; SIGNAL_UNKILLABLE) &amp;&amp; handler == SIG_DFL &amp;&amp; !(force &amp;&amp; sig_kernel_only(sig))) return true; /* Only allow kernel generated signals to this kthread */ // PF_KTHREAD是内核线程 if (unlikely((t-&gt;flags &amp; PF_KTHREAD) &amp;&amp; (handler == SIG_KTHREAD_KERNEL) &amp;&amp; !force)) return true; return sig_handler_ignored(handler, sig);&#125; flag信号： 123456// flag可能取值如下：PF_FORKNOEXEC 进程刚创建，但还没执行。PF_SUPERPRIV 超级用户特权。PF_DUMPCORE dumped core。PF_SIGNALED 进程被信号(signal)杀出。PF_EXITING 进程开始关闭 再看sig_handler_ignored： 1234567891011121314static inline bool sig_handler_ignored(void __user *handler, int sig)&#123; /* Is it explicitly or implicitly ignored? */ // 属于可以被忽略的信号 return handler == SIG_IGN || // 默认信号处理程序，也就是说信号是SIGCONT，SIGCHLD，SIGWINCH，SIGURG他们的默认处理信号是忽略 (handler == SIG_DFL &amp;&amp; sig_kernel_ignore(sig));&#125;#define sig_kernel_ignore(sig) siginmask(sig, SIG_KERNEL_IGNORE_MASK)#define SIG_KERNEL_IGNORE_MASK (\\ rt_sigmask(SIGCONT) | rt_sigmask(SIGCHLD) | \\ rt_sigmask(SIGWINCH) | rt_sigmask(SIGURG) ) SIGCONT，SIGCHLD，SIGWINCH，SIGURG： 12345678SIGCHLD：● 子进程终止时；● 子进程接收到SIGSTOP信号停止时；● 子进程处在停止态，接收到SIGCONT信号后被唤醒时。SIGWINCH:窗口大小改变时SIGURG：有&quot;紧急&quot;数据或out-of-band数据到达socket时产生. 总结 信号时SIGKILL 或 SIGSTOP目标进程是init进程，该信号会被忽略即不能响应信号 如果进程刚被fork，发送的信号不是必须被处理的信号，且handle是默认的handle，则不能响应信号 目标线程是内核线程，并且是默认内核线程处理程序，但是不能被发送，则不能响应信号 信号是SIGCONT，SIGCHLD，SIGWINCH，SIGURG他们的默认处理信号是忽略","categories":[{"name":"Docker","slug":"Docker","permalink":"http://jerryblogs.com/categories/Docker/"}],"tags":[{"name":"Docker","slug":"Docker","permalink":"http://jerryblogs.com/tags/Docker/"}]},{"title":"Docker的命名空间","slug":"docker-namespace","date":"2021-08-09T15:36:40.000Z","updated":"2023-04-20T13:39:13.005Z","comments":true,"path":"2021/08/09/docker-namespace/","link":"","permalink":"http://jerryblogs.com/2021/08/09/docker-namespace/","excerpt":"","text":"命名空间Linux内核用命名空间来区分内核资源。不同的进程看到的资源是不同的。 这里的资源包括pid，hostname，userid，文件名，网络访问和进程间通信 在Linux内核3.8版本，内核对容器有足够的支持 内核空间的类型 Linux内核5.6，支持8种命名空间。 mount：mount命名空间控制挂载点 在创建命名空间时，当前的命名空间会复制到新的命名空间。但是已经创建的挂载点不会在明明见之间传播。 创建此类型的标志clone_NEW PID：提供独立于其他命名空间的pid集合，不同pid namespace里面的pid可以重复 PID命名空间内的进程ID是唯一的，从1开始分配 PID 1进程的终止将终止该命名空间以及其后代所有进程 PID namespace的使用需要配置参数CONFIG_PID_NS Network(net)：网络命名空间虚拟化提供隔离的网络相关的系统隔离资源，如网络设备、IPv4、IPv6协议栈，IP路由表、防火墙规则、&#x2F;proc&#x2F;net目录、&#x2F;sys&#x2F;class&#x2F;net目录、&#x2F;proc&#x2F;sys&#x2F;net下的各种文件、端口号等等。特别的，网络命名空间隔离UNix域抽象套接字命名空间。 物理网络设备：当网络命名空间内的进程终止，网络命名空间被释放时，物理网络设备将会回到初始话网络命名空间。 虚拟网络设备：提供了管道式的，抽象化的，可以在网络命名空间之间使用的隧道，在一个物理网络设备到另一个网络命名空间之间使用的桥梁。当一个namespace被释放，它包含的veth设备将会被销毁。 使用network namespace需要内核配置CONFIG_NET_NS参数 User：用来区分安全标识符和属性，特别是UserID、GroupID、根目录和keys。 Cgropu：（Cgroup可以对一组进程做资源控制） IPC：隔离了IPC资源，即SystemV IPC对象，POSIX消息队列。需要CONFIG_IPC_NS内核参数 Time：CONFIG_TIME_NS内核参数。虚拟化CLOCK_MONOTONIC和CLOCK_BOOTTIME。 UTS：虚拟化hostname和NIS域名。CONFIG_UTS_NS PID namespacenamespace init process第一个在namespace中创建的进程，是任何在这个pid namespace中的孤儿进程的父进程。 当init终止，内核通过发送SIGKILL信号终止namespace中所有的子进程，在这种情况下，调用fork将返回ENOMEN错误，因为init进程已经终止。 内核会帮助init进程屏蔽掉任何其他信号，防止其他进程不小心kill掉init进程导致系统挂掉。可以在父PID namespace中发送SIGKILL信号终止namespace中init进程。通过什么方式屏蔽？ PID namespace nestPID namespace可以嵌套：每一个PID namespace都有一个parent，除了根PID namespace。父PID namespace是使用clone()或unshare()函数创建的进程。从Linux 3.7开始，内核限制最深嵌套是32。 子PID namespace中的进程对于父PID namespace中的进程是不可见的。进程可以下降到子命名空间，但是不能进入任何一个父命名空间。 进程的PID命名空间在进程创建时确定，不可更改。 &#x2F;proc 和 PID namespace对一个PID namespace而言，&#x2F;proc目录只包含当前namespace和它所有子孙后代namespace里的进程的信息。 mount namespacemount namespace隔离了不同的挂载点，每个挂载的命名空间实例将看到不同的单目录层次结构。使用clone和unshare传递CLONE_NEWS信号能够创建心的mount namespace。 当一个心得mount namespace创建，将进行如下初始化： 使用clone创建，将子namespace的挂载点列表是父挂载点列表的复制 使用unshare创建，新命名空间挂载点列表是调用者之前命名空间挂载点列表的复制 mount namespace的限制 每一个mount namespace拥有一个user namespace。一个新的mount namespace是另一个namespace的副本，新旧namespace拥有不同的user namespace，那么新的namespace将被赋予更低的特权级别 当创建一个更低特权级别的mount namespace，共享挂载将会减少到从属挂载，这确保更低特权级别执行的映射不会传播到更高特权级别 来自更高特权级别的单个单元的挂载将会被加锁，不会被更低优先级别的mount namespace分开。unshare的CLONE_NEWS操作将原有命名空间当中的mount以及其递归的mount作为单个单元进行传播。 在一个mount namespace作为挂载点且不在其他mount namespace的文件或目录可以被重命名、取消链接和删除（不是挂载点）。 共享子树背景：为了加载一个新的硬盘，使其在所有的mount namespace中可用，需要在所有的mount namespace中执行挂载操作。 共享子树功能在Linux 2.6.15中引入。该功能允许自动的、在命名空间之前传播的受控的挂载和卸载事件。 每个挂载点被标记为如下的传播类型： MS_SHARED：在一组对等节点之间共享事件。 MS_PRIVATE：这个挂载点是私有的，挂载和卸载事件不会传播进来或传播出去。 MS_SLAVE：事件将会在master的共享对等组之间传播，不会传播到其他组。（一组挂载点可以是其他组的slave） MS_UNBINDDABLE：类似于私有的mount，这个mount将不能被绑定挂载。","categories":[{"name":"Docker","slug":"Docker","permalink":"http://jerryblogs.com/categories/Docker/"}],"tags":[{"name":"Docker","slug":"Docker","permalink":"http://jerryblogs.com/tags/Docker/"}]},{"title":"Docker的架构","slug":"docker-framework","date":"2021-08-08T17:03:04.000Z","updated":"2023-04-20T13:39:05.424Z","comments":true,"path":"2021/08/09/docker-framework/","link":"","permalink":"http://jerryblogs.com/2021/08/09/docker-framework/","excerpt":"","text":"Docker的架构docker是client-server架构，client通过socket或restful API与docker daemon之间进行数据交互。两者可以跑在同一个系统上，或者使用远程的Docker daemon。后者负责繁重的构建、运行、分发docker 容器。 docker内部包含三个组件： docker images docker registers 镜像仓库（私有和公有） docker containers Docker image如何工作docker image是层次化的只读模板，它利用union file system将不同的层组成一个image，同时允许覆盖单独文件系统的文件和目录，形成统一的文件系统。当image增加层或者更新层，需要重新构建image。 Docker container work当从docker images 运行一个container，它会在image的顶层添加一个读写层（union file system）以使你的应用可以运行。 当运行一个container，会发生什么？ pull the image：docker检查镜像是否存在，当镜像不存在，会从docker hub中拉取；当镜像存在，即使用其构建。 create a new container：一旦image存在，docker使用其创建一个镜像。 分配一个文件系统并且挂载一个读写层：容器在文件系统之中被创建，并且读写层被添加进image 分配一个网络&#x2F;网桥接口：分配一个网络&#x2F;网桥接口，允许容器与本机交互 设置IP地址：从IP池中寻找并分配一个可用的IP地址 执行指定的进程：运行应用程序 捕获并且提供应用的输出：连接并且记录标准输入，输出和错误 当container结束时，停止并且删除contianer 使用的底层技术namespaceLinuex 内核提供了一层隔离，容器的每个方面运行在namespace，并且对它所处的namespace之外没有访问权限 Control GroupsCGroups允许Docker将可用硬件资源共享给容器，并在必要时设置约束和限制，，比如限制特定容器的可用内存。 Union file systemUnion通过创建层来操作的文件系统。","categories":[{"name":"Docker","slug":"Docker","permalink":"http://jerryblogs.com/categories/Docker/"}],"tags":[{"name":"Docker","slug":"Docker","permalink":"http://jerryblogs.com/tags/Docker/"}]},{"title":"如何阅读docker源码","slug":"docker-learn","date":"2021-08-08T14:00:00.000Z","updated":"2023-04-20T13:39:10.866Z","comments":true,"path":"2021/08/08/docker-learn/","link":"","permalink":"http://jerryblogs.com/2021/08/08/docker-learn/","excerpt":"","text":"来自docker-dev群组：https://groups.google.com/g/docker-dev/c/g0jUDDltp4o learn go figure out how the basic command flags work build docker executables with minor changes 来自StackOverflow：https://stackoverflow.com/questions/22639685/new-to-docker-wonder-how-to-read-dockers-source-code 理解Cgroup和Linux内核的命名空间 阅读 docker’s github source code repository和contributing to docker guide","categories":[{"name":"Docker","slug":"Docker","permalink":"http://jerryblogs.com/categories/Docker/"}],"tags":[{"name":"Docker","slug":"Docker","permalink":"http://jerryblogs.com/tags/Docker/"}]}],"categories":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"http://jerryblogs.com/categories/Kubernetes/"},{"name":"Linux","slug":"Linux","permalink":"http://jerryblogs.com/categories/Linux/"},{"name":"Docker","slug":"Docker","permalink":"http://jerryblogs.com/categories/Docker/"}],"tags":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"http://jerryblogs.com/tags/Kubernetes/"},{"name":"Linux","slug":"Linux","permalink":"http://jerryblogs.com/tags/Linux/"},{"name":"Docker","slug":"Docker","permalink":"http://jerryblogs.com/tags/Docker/"}]}