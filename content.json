{"meta":{"title":"Jerry的小站","subtitle":"博客/工具分享","description":"一个集博客和工具咨询分享于一体的平台","author":"Jerry Gao","url":"http://jerryblogs.com","root":"/"},"pages":[{"title":"","date":"2023-04-20T12:28:29.841Z","updated":"2023-04-20T12:28:29.841Z","comments":true,"path":"404.html","permalink":"http://jerryblogs.com/404.html","excerpt":"","text":"404 很抱歉，您访问的页面不存在 可能是输入地址有误或该地址已被删除"},{"title":"","date":"2023-07-05T08:46:22.169Z","updated":"2023-07-05T08:46:22.169Z","comments":true,"path":"about/index.html","permalink":"http://jerryblogs.com/about/index.html","excerpt":"","text":"Jerry Gao 高俊斌，98年生，是一名SRE（运维开发）工程师，曾就职于网易互娱 网易就职期间拿过运维质量优化奖，贡献过最佳实践文档 5+年Python经验，2+年Go经验，对Linux&#x2F;云原生领域有浓厚的兴趣 自学编程，大学时期不好好学习，比较喜欢折腾，玩儿过前端(Vue,React)&#x2F;后端(Python,Go)&#x2F;爬虫，拿过奖，卖过软件&#x2F;数据，甚至玩儿过React Native，感觉自己啥都会 大学时期开设币圈自媒体，撰写技术博客 大一开始学编程开始就用Github，上面是自己成长的印记 爱好 旅游，去和陌生人交流，去体验他们的人生，他们的生活 足球，2013年广州恒大夺冠开始热爱足球，足球是我的生命 做饭，把一堆活蹦乱跳的小鱼小虾都变成可口的红烧鱼，黄焖大虾，那是满满的成就感 民谣，一边听李志&#x2F;宋冬野&#x2F;赵雷，一边写代码 家乡现居深圳，家乡漳州。 漳州，位于中国福建省东南沿海，是一座历史悠久、文化灿烂的古城。漳州地处于山海交汇之间，山清水秀，自然风光美不胜收。 首先，漳州的自然景观令人陶醉。这里有蜿蜒曲折的河流，如林溪、诏安溪等，水清澈见底，悠然流淌。还有峡谷、瀑布、溶洞等奇峰异石，如仙都、雁洋、霞阳等，形态各异，美不胜收。尤其是世界文化自然双遗产——土楼，作为漳州独特的民居建筑，更是引人入胜。这些土楼如同宛如童话般的梦幻世界，有着千姿百态的外形，深受游客喜爱。 其次，漳州的气候四季如春，温暖湿润，充满了生机和活力。春天，山花烂漫，野草如茵，百花争艳；夏天，绿树成荫，溪水潺潺，清凉宜人；秋天，硕果累累，丹桂飘香，金黄一片；冬天，阳光温暖，气候宜人，是避寒胜地。这里的自然景色犹如一幅幅画卷，令人陶醉其中。 此外，漳州还有丰富的生物资源，拥有众多珍稀植物和动物。如南靖土楼周边的茶园、竹林等，以及漳浦国家级自然保护区和天台山国家级自然保护区，都是得天独厚的生态环境。这里的自然生态保护得当，生物多样性丰富，是一个理想的生态旅游胜地。 漳州是一个融合了自然、历史、文化的城市，其独特的自然风光和丰富的生物资源，使其成为一处旅游胜地，吸引了众多的游客前来观光、度假和探索。无论是欣赏山水之美，还是领略古城之韵，漳州都是一个让人陶醉的地方，仿佛是一幅神奇的画卷，让人流连忘返。 (其实以上来自ChatGpt)"},{"title":"所有分类","date":"2023-04-20T15:13:33.291Z","updated":"2023-04-20T15:13:33.291Z","comments":true,"path":"categories/index.html","permalink":"http://jerryblogs.com/categories/index.html","excerpt":"","text":""},{"title":"Page","date":"2013-12-26T14:52:56.000Z","updated":"2023-04-19T19:09:41.010Z","comments":true,"path":"page/index.html","permalink":"http://jerryblogs.com/page/index.html","excerpt":"","text":"This is a page test."},{"title":"所有标签","date":"2023-04-20T12:16:46.212Z","updated":"2023-04-20T12:16:46.212Z","comments":true,"path":"tag/index.html","permalink":"http://jerryblogs.com/tag/index.html","excerpt":"","text":""}],"posts":[{"title":"内存资源的服务质量（QoS）","slug":"内存资源的服务质量","date":"2023-09-03T11:57:52.000Z","updated":"2023-09-04T10:07:33.412Z","comments":true,"path":"2023/09/03/内存资源的服务质量/","link":"","permalink":"http://jerryblogs.com/2023/09/03/%E5%86%85%E5%AD%98%E8%B5%84%E6%BA%90%E7%9A%84%E6%9C%8D%E5%8A%A1%E8%B4%A8%E9%87%8F/","excerpt":"","text":"什么是QoS（服务质量）？QoS，Quality of service，即服务质量。它是对服务整体性能的描述或测量。特别是网络用户所看到的性能。为了定量测量服务质量，通常会考虑网络服务的以下几个方面，例如丢包、比特率、吞吐量、传输延迟、可用性、抖动等。在计算机网络和其他分组交换电信网络领域，服务质量是指流量优先级和资源预留控制机制，而不是所实现的服务质量。服务质量是指为不同的应用程序、用户或数据流提供不同优先级的能力，或者保证数据流达到一定性能水平的能力。 Memory QoS在Kunernetes的历史最早Memory QoS在Kubernetes v1.22添加，后来引入了memory.high公式等一些限制。这些限制在v1.27版本被解决。 2023.04月发布的Kubernetes v1.27对Memory QoS的更改，提高了Linux节点中的内存的管理能力。 如何开启Memory QoS要求 操作系统发行版启用 cgroup v2 Linux 内核为 5.8 或更高版本 容器运行时支持 cgroup v2。例如： containerd v1.4 和更高版本 cri-o v1.20 和更高版本 kubelet 和容器运行时被配置为使用 systemd cgroup 驱动 启用Memory QoS验证是否支持cgroup V21stat -fc %T /sys/fs/cgroup/ 对于 cgroup v2，输出为 cgroup2fs 对于 cgroup v1，输出为 tmpfs linux发行版cgroup v2支持： Container-Optimized OS（从 M97 开始） Ubuntu（从 21.10 开始，推荐 22.04+） Debian GNU&#x2F;Linux（从 Debian 11 Bullseye 开始） Fedora（从 31 开始） Arch Linux（从 2021 年 4 月开始） RHEL 和类似 RHEL 的发行版（从 9 开始） 要手动启动cgroup v2，如果发行版使用grub引导，则通过修改/etc/default/grub下的GRUB_CMDLINE_LINUX，在其中添加systemd.unified_cgroup_hierarchy=1，然后执行sudo update-grub。 Containerd版本 &gt;&#x3D; v1.6.0在v1.6.0合并了该PR以支持在创建容器时支持对cgroup v2字段的解释 kubelet配置开启MemoryQoS=true1234apiVersion: kubelet.config.k8s.io/v1beta1kind: KubeletConfigurationfeatureGates: MemoryQoS: true 迁移到cgroup v2 升级到一个使用cgroup v2的内核版本。即Linux 内核为 5.8 或更高版本。 如果有应用基于cgroup v1直接访问cgroup文件系统，需要将这些应用更新到cgroup v2的版本。 一些第三方依赖和监控服务可能有此依赖 以独立的DaemonSet方式运行的cAdvisor 以监控 Pod 和容器，需要将其更新到v0.43.0版本及以上 部署Java应用程序，则最好完全支持cgroup v2以及更高的版本 OpenJDK &#x2F; HotSpot: jdk8u372、11.0.16、15 及更高的版本 IBM Semeru Runtimes: jdk8u345-b01、11.0.16.0、17.0.4.0、18.0.2.0 及更高的版本 IBM Java: 8.0.7.15 及更高的版本 正在使用 uber-go&#x2F;automaxprocs 包，则确保版本 &gt;&#x3D; v1.5.1 背景当Kubelet将容器作为Pod的一部分启动时，Kubelete将CPU和内存限制传递给容器运行时，容器运行时将CPU和内存分配给容器。如果有空余的CPU时间，则会为容器分配他们的request数量，容器使用的CPU不能超过限制。如果容器使用的CPU数量超过了限制，则容器的CPU使用率将收到限制。但是对于内存来讲，容器运行时仅使用内存limit，丢弃request，request仅用于影响调度。当容器使用的内存超过限制，则会调用Linux out of memory Kill。 Momory Request对于Memory request来讲，它主要在kube-schedule在调度时使用。在cgroup v1中，没有任何控件来指定cgroup将始终保持最小的内存，因此，容器运行时将不使用Pod的内存request字段来设置内存的最小分配量。 而在Cgroup v2中，cgroup引入了一个memory.min设置，用于指定cgroup中的进程的最小内存使用量。Cgroup v2将始终保持内存分配大于memory.min，如果它无法保证，则会调用Linux OOM Kill杀死进程。即cgroup将会至少保证容器有memory.min的内存量可以使用，否则，它将会Kill掉进程（可能是在Cgroup之外），以保证内存可用。 Memory Limit对于Memory limit来讲，memory指定内存限制。如果容器尝试分配更多的内存，超过了内存限制，Linux将通过OOM Kill来终止容器。如果终止的是容器的主进程，将导致容器退出。memory.limit_in_bytes 接口用于设置内存使用限制。然而，与CPU不同的是，它不可能应用内存限制：一旦容器超过内存限制，它就会被OOM杀死。 在cgroups v2中，memory.max类似于cgroupv1中的memory.limit_in_bytes。内存QoS将memory.max映射到spec.containers[].resources.limits.memory 以指定内存使用的硬限制。如果内存消耗超过此水平，内核将调用其 OOM Killer。 cgroups v2还添加了memory.high配置。内存QoS使用memory.high设置内存使用限制。如果违反memory.high限制，则有问题的cgroup会受到限制，并且内核会尝试回收内存，这可能会避免OOM终止。 Memory QoS怎么运行Cgroups v2 内存控制器接口和 Kubernetes 容器资源映射内存QoS使用cgroups v2的内存控制器来保证Kubernetes中的内存资源。此功能使用的 cgroupv2 接口有： memory.max memory.min memory.high Momory QoS 级别memory.max 映射到 Pod 规范中指定的 limits.memory 。 kubelet 和容器运行时在各自的 cgroup 中配置限制。内核强制执行限制以防止容器使用超过配置的资源限制。如果容器中的进程尝试消耗超过指定的限制，内核将终止进程并出现内存不足 (OOM) 错误。 memory.min 映射到 requests.memory ，这会导致保留内核永远不应回收的内存资源。这就是内存 QoS 确保 Kubernetes Pod 内存可用性的方式。如果没有不受保护的可回收内存可用，则会调用 OOM 杀手以提供更多可用内存。 对于内存保护，除了限制内存使用的原始方式之外，内存 QoS 还会限制接近其内存限制的工作负载，确保系统不会因内存使用的零星增加而不堪重负。当您启用 MemoryQoS 功能时，KubeletConfiguration 中将提供一个新字段 memoryThrottlingFactor 。默认设置为 0.9。 memory.high 映射到通过使用 memoryThrottlingFactor 、 requests.memory 和 limits.memory 计算得出的限制限制，如下式所示，并将值向下舍入为最接近的页面尺寸： 注意：如果容器没有指定内存限制，则 limits.memory 将替换节点可分配内存。 注意 memory.high 仅在容器级别 cgroup 上设置，而 memory.min 在容器、pod 和节点级别 cgroup 上设置。 支持的Pod QoS类别Guaranteed pods即resource request &#x3D;&#x3D; limit。通过不设置memory.high，memory qos在这些Pod上会被禁用。这确保了Guaranteed Pod 可以充分利用其内存请求，达到设定的限制，并且不会遇到任何限制。 Burstable pods根据QoS定义，Burstable pods要求Pod中至少有一个容器具有CPU或内存请求或限制。 当requests.memory和limits.memory设置时，公式按原样使用： 1memory.high = pod.spec.containers[i].resources.requests[memory] + MemoryThrottilngFactor * &#123; (pod.spec.containers[i].resources.limits[memory]) - pod.spec.containers[i].resources.requests[memory] &#125; 当设置了 requests.memory 并且未设置 limit.memory 时，limits.memory 会替换公式中的节点可分配内存： 1memory.high = pod.spec.containers[i].resources.requests[memory] + MemoryThrottilngFactor * (Node AllocatableMemory) - pod.spec.containers[i].resources.:requests[memory] 根据 QoS 定义，BestEffort 不需要任何内存或 CPU 限制或请求。对于这种情况，kubernetes 设置 requests.memory &#x3D; 0 并用 Limits.memory 替换公式中的节点可分配内存： 1memory.high = MemoryThrottilngFactor * NodeAllocateableMemory 摘要：只有 Burstable 和 BestEffort QoS 类别中的 Pod 才会设置 memory.high 。Guaranteed QoS Pod 不会设置 memory.high ，因为它们的内存是有保证的。 参考文档 维基百科-服务质量QoS 关于 cgroup v2 Kubernetes 1.27: Quality-of-Service for Memory Resources (alpha)","categories":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"http://jerryblogs.com/categories/Kubernetes/"}],"tags":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"http://jerryblogs.com/tags/Kubernetes/"}]},{"title":"GKE的Prometheus","slug":"GKE的Prometheus","date":"2023-08-09T06:51:52.000Z","updated":"2023-08-11T03:24:30.649Z","comments":true,"path":"2023/08/09/GKE的Prometheus/","link":"","permalink":"http://jerryblogs.com/2023/08/09/GKE%E7%9A%84Prometheus/","excerpt":"","text":"概述PrometheusPrometheus是一个云原生计算基金会项目，是一个系统和服务监控系统。它以给定的时间间隔从配置的目标收集指标，评估规则表达式，显示结果，并可以在观察到指定条件时触发警报。Prometheus是Kubeernetes的事实标准。Prometheus仓库地址：https://github.com/prometheus/prometheus。 Prometheus包含以下组件： Prometheus Server: 数据搜集：定期从配置的位置抓取指标 数据存储：存储抓取到的指标数据 查询：提供PromQL 规则提醒或警报：对指标数据进行规则评估，生成新的时间序列或触发告警 Exporters：这些是外部进程，用于将现有的第三方服务和应用程序指标转换为Prometheus可以抓取的格式。例如，node_exporter用于提取操作系统级指标。 Pushgateway：用于短暂的批处理作业。由于 Prometheus 本质上是一个抓取模型，因此 Pushgateway 允许短暂的任务推送指标，以便 Prometheus 可以随后抓取 Prometheus Pushgateway仓库。允许临时和批处理作业向 Prometheus 公开其指标，这些类型的作业可能存在的时间不够长，无法抓取，因此他们可以将其指标推送到 Pushgateway，然后，Pushgateway将这些指标公开给Prometheus。 AlertManager：管理警报。当 Prometheus 检测到某些条件满足预先配置的规则时，它会向 AlertManager 发送警报，然后 AlertManager 负责通知或采取其他操作。 Client Libraries：允许用户在其应用程序中集成和导出自定义指标。 Google Cloud Managed Service for PrometheusGoogle Cloud Managed Service for Prometheus是Google对于Prometheus的托管服务，它托管指标的完全托管的收集、存储和查询服务。它建立在Monarch之上，Monarch是一个全球可扩展数据存储服务，为Google的所有应用程序监控提供支持。在标准 Prometheus 部署中，数据收集、查询评估、规则和提醒评估，以及数据存储都在单个 Prometheus 服务器中处理。Managed Service for Prometheus 将这些功能的责任拆分为多个组件： 数据收集由代管式收集器、自行部署的收集器、OpenTelemetry 收集器或 Ops Agent 处理，这些收集器或代理会爬取本地导出器，并将收集到的数据转发到 Monarch。这些收集器可用于 Kubernetes 和传统工作负载，并且可以在任何地方运行，包括其他云和本地部署。 查询评估由 Monarch 处理，它会在所有 Google Cloud 区域和多达 1,000 个 Google Cloud 项目中执行查询并联合结果。 规则和提醒评估由本地运行和本地配置的规则评估器组件处理，这些组件针对全局 Monarch 数据存储区执行规则和提醒，并将所有触发的提醒转发到 Prometheus AlertManager 数据存储由 Monarch 处理，它会将所有 Prometheus 数据存储 24 个月，不产生额外费用。 Grafana 连接到全局 Monarch 数据存储区，而不是连接到各个 Prometheus 服务器。 部署代管式搜集器以下默认已经启用了 Cloud Monitoring API 的 Google Cloud 项目，创建了GKE集群并在本地设置好了gcloud和kubectl工具。 启用代管式搜集器启用代管式收集会在集群中安装以下组件： gmp-operator Deployment，用于为 Managed Service for Prometheus 部署 Kubernetes Operator。 rule-evaluator Deployment，用于配置并运行提醒和记录规则。 collector DaemonSet，用于从与收集器所在节点上运行的 Pod 爬取指标来横向扩缩收集。 alertmanager StatefulSet，配置为向首选通知渠道发送触发的提醒。 在GKE 1.25 版或更高版本的 GKE Autopilot 集群和GKE 1.27 版或更高版本的 GKE Standard 集群，代管式搜集器是默认启用的。 1gcloud container clusters update CLUSTER_NAME --enable-managed-prometheus --zone ZONE/REGION 配置PodMonitoring资源假设有一个在metrics 端口上发出 example_requests_total 计数器指标和 example_random_numbers 直方图指标的应用： 1234567891011apiVersion: monitoring.googleapis.com/v1kind: PodMonitoringmetadata: name: prom-examplespec: selector: matchLabels: app: prom-example endpoints: - port: metrics interval: 30s 启用目标状态启用目标状态可以检查资源的目标状态，通过如下启动： 12345678apiVersion: monitoring.googleapis.com/v1kind: OperatorConfigmetadata: namespace: gmp-public name: configfeatures: targetStatus: enabled: true 查询应用注入的指标参见 https://cloud.google.com/stackdriver/docs/managed-prometheus/query?hl=zh-cn#gmp-in-monitoring 配置Prometheus和Grafana部署Prometheus前端页面123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172apiVersion: apps/v1kind: Deploymentmetadata: name: frontendspec: replicas: 2 selector: matchLabels: app: frontend template: metadata: labels: app: frontend spec: automountServiceAccountToken: true affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: kubernetes.io/arch operator: In values: - arm64 - amd64 - key: kubernetes.io/os operator: In values: - linux containers: - name: frontend env: - name: AUTH_USERNAME value: AUTH_USERNAME - name: AUTH_PASSWORD value: AUTH_PASSWORD image: gke.gcr.io/prometheus-engine/frontend:v0.7.0-gke.0 args: - &quot;--web.listen-address=:9090&quot; - &quot;--query.project-id=$PROJECT_ID&quot; ports: - name: web containerPort: 9090 readinessProbe: httpGet: path: /-/ready port: web securityContext: allowPrivilegeEscalation: false capabilities: drop: - all privileged: false runAsGroup: 1000 runAsNonRoot: true runAsUser: 1000 livenessProbe: httpGet: path: /-/healthy port: web---apiVersion: v1kind: Servicemetadata: name: frontendspec: clusterIP: None selector: app: frontend ports: - name: web port: 9090 修改上述AUTH_USERNAME，AUTH_PASSWORD，然后将$PROJECT_ID修改为字面量，如果不知道可以通过gcloud projects list查看。 Grafana页面12kubectl -n monitor apply -f https://raw.githubusercontent.com/GoogleCloudPlatform/prometheus-engine/beb779d32f4dd531a3faad9f2916617b8d9baefd/examples/grafana.yamlkubectl -n monitor port-forward svc/grafana 3000 然后可以在本地访问http://localhost:3000，即可看到Grafana页面。 参考文档 Google Cloud Managed Service for Prometheus 代管式收集使用入门 配置查询页面","categories":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"http://jerryblogs.com/categories/Kubernetes/"},{"name":"运维开发","slug":"Kubernetes/运维开发","permalink":"http://jerryblogs.com/categories/Kubernetes/%E8%BF%90%E7%BB%B4%E5%BC%80%E5%8F%91/"}],"tags":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"http://jerryblogs.com/tags/Kubernetes/"}]},{"title":"docker-checkpoint-容器运行时快照","slug":"docker-checkpoint-容器运行时快照","date":"2023-07-04T11:24:44.000Z","updated":"2023-07-04T12:01:37.911Z","comments":true,"path":"2023/07/04/docker-checkpoint-容器运行时快照/","link":"","permalink":"http://jerryblogs.com/2023/07/04/docker-checkpoint-%E5%AE%B9%E5%99%A8%E8%BF%90%E8%A1%8C%E6%97%B6%E5%BF%AB%E7%85%A7/","excerpt":"","text":"介绍checkpoint&#x2F;restore和restore与CRIUcheckpoint和restore是一个实验性的功能，它允许讲一个正在运行的容器冻结，并且将其状态转换成磁盘上的文件集合，使你可以从checkpoint恢复容器的状态。checkpoint基于CRIU实现，CRIU是一个Linux应用程序，在用户空间将一个进程树冻结并将其状态保存到磁盘，使得容器或应用程序可以实时迁移，远程调试和快照等。CRIU的工作过程是： 冻结正在运行的应用程序 将整个进程树的地址空间和状态检查到镜像文件的集合 从检查点恢复进程树 从冻结点恢复应用程序 CRIU与Docker集成的现状以及使用方式从Docker1.17开始，CRIU是Docker的实验性功能。 安装，由于CRIU是Docker的外部依赖，所以在使用Docker checkpoint之前，需要安装CRIU。 Debian 10 1234wget http://github.com/checkpoint-restore/criu/archive/v3.18/criu-3.18.tar.gzapt install libprotobuf-dev libprotobuf-c-dev protobuf-c-compiler protobuf-compiler python3-protobufapt install pkg-config python-ipaddress libbsd-dev libnftables-dev libcap-dev libnet1-dev libaio-dev libgnutls28-dev python3-future iproute2 libnl-3-dev python3-pipcd ~/criu-3.18 &amp;&amp; make 此时已经编译完成，你可以直接运行~/criu-3.18/criu/criu或者： 12apt install asciidoc xmltomake install 完成以后，运行criu -h不报错即安装成功。 由于checkpoint&#x2F;restore是一个实验性功能，我们需要手动启用： 12echo &quot;&#123;\\&quot;experimental\\&quot;: true&#125;&quot; &gt;&gt; /etc/docker/daemon.jsonsystemctl restart docker 运行一个测试容器 1docker run -d --name looper busybox /bin/sh -c &#x27;i=0; while true; do echo $i; i=$(expr $i + 1); sleep 1; done&#x27; 运行docker logs -f 容器ID可以查看i在增加。 运行 1docker checkpoint create looper checkpoint1 可以看到容器已经停止运行： 123root@VM-12-4-debian:~# docker ps -aCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES04b7a82b2c86 busybox &quot;/bin/sh -c &#x27;i=0; wh…&quot; About a minute ago Exited (137) 5 seconds ago looper 恢复容器 我们恢复容器的状态： 1docker start --checkpoint=checkpoint1 looper 这时候再次查看容器日志，我们可以发现i没有从0开始，而是从checkpoint的状态继续递增。 这个时候，如果我们查看/var/lib/docker/containers/&lt;容器ID&gt;/checkpoints，我们可以看到目录下的一堆img文件，这便是criu生成的镜像文件。 --checkpoint-dir选项自定义存储路径 1234# 新建一个目录来存储restore后的镜像文件mkdir /tmp/ck/# 创建checkpointdocker checkpoint create --checkpoint-dir=/tmp/ck/ looper checkpoint2 查看/tmp/ck/，我们可以发现，/tmp/ck/checkpoint2/目录下有一堆img文件。 使用自定义目录恢复容器状态 我们可以使用--checkpoint-dir选项来指定目录，如： 1docker start --checkpoint-dir=/tmp/ck/ --checkpoint=checkpoint2 looper 需要注意的点 当docker run 添加了–security-opt选项之后，可能会出现如下报错 1Error (criu/cr-dump.c:203): 9143 has rseq but kernel lacks get_rseq_conf feature 当不添加--security-opt seccomp:unconfined可以checkout。除此之外，将内核升级到v5.15可以进行checkout。 Error response from daemon: custom checkpointdir is not supported 使用自定义checkpointdir发现不支持。可能是你的内核不支持，CRIU支持的必须开启的内核Feature。 一个解决方案是，将checkpointdir目录内容复制到&#x2F;var&#x2F;lib&#x2F;docker&#x2F;containers&#x2F;&lt;容器ID&gt;&#x2F;checkpoints内，在恢复时去除–checkpoint-dir选项，即可启动，如： 12cp -r /tmp/ck/* /var/lib/docker/containers/&lt;容器ID&gt;/checkpointsdocker start --checkpoint=checkpoint2 looper3 即可恢复成功。 原理Checkpoint过程该过程严重依赖&#x2F;proc目录，CRIU从&#x2F;proc目录搜集： 文件描述符信息（通过&#x2F;proc&#x2F;$pid&#x2F;fd和&#x2F;proc&#x2F;$pid&#x2F;fdinfo） 管道参数 内存映射（通过&#x2F;proc&#x2F;$pid&#x2F;maps和&#x2F;proc&#x2F;$pid&#x2F;map_files&#x2F;） 等等 搜集进程树并冻结它进程组组长的pid通过命令行(–tree)获取。通过&#x2F;proc&#x2F;$pid&#x2F;task&#x2F;目录搜集正在运行的线程，并且通过&#x2F;proc&#x2F;$pid&#x2F;task&#x2F;$tid&#x2F;children递归地搜集子线程。使用ptrace的PTRACE_SEIZE命令停止task。 搜集task资源并且进行存储CRIU通过读取所有已经搜集的task的信息，并且将它们写到文件中，主要是通过： 虚拟内存区域从&#x2F;proc&#x2F;$pid&#x2F;smaps解析，映射文件从&#x2F;proc&#x2F;$pid&#x2F;map_files链接读取 文件描述符通过&#x2F;proc&#x2F;$pid&#x2F;fd读取 task的内核参数通过ptrace接口和&#x2F;proc&#x2F;$pid&#x2F;stat读取 CRIU通过ptrace接口注入寄生代码到task当中，一开始，我们只为CS:IP的mmap系统调用注入了几个字节，任务在启动的时候就有了。然后ptrace允许我们运行一个注入的系统调用，我们分配足够的内存给我们需要转储的寄生虫代码块。之后，寄生代码被复制到dumpee地址空间内的新位置，CS:IP设置分别指向我们的寄生代码。 从上下文解析来看，CRIU提供更多的信息，比如Credentials和Contents of memory 清除在所有东西都转储之后（比如内存页，只能从dumpee地址空间内写出），我们再次使用trace工具，通过删除所有的寄生代码和恢复原始代码来治愈dumpee。然后CRIU从任务中分离出来，它们继续运行。 Restore过程The restore procedure (aka restorer) is done by CRIU morphing itself into the tasks it restores. On the top-level it consists of 4 steps 共享资源读取img文件并找到哪些进程共享哪些资源，然后共享资源被其中一个进程恢复，其他进程要么在第二阶段继承一个资源，要么以其他方式获取（如会话）。例如，通过unix套接字与SCM_CREDS消息一起发送的共享文件，或通过memfd文件描述符恢复的共享内存区域。 Fork进程树在这一步，CRIU多次调用fork()来重新创建需要恢复的进程。注意，线程不是在这里恢复的，而是在第四步。 恢复基本任务资源这里 CRIU 恢复所有资源，但是 内存映射精确位置 定时器 凭据 线程 上述资源是最后恢复的，原因是在这个阶段，CRIU打开文件，准备命名空间，映射（并填充数据）私有内存区域，创建套接字，调用chdir()和chroot()，并做一些其他工作。 切换到恢复上下文，恢复其余部分并继续恢复器blob的原因很简单。由于 criu 会转变为目标进程，因此它必须取消映射其所有内存并放回目标内存。这样做时，内存中应该存在一些代码（执行 munmap 和 mmap 的代码）。因此，引入了恢复程序 blob。这是一小段代码，与 criu 映射和目标映射不相交。在第 2 阶段结束时，criu 跳入该 blob 并恢复内存映射。在同一位置，我们恢复计时器，以免它们过早触发，在这里，我们恢复凭据，让criu执行特权操作（例如 fork-with-pid）和线程，以免它们遭受突然的内存布局更改。 See also: restorer context, tree after restore 参考文档 docker checkpoint https://criu.org/Main_Page How did the Quake demo from DockerCon Work? CRIU in Docker CRIU Github仓库 Error response from daemon: custom checkpointdir is not supported has rseq but kernel lacks get_rseq_conf feature design of how Checkpoint and Restore work in CRIU","categories":[{"name":"Docker","slug":"Docker","permalink":"http://jerryblogs.com/categories/Docker/"}],"tags":[{"name":"Docker","slug":"Docker","permalink":"http://jerryblogs.com/tags/Docker/"}]},{"title":"pflag-flag库的直接替代","slug":"pflag-flag库的直接替代","date":"2023-06-30T09:18:07.000Z","updated":"2023-06-30T09:33:15.151Z","comments":true,"path":"2023/06/30/pflag-flag库的直接替代/","link":"","permalink":"http://jerryblogs.com/2023/06/30/pflag-flag%E5%BA%93%E7%9A%84%E7%9B%B4%E6%8E%A5%E6%9B%BF%E4%BB%A3/","excerpt":"","text":"为什么要使用pflagGolang是一门性能十分强大语言。当我们在用Golang开发一个命令行程序的时候，我们经常会用到官方的flag库，但是官方的flag库使用并编译运行过后，你会发现它与我们经常使用的POSIX&#x2F;GNU风格的命令行程序有所不同。比如说： 12345678&gt;&gt;&gt; go run artifact.go -hUsage of /var/folders/m7/gkmw5fms7qq62k2ft42_ts1c0000gn/T/go-build280567700/b001/exe/artifact: -conn string 连接器名称,多个连接器以逗号分隔 -cookie string 携带的cookie,用来做身份验证 (default &quot;xxxxxx&quot;) -delete 删除旧的压缩包 (default true) 我们开发了一个名为artifact的命令行程序，运行go run artifact.go -h打印帮助信息，我们会发现，不同的flag为-flag，而不是--flag并且提供一个较短的别名如-f以供我们快速输入。 那么当我们使用了pflag之后，帮助信息就会变成这个样子： 1234&gt;&gt;&gt; go run artifact.go -hUsage of /var/folders/m7/gkmw5fms7qq62k2ft42_ts1c0000gn/T/go-build1444358319/b001/exe/artifact: --conn string 连接器名称,多个连接器以逗号分隔 --cookie string 携带的cookie,用来做身份验证 (default &quot;xxxxx&quot;) 可以看到，它已经是POSIX&#x2F;GNU风格的命令行程序了。 怎么用使用方式其实很简单，因为pflag是flag的直接替换包，我们只需要将import &quot;flag&quot;直接替换为flag &quot;github.com/spf13/pflag&quot;。 在替换之前，我们需要先安装pflag这个包，在命令行运行： 1go get github.com/spf13/pflag 要进行测试，可以使用： 1go test github.com/spf13/pflag 上面的命令行程序我们并没有使用pflag为不同的选项提供一个shorthands，直接替换后，我们可以使用如 1flag.StringVarP(&amp;cookie, &quot;cookie&quot;, &quot;c&quot;, &quot;xxxx&quot;, &quot;携带的cookie,用来做身份验证&quot;) 来直接为cookie提供一个-c的shorthands，效果为： 1234&gt;&gt;&gt; go run artifact.go -hUsage of /var/folders/m7/gkmw5fms7qq62k2ft42_ts1c0000gn/T/go-build3700657530/b001/exe/artifact: --conn string 连接器名称,多个连接器以逗号分隔 -c, --cookie string 携带的cookie,用来做身份验证 (default &quot;xxxxx&quot;)","categories":[{"name":"Golang","slug":"Golang","permalink":"http://jerryblogs.com/categories/Golang/"}],"tags":[{"name":"Golang","slug":"Golang","permalink":"http://jerryblogs.com/tags/Golang/"}]},{"title":"Kubernetes中的Kubelet组件","slug":"Kubernetes中的Kubelet组件","date":"2023-06-29T18:34:14.000Z","updated":"2023-09-11T13:52:45.717Z","comments":true,"path":"2023/06/30/Kubernetes中的Kubelet组件/","link":"","permalink":"http://jerryblogs.com/2023/06/30/Kubernetes%E4%B8%AD%E7%9A%84Kubelet%E7%BB%84%E4%BB%B6/","excerpt":"","text":"概述Kubelet是Kubernetes集群中的核心节点，运行在每一个节点之上，负责管理节点的容器运行时。 节点注册：在启动时将节点注册到控制平面，以便主控制平面知道该节点的存在 容器启动与监控（Container Startup and Monitoring）：kubelet负责启动和监控在节点上运行的容器。它根据从主控制平面接收到的Pod定义，创建并管理相应的容器。它监视容器的运行状态，如果容器停止或崩溃，kubelet会重新启动容器以确保其持续运行。 资源管理（Resource Management）：kubelet负责监视节点的资源使用情况，并根据预设的资源限制和请求来管理容器的资源分配。它会与容器运行时（如Docker、Containerd等）交互，确保容器只使用其分配的资源量。 存储卷管理（Volume Management）：kubelet负责挂载和管理Pod中定义的存储卷。它会与存储插件进行交互，将存储卷挂载到容器中，并确保存储卷的可用性和一致性。 安全与准入控制（Security and Admission Control）：kubelet负责执行容器的安全策略和准入控制机制。它会与容器运行时进行交互，确保容器在运行时具备必要的安全特性，并执行一些策略，如限制容器的权限和网络访问等。 网络管理（Networking）：kubelet负责管理容器的网络设置。它会与网络插件进行交互，为容器分配IP地址，并配置容器间和容器与外部网络的通信。 节点状态报告（Node Status Reporting）：kubelet会定期向主控制平面报告节点的状态信息，包括节点的资源使用情况、已运行的容器列表等。这样主控制平面就能了解到节点的健康状况和容器的运行情况。 在这里，我们主要看： 节点是如何被注册的？ 容器如何启动？如何被监控 kubelet如何进行资源管理？ 源码分析cmd首先我们先看cmd模块，看看kubelet的初始化过程以及对外暴露了哪些使用接口。 12345func main() &#123; command := app.NewKubeletCommand() code := cli.Run(command) os.Exit(code)&#125; app.NewKubeletCommand()这个主要是创建了一个cobra.Command，关于cobra.Command，请看cobra的介绍和pflag的介绍，让我们来看下做了什么。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181// NewKubeletCommand creates a *cobra.Command object with default parametersfunc NewKubeletCommand() *cobra.Command &#123; // 定义了kubelet命令 cleanFlagSet := pflag.NewFlagSet(componentKubelet, pflag.ContinueOnError) // 类似于cobra的智能建议功能，这里是将输错的选项自动识别为正确的选项 cleanFlagSet.SetNormalizeFunc(cliflag.WordSepNormalizeFunc) // 创建一些默认的flag，包含默认的值 /* func NewKubeletFlags() *KubeletFlags &#123; return &amp;KubeletFlags&#123; ContainerRuntimeOptions: *NewContainerRuntimeOptions(), // 容器运行时期望的选项 CertDirectory: &quot;/var/lib/kubelet/pki&quot;, // pki证书的目录，请查看https://kubernetes.io/zh-cn/docs/setup/best-practices/certificates/ RootDirectory: filepath.Clean(defaultRootDir), // kubelet相关文件的目录，默认是/var/lib/kubelet MaxContainerCount: -1, MaxPerPodContainerCount: 1, MinimumGCAge: metav1.Duration&#123;Duration: 0&#125;, RegisterSchedulable: true, // 注册节点的实现，使节点可调度 NodeLabels: make(map[string]string), // 节点标签 &#125; &#125; */ kubeletFlags := options.NewKubeletFlags() kubeletConfig, err := options.NewKubeletConfiguration() // programmer error if err != nil &#123; klog.ErrorS(err, &quot;Failed to create a new kubelet configuration&quot;) os.Exit(1) &#125; cmd := &amp;cobra.Command&#123; Use: componentKubelet, // 字面量，值为“kubelet” Long: `The kubelet is the primary &quot;node agent&quot; that runs on eachnode. It can register the node with the apiserver using one of: the hostname; a flag tooverride the hostname; or specific logic for a cloud provider.The kubelet works in terms of a PodSpec. A PodSpec is a YAML or JSON objectthat describes a pod. The kubelet takes a set of PodSpecs that are provided throughvarious mechanisms (primarily through the apiserver) and ensures that the containersdescribed in those PodSpecs are running and healthy. The kubelet doesn&#x27;t managecontainers which were not created by Kubernetes.Other than from an PodSpec from the apiserver, there are two ways that a containermanifest can be provided to the Kubelet.File: Path passed as a flag on the command line. Files under this path will be monitoredperiodically for updates. The monitoring period is 20s by default and is configurablevia a flag.HTTP endpoint: HTTP endpoint passed as a parameter on the command line. This endpointis checked every 20 seconds (also configurable with a flag).`, // The Kubelet has special flag parsing requirements to enforce flag precedence rules, // so we do all our parsing manually in Run, below. // DisableFlagParsing=true provides the full set of flags passed to the kubelet in the // `args` arg to Run, without Cobra&#x27;s interference. DisableFlagParsing: true, SilenceUsage: true, RunE: func(cmd *cobra.Command, args []string) error &#123; // initial flag parse, since we disable cobra&#x27;s flag parsing if err := cleanFlagSet.Parse(args); err != nil &#123; return fmt.Errorf(&quot;failed to parse kubelet flag: %w&quot;, err) &#125; // check if there are non-flag arguments in the command line cmds := cleanFlagSet.Args() if len(cmds) &gt; 0 &#123; return fmt.Errorf(&quot;unknown command %+s&quot;, cmds[0]) &#125; // short-circuit on help help, err := cleanFlagSet.GetBool(&quot;help&quot;) if err != nil &#123; return errors.New(`&quot;help&quot; flag is non-bool, programmer error, please correct`) &#125; if help &#123; return cmd.Help() &#125; // short-circuit on verflag verflag.PrintAndExitIfRequested() // 从初始的配置中设置功能门 if err := utilfeature.DefaultMutableFeatureGate.SetFromMap(kubeletConfig.FeatureGates); err != nil &#123; return fmt.Errorf(&quot;failed to set feature gates from initial flags-based config: %w&quot;, err) &#125; // validate the initial KubeletFlags // ValidateKubeletFlags 验证 Kubelet 的配置标志，并在无效时返回错误。主要是遍历nodelabel并验证其合法性，如果不属于kubelet标签，则返回未知标签，如果没通过合法性验证，主要是名称符合DNS1123subdomain规则(https://datatracker.ietf.org/doc/html/rfc1123和https://kuboard.cn/learning/k8s-intermediate/obj/names.html#names)值不超过63个字符 if err := options.ValidateKubeletFlags(kubeletFlags); err != nil &#123; return fmt.Errorf(&quot;failed to validate kubelet flags: %w&quot;, err) &#125; // 镜像垃圾收集器（image garbage collector）负责清理不再使用的镜像。然而，--pod-infra-container-image所指定的镜像不会被镜像垃圾收集器清理，因为它是作为Pod基础设施容器而存在的，始终被使用 if cleanFlagSet.Changed(&quot;pod-infra-container-image&quot;) &#123; klog.InfoS(&quot;--pod-infra-container-image will not be pruned by the image garbage collector in kubelet and should also be set in the remote runtime&quot;) &#125; // load kubelet config file, if provided if configFile := kubeletFlags.KubeletConfigFile; len(configFile) &gt; 0 &#123; kubeletConfig, err = loadConfigFile(configFile) if err != nil &#123; return fmt.Errorf(&quot;failed to load kubelet config file, error: %w, path: %s&quot;, err, configFile) &#125; // We must enforce flag precedence by re-parsing the command line into the new object. // This is necessary to preserve backwards-compatibility across binary upgrades. // See issue #56171 for more details. if err := kubeletConfigFlagPrecedence(kubeletConfig, args); err != nil &#123; return fmt.Errorf(&quot;failed to precedence kubeletConfigFlag: %w&quot;, err) &#125; // update feature gates based on new config if err := utilfeature.DefaultMutableFeatureGate.SetFromMap(kubeletConfig.FeatureGates); err != nil &#123; return fmt.Errorf(&quot;failed to set feature gates from initial flags-based config: %w&quot;, err) &#125; &#125; // Config and flags parsed, now we can initialize logging. logs.InitLogs() if err := logsapi.ValidateAndApplyAsField(&amp;kubeletConfig.Logging, utilfeature.DefaultFeatureGate, field.NewPath(&quot;logging&quot;)); err != nil &#123; return fmt.Errorf(&quot;initialize logging: %v&quot;, err) &#125; cliflag.PrintFlags(cleanFlagSet) // We always validate the local configuration (command line + config file). // This is the default &quot;last-known-good&quot; config for dynamic config, and must always remain valid. if err := kubeletconfigvalidation.ValidateKubeletConfiguration(kubeletConfig, utilfeature.DefaultFeatureGate); err != nil &#123; return fmt.Errorf(&quot;failed to validate kubelet configuration, error: %w, path: %s&quot;, err, kubeletConfig) &#125; if (kubeletConfig.KubeletCgroups != &quot;&quot; &amp;&amp; kubeletConfig.KubeReservedCgroup != &quot;&quot;) &amp;&amp; (strings.Index(kubeletConfig.KubeletCgroups, kubeletConfig.KubeReservedCgroup) != 0) &#123; klog.InfoS(&quot;unsupported configuration:KubeletCgroups is not within KubeReservedCgroup&quot;) &#125; // construct a KubeletServer from kubeletFlags and kubeletConfig kubeletServer := &amp;options.KubeletServer&#123; KubeletFlags: *kubeletFlags, KubeletConfiguration: *kubeletConfig, &#125; // use kubeletServer to construct the default KubeletDeps kubeletDeps, err := UnsecuredDependencies(kubeletServer, utilfeature.DefaultFeatureGate) if err != nil &#123; return fmt.Errorf(&quot;failed to construct kubelet dependencies: %w&quot;, err) &#125; if err := checkPermissions(); err != nil &#123; klog.ErrorS(err, &quot;kubelet running with insufficient permissions&quot;) &#125; // make the kubelet&#x27;s config safe for logging config := kubeletServer.KubeletConfiguration.DeepCopy() for k := range config.StaticPodURLHeader &#123; config.StaticPodURLHeader[k] = []string&#123;&quot;&lt;masked&gt;&quot;&#125; &#125; // log the kubelet&#x27;s config for inspection klog.V(5).InfoS(&quot;KubeletConfiguration&quot;, &quot;configuration&quot;, config) // set up signal context for kubelet shutdown ctx := genericapiserver.SetupSignalContext() utilfeature.DefaultMutableFeatureGate.AddMetrics() // run the kubelet return Run(ctx, kubeletServer, kubeletDeps, utilfeature.DefaultFeatureGate) &#125;, &#125; // keep cleanFlagSet separate, so Cobra doesn&#x27;t pollute it with the global flags kubeletFlags.AddFlags(cleanFlagSet) options.AddKubeletConfigFlags(cleanFlagSet, kubeletConfig) options.AddGlobalFlags(cleanFlagSet) cleanFlagSet.BoolP(&quot;help&quot;, &quot;h&quot;, false, fmt.Sprintf(&quot;help for %s&quot;, cmd.Name())) // ugly, but necessary, because Cobra&#x27;s default UsageFunc and HelpFunc pollute the flagset with global flags const usageFmt = &quot;Usage:\\n %s\\n\\nFlags:\\n%s&quot; cmd.SetUsageFunc(func(cmd *cobra.Command) error &#123; fmt.Fprintf(cmd.OutOrStderr(), usageFmt, cmd.UseLine(), cleanFlagSet.FlagUsagesWrapped(2)) return nil &#125;) cmd.SetHelpFunc(func(cmd *cobra.Command, args []string) &#123; fmt.Fprintf(cmd.OutOrStdout(), &quot;%s\\n\\n&quot;+usageFmt, cmd.Long, cmd.UseLine(), cleanFlagSet.FlagUsagesWrapped(2)) &#125;) return cmd&#125; 首先用cobra和pflag库为模版使得kubelet更加符合类Unix风格，同时定义了组件的名称、描述、错误处理等属性。 cobra.Command的RunE属性：该属性会被调用并返回结果，程序以它为主要的入口。 (1). 首先如果命令中存在help或者version则使用短路原则打印kubelet的帮助信息 (2). 加载配置文件并给kubeletConfig结构体赋值，同时设置功能门 (3). 初始化日志 (4). 验证本地配置文件+命令行的有效性 (5). 通过使用kubeletConfig来构造kubeletServer然后构造kubeletDeps (6). 检查当前运行的权限，必须以uid必须为0，即应该以root权限运行 (7). 运行kubeletServer（见下文） 一些cobra全局标志的处理，在这里不做介绍，不是我们主要的关注点 kubeletServer的启动我们看上文的Run方法： 1234567891011func Run(ctx context.Context, s *options.KubeletServer, kubeDeps *kubelet.Dependencies, featureGate featuregate.FeatureGate) error &#123; ... 日志打印和兼容windows启动的处理 ... if err := run(ctx, s, kubeDeps, featureGate); err != nil &#123; return fmt.Errorf(&quot;failed to run Kubelet: %w&quot;, err) &#125; return nil&#125; 可以看到Run方法除了做一些日志打印和Windows平台的兼容之外，就是直接调用了run方法来做具体的启动处理。这里的Run方法有四个参数： context.Context *options.KubeletServer: kubeletServer的配置 *kubelet.Dependencies: 注入依赖项，从配置中取得 featuregate.FeatureGate: 开启的功能门 我们来看KubeletServer: 1234type KubeletServer struct &#123; KubeletFlags kubeletconfig.KubeletConfiguration&#125; 它是kubeletServer用来启动服务的配置，其中，KubeletFlags是一些无法被修改的内容，当节点（kubelet Server）启动后，不能被修改。这里主要是存放一些不共享的内容，比如说： 1234type KubeletFlags struct &#123; HostnameOverride string NodeIP string&#125; HostnameOverride和NodeIP分别代表节点的主机名和节点的IP，用来识别节点。kubeletconfig.KubeletConfiguration指的是可以在集群之间共享的配置集，它是一个runtime对象，意味着在运行时可能被修改。 来看看run方法： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106func run(ctx context.Context, s *options.KubeletServer, kubeDeps *kubelet.Dependencies, featureGate featuregate.FeatureGate) (err error) &#123; 1. 使用utilfeature.DefaultMutableFeatureGate.SetFromMap设置初始featureGate 2. 使用options.ValidateKubeletServer(s)验证初始featureGate 3. 使用utilfeature.DefaultFeatureGate.Enabled(features.MemoryQoS)，如果打开了MemoryQoS，则打印警告信息 a. MemoryQoS是一个关于内存资源服务质量提升的功能门，是在kubernetes v1.22引入的一个alpha功能，改进了Linux节点实现内存资源请求的限制。 b. 在cgroup v1，当CPU资源即将达到限制时，Kubernetes会开始限制Pod的CPU资源的使用，Pod并不会终止；但是cgroup v1无法压缩内存的使用，当超过内存限制时，容器将会被OOM终止。 c. cgroup v2，提供了丰富的参数用于实现内存预留与分配限速，MemoryQos使用memory.min/memory.high来实现对内存的限速分配，从而避免Burstable Pod被Kill和节点瞬时压力暴涨的风险 4. 获取锁文件，避免两个kubelet同时运行 ... 5. 将kubelet的配置暴露在/configz端点，可以通过访问/api/v1/nodes/[node_name]/proxy/configz来获取kubelet的配置 // 初始化可用的API版本，并将该配置保存到配置（全局变量configs） err = initConfigz(&amp;s.KubeletConfiguration) // 见部分具体实现细节 if err != nil &#123; klog.ErrorS(err, &quot;Failed to register kubelet configuration with configz&quot;) &#125; ... 6. 设置云服务商接口 // 返回一个云商的Interface抽象、可插拔接口 if kubeDeps.Cloud == nil &#123; // s.CloudProvider是一个字符串，表示云服务商 // 判断是否是内部云服务商 if !cloudprovider.IsExternal(s.CloudProvider) &#123; // cloudprovider.DeprecationWarningForProvider 如果是内部云服务提供商，提示：内部云服务提供商已被废弃 // 使用external-cloud-provider cloudprovider.DeprecationWarningForProvider(s.CloudProvider) // 初始化云服务商实例 // s.CloudProvider是云服务商的名字，s.CloudConfigFile是云服务商的配置文件 cloud, err := cloudprovider.InitCloudProvider(s.CloudProvider, s.CloudConfigFile) if err != nil &#123; return err &#125; if cloud != nil &#123; klog.V(2).InfoS(&quot;Successfully initialized cloud provider&quot;, &quot;cloudProvider&quot;, s.CloudProvider, &quot;cloudConfigFile&quot;, s.CloudConfigFile) &#125; kubeDeps.Cloud = cloud &#125; &#125; // 获取本机hostname，如果设置了s.HostnameOverride就用s.HostnameOverride，转换为小写 hostName, err := nodeutil.GetHostname(s.HostnameOverride) ... // 通过cloud.Instance.CurrentNodeName接口获取当前节点的节点名称，大多数时候是主机名 nodeName, err := getNodeName(kubeDeps.Cloud, hostName) 7. 一大串身份验证相关的逻辑，启动证书管理器，身份验证器，使用kubeconfig, bootstrap kubconfig和cert证书向apiserver新建或轮转kubectl证书 // 以下全是kubeconfig身份验证相关 switch &#123; case standaloneMode: kubeDeps.KubeClient = nil kubeDeps.EventClient = nil kubeDeps.HeartbeatClient = nil klog.InfoS(&quot;Standalone mode, no API client&quot;) case kubeDeps.KubeClient == nil, kubeDeps.EventClient == nil, kubeDeps.HeartbeatClient == nil: // 构建kubeconfig证书（新获取或证书轮转，通过kubeconfig，bootstrap kubeconfig和cert证书） clientConfig, onHeartbeatFailure, err := buildKubeletClientConfig(ctx, s, kubeDeps.TracerProvider, nodeName) if err != nil &#123; return err &#125; if onHeartbeatFailure == nil &#123; return errors.New(&quot;onHeartbeatFailure must be a valid function other than nil&quot;) &#125; kubeDeps.OnHeartbeatFailure = onHeartbeatFailure kubeDeps.KubeClient, err = clientset.NewForConfig(clientConfig) if err != nil &#123; return fmt.Errorf(&quot;failed to initialize kubelet client: %w&quot;, err) &#125; // make a separate client for events // 为event创建一个单独的证书 eventClientConfig := *clientConfig eventClientConfig.QPS = float32(s.EventRecordQPS) eventClientConfig.Burst = int(s.EventBurst) kubeDeps.EventClient, err = v1core.NewForConfig(&amp;eventClientConfig) if err != nil &#123; return fmt.Errorf(&quot;failed to initialize kubelet event client: %w&quot;, err) &#125; // make a separate client for heartbeat with throttling disabled and a timeout attached // 为心跳创建一个单独的客户端，禁用节流并附加超时 heartbeatClientConfig := *clientConfig heartbeatClientConfig.Timeout = s.KubeletConfiguration.NodeStatusUpdateFrequency.Duration // The timeout is the minimum of the lease duration and status update frequency leaseTimeout := time.Duration(s.KubeletConfiguration.NodeLeaseDurationSeconds) * time.Second if heartbeatClientConfig.Timeout &gt; leaseTimeout &#123; heartbeatClientConfig.Timeout = leaseTimeout &#125; heartbeatClientConfig.QPS = float32(-1) kubeDeps.HeartbeatClient, err = clientset.NewForConfig(&amp;heartbeatClientConfig) if err != nil &#123; return fmt.Errorf(&quot;failed to initialize kubelet heartbeat client: %w&quot;, err) &#125; &#125; if kubeDeps.Auth == nil &#123; auth, runAuthenticatorCAReload, err := BuildAuth(nodeName, kubeDeps.KubeClient, s.KubeletConfiguration) if err != nil &#123; return err &#125; kubeDeps.Auth = auth runAuthenticatorCAReload(ctx.Done()) &#125;&#125; 部分具体实现细节cloud-provider&#x2F;plogin.go这个文件是一些云服务商相关的接口 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222// Factory是一个返回cloudprovider.Interface的函数工厂，config用于加载特定云商的配置。// 如果没有配置，则这个参数将是niltype Factory func(config io.Reader) (Interface, error)// 所有已经注册的云商var ( // 互斥锁，用于保证providers变量的线程安全 providersMutex sync.Mutex // 所有已注册的云商 providers = make(map[string]Factory) // 已经被废弃的内部支持的云商 deprecatedCloudProviders = []struct &#123; name string external bool detail string &#125;&#123; &#123;&quot;azure&quot;, false, &quot;The Azure provider is deprecated and will be removed in a future release. Please use https://github.com/kubernetes-sigs/cloud-provider-azure&quot;&#125;, &#123;&quot;gce&quot;, false, &quot;The GCE provider is deprecated and will be removed in a future release. Please use https://github.com/kubernetes/cloud-provider-gcp&quot;&#125;, &#123;&quot;vsphere&quot;, false, &quot;The vSphere provider is deprecated and will be removed in a future release. Please use https://github.com/kubernetes/cloud-provider-vsphere&quot;&#125;, &#125;)// 通过名称注册一个cloudprovider.Factory，该函数工厂在kubeserver启动时将被调用func RegisterCloudProvider(name string, cloud Factory) &#123; providersMutex.Lock() defer providersMutex.Unlock() if _, found := providers[name]; found &#123; klog.Fatalf(&quot;Cloud provider %q was registered twice&quot;, name) &#125; klog.V(1).Infof(&quot;Registered cloud provider %q&quot;, name) providers[name] = cloud&#125;// 是否是已经注册的云商func IsCloudProvider(name string) bool &#123; providersMutex.Lock() defer providersMutex.Unlock() _, found := providers[name] return found&#125;// 调用函数工厂，创建一个cloudprovider.Interface云商实例func GetCloudProvider(name string, config io.Reader) (Interface, error) &#123; providersMutex.Lock() defer providersMutex.Unlock() f, found := providers[name] if !found &#123; return nil, nil &#125; return f(config)&#125;// 创建指定云提供商的实例func InitCloudProvider(name string, configFilePath string) (Interface, error) &#123; var cloud Interface var err error if name == &quot;&quot; &#123; return nil, nil &#125; if IsExternal(name) &#123; klog.Info(&quot;External cloud provider specified&quot;) return nil, nil &#125; if configFilePath != &quot;&quot; &#123; var config *os.File config, err = os.Open(configFilePath) if err != nil &#123; klog.Fatalf(&quot;Couldn&#x27;t open cloud provider configuration %s: %#v&quot;, configFilePath, err) &#125; // 新建一个云商的实例，name是云商的名字，config是云商的配置文件，类型是io.Reader defer config.Close() cloud, err = GetCloudProvider(name, config) &#125; else &#123; // Pass explicit nil so plugins can actually check for nil. See // &quot;Why is my nil error value not equal to nil?&quot; in golang.org/doc/faq. // nil != nil的原因是，interface包含两个属性，T表示类型，V表示值。当T赋予具体的类型，V为nil时，即 interface &#123;T: int, V: nil&#125; // 这个interface将不等于nil，因为T值已被设置。也就是只有interface &#123;T: nil, V: nil&#125;时，这个interface才是 nil == nil // 这里要讲的是，如果这里显式传nil，当后续处理判断config == nil时不会出现不符合预期的情况，否则将会对后续的插件造成困扰 cloud, err = GetCloudProvider(name, nil) &#125; if err != nil &#123; return nil, fmt.Errorf(&quot;could not init cloud provider %q: %v&quot;, name, err) &#125; if cloud == nil &#123; return nil, fmt.Errorf(&quot;unknown cloud provider %q&quot;, name) &#125; return cloud, nil&#125;最后来看cloud.Interface这个接口// Interface is an abstract, pluggable interface for cloud providers.// 云商的抽象、可插拔接口// Interface is an abstract, pluggable interface for cloud providers.// 云商的抽象、可插拔接口type Interface interface &#123; // Initialize provides the cloud with a kubernetes client builder and may spawn goroutines // to perform housekeeping or run custom controllers specific to the cloud provider. // Any tasks started here should be cleaned up when the stop channel closes. // Initialize 为云提供了 kubernetes 客户端构建器，并可能生成 goroutine // 执行内务管理或运行特定于云提供商的自定义控制器。 // 当停止通道关闭时，应清除此处启动的任何任务。 Initialize(clientBuilder ControllerClientBuilder, stop &lt;-chan struct&#123;&#125;) // LoadBalancer returns a balancer interface. Also returns true if the interface is supported, false otherwise. //LoadBalancer 返回一个负载均衡接口。如果支持该接口，也返回 true，否则返回 false。 LoadBalancer() (LoadBalancer, bool) // Instances returns an instances interface. Also returns true if the interface is supported, false otherwise. // Instances 返回一个实例接口。如果支持该接口，也返回 true，否则返回 false。 Instances() (Instances, bool) // InstancesV2 is an implementation for instances and should only be implemented by external cloud providers. // Implementing InstancesV2 is behaviorally identical to Instances but is optimized to significantly reduce // API calls to the cloud provider when registering and syncing nodes. Implementation of this interface will // disable calls to the Zones interface. Also returns true if the interface is supported, false otherwise. // InstancesV2 是实例的实现，只能由外部云提供商实现。 // 实现 InstancesV2 在行为上与 Instances 相同，但经过优化，可以显著减少在注册和同步节点时对云提供商的 API 调用。 // 实现此接口将禁用对 Zones 接口的调用。如果支持该接口，也返回 true，否则返回 false。 InstancesV2() (InstancesV2, bool) // Zones returns a zones interface. Also returns true if the interface is supported, false otherwise. // DEPRECATED: Zones is deprecated in favor of retrieving zone/region information from InstancesV2. // This interface will not be called if InstancesV2 is enabled. // Zones 返回一个区域接口。如果支持该接口，也返回 true，否则返回 false。 // 已弃用：Zones 已弃用，以便从 InstancesV2 检索区域信息。 // 如果启用了 InstancesV2，则不会调用此接口。 Zones() (Zones, bool) // Clusters returns a clusters interface. Also returns true if the interface is supported, false otherwise. // Clusters 返回一个集群接口。如果支持该接口，也返回 true，否则返回 false。 Clusters() (Clusters, bool) // Routes returns a routes interface along with whether the interface is supported. // Routes 返回一个路由接口以及该接口是否受支持。 Routes() (Routes, bool) // ProviderName returns the cloud provider ID. // ProviderName 返回云提供商 ID。 ProviderName() string // HasClusterID returns true if a ClusterID is required and set // HasClusterID 如果需要并设置了 ClusterID，则返回 true HasClusterID() bool&#125;看Instance// Instances is an abstract, pluggable interface for sets of instances.type Instances interface &#123; // NodeAddresses returns the addresses of the specified instance. NodeAddresses(ctx context.Context, name types.NodeName) ([]v1.NodeAddress, error) // NodeAddressesByProviderID returns the addresses of the specified instance. // The instance is specified using the providerID of the node. The // ProviderID is a unique identifier of the node. This will not be called // from the node whose nodeaddresses are being queried. i.e. local metadata // services cannot be used in this method to obtain nodeaddresses NodeAddressesByProviderID(ctx context.Context, providerID string) ([]v1.NodeAddress, error) // InstanceID returns the cloud provider ID of the node with the specified NodeName. // Note that if the instance does not exist, we must return (&quot;&quot;, cloudprovider.InstanceNotFound) // cloudprovider.InstanceNotFound should NOT be returned for instances that exist but are stopped/sleeping InstanceID(ctx context.Context, nodeName types.NodeName) (string, error) // InstanceType returns the type of the specified instance. InstanceType(ctx context.Context, name types.NodeName) (string, error) // InstanceTypeByProviderID returns the type of the specified instance. InstanceTypeByProviderID(ctx context.Context, providerID string) (string, error) // AddSSHKeyToAllInstances adds an SSH public key as a legal identity for all instances // expected format for the key is standard ssh-keygen format: &lt;protocol&gt; &lt;blob&gt; AddSSHKeyToAllInstances(ctx context.Context, user string, keyData []byte) error // CurrentNodeName returns the name of the node we are currently running on // On most clouds (e.g. GCE) this is the hostname, so we provide the hostname CurrentNodeName(ctx context.Context, hostname string) (types.NodeName, error) // InstanceExistsByProviderID returns true if the instance for the given provider exists. // If false is returned with no error, the instance will be immediately deleted by the cloud controller manager. // This method should still return true for instances that exist but are stopped/sleeping. InstanceExistsByProviderID(ctx context.Context, providerID string) (bool, error) // InstanceShutdownByProviderID returns true if the instance is shutdown in cloudprovider InstanceShutdownByProviderID(ctx context.Context, providerID string) (bool, error)&#125;// Instances is an abstract, pluggable interface for sets of instances.// 实例集的抽象、可插入接口。type Instances interface &#123; // NodeAddresses returns the addresses of the specified instance. // NodeAddresses返回指定实例的地址。 NodeAddresses(ctx context.Context, name types.NodeName) ([]v1.NodeAddress, error) // NodeAddressesByProviderID returns the addresses of the specified instance. // The instance is specified using the providerID of the node. The // ProviderID is a unique identifier of the node. This will not be called // from the node whose nodeaddresses are being queried. i.e. local metadata // services cannot be used in this method to obtain nodeaddresses // NodeAddressesByProviderID 返回指定实例的地址。 // 使用节点的 providerID 指定实例。ProviderID 是节点的唯一标识符。这不会从正在查询其节点地址的节点调用。 // 即不能在此方法中使用本地元数据服务来获取节点地址 NodeAddressesByProviderID(ctx context.Context, providerID string) ([]v1.NodeAddress, error) // InstanceID returns the cloud provider ID of the node with the specified NodeName. // Note that if the instance does not exist, we must return (&quot;&quot;, cloudprovider.InstanceNotFound) // cloudprovider.InstanceNotFound should NOT be returned for instances that exist but are stopped/sleeping // InstanceID 返回具有指定 NodeName 的节点的云提供商 ID。 // 请注意，如果实例不存在，我们必须返回（“”，cloudprovider.InstanceNotFound） // cloudprovider.InstanceNotFound 不应返回存在但已停止/休眠的实例 InstanceID(ctx context.Context, nodeName types.NodeName) (string, error) // InstanceType returns the type of the specified instance. // InstanceType 返回指定实例的类型。 InstanceType(ctx context.Context, name types.NodeName) (string, error) // InstanceTypeByProviderID returns the type of the specified instance. // InstanceTypeByProviderID返回指定实例的类型 InstanceTypeByProviderID(ctx context.Context, providerID string) (string, error) // AddSSHKeyToAllInstances adds an SSH public key as a legal identity for all instances // expected format for the key is standard ssh-keygen format: &lt;protocol&gt; &lt;blob&gt; // AddSSHKeyToAllInstances 将 SSH 公钥作为所有实例的合法标识添加 AddSSHKeyToAllInstances(ctx context.Context, user string, keyData []byte) error // CurrentNodeName returns the name of the node we are currently running on // On most clouds (e.g. GCE) this is the hostname, so we provide the hostname // CurrentNodeName返回我们当前正在运行的节点的名称 // 在大多数云（例如 GCE）上，这是主机名，因此我们提供主机名 CurrentNodeName(ctx context.Context, hostname string) (types.NodeName, error) // InstanceExistsByProviderID returns true if the instance for the given provider exists. // If false is returned with no error, the instance will be immediately deleted by the cloud controller manager. // This method should still return true for instances that exist but are stopped/sleeping. // InstanceExistsByProviderID 如果给定提供程序的实例存在，则返回 true。 InstanceExistsByProviderID(ctx context.Context, providerID string) (bool, error) // InstanceShutdownByProviderID returns true if the instance is shutdown in cloudprovider // InstanceShutdownByProviderID 如果云提供商中的实例已关闭，则返回 true InstanceShutdownByProviderID(ctx context.Context, providerID string) (bool, error)&#125; UnsecuredDependencies(kubeletServer, utilfeature.DefaultFeatureGate)123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254// kubeletDeps的构造kubeletDeps, err := UnsecuredDependencies(kubeletServer, utilfeature.DefaultFeatureGate)这里接受两个参数kubeletServer，是由命令行配置kubeletFlags和kubeConfig为属性的结构体，utilfeature.DefaultFeatureGate是默认的功能门，可用于开关kubelet的某些特性。kubeletServer := &amp;options.KubeletServer&#123; KubeletFlags: *kubeletFlags, KubeletConfiguration: *kubeletConfig,&#125;// UnsecuredDependencies 用于返回Dependencies适合运行的依赖项，或者当设置不可用时返回错误。它不用于启动后台进程和不做身份校验。func UnsecuredDependencies(s *options.KubeletServer, featureGate featuregate.FeatureGate) (*kubelet.Dependencies, error) &#123; // 初始化TLS配置项，如果没有配置 // InitializeTLS 检查已配置的 TLSCertFile 和 TLSPrivateKeyFile：如果未指定，则会生成新的自签名证书和密钥文件。返回已配置的 server.TLSOptions 对象。 tlsOptions, err := InitializeTLS(&amp;s.KubeletFlags, &amp;s.KubeletConfiguration) // s.ExperimentalMounterPath安装程序二进制文件的路径，未填则使用默认路径 mounter := mount.New(s.ExperimentalMounterPath) // New 返回一个子路径.当前系统的接口. subpather := subpath.New(mounter) //NewHostUtil 在不支持的平台返回一个实现 HostUtils 接口的结构体 hu := hostutil.NewHostUtil() // New 返回一个新的接口，它将 os/exec 运行命令 var pluginRunner = exec.New() // ProbeVolumePlugins 将所有卷插件收集到一个易于使用的列表中 plugins, err := ProbeVolumePlugins(featureGate) if err != nil &#123; return nil, err &#125; // NewNoopTracerProvider 返回 TracerProvider 的实现 // 不执行任何操作。从返回的结果创建的 Tracer 和 Spans // TracerProvider 也不执行任何操作。 tp := oteltrace.NewNoopTracerProvider() // features.KubeletTracing在 kubelet 中添加对分布式跟踪的支持 if utilfeature.DefaultFeatureGate.Enabled(features.KubeletTracing) &#123; tp, err = newTracerProvider(s) if err != nil &#123; return nil, err &#125; &#125; return &amp;kubelet.Dependencies&#123; Auth: nil, // default does not enforce auth[nz] CAdvisorInterface: nil, // cadvisor.New launches background processes (bg http.ListenAndServe, and some bg cleaners), not set here Cloud: nil, // cloud provider might start background processes ContainerManager: nil, KubeClient: nil, HeartbeatClient: nil, EventClient: nil, TracerProvider: tp, HostUtil: hu, Mounter: mounter, Subpather: subpather, OOMAdjuster: oom.NewOOMAdjuster(), OSInterface: kubecontainer.RealOS&#123;&#125;, VolumePlugins: plugins, DynamicPluginProber: GetDynamicPluginProber(s.VolumePluginDir, pluginRunner), TLSOptions: tlsOptions&#125;, nil&#125;func InitializeTLS(kf *options.KubeletFlags, kc *kubeletconfiginternal.KubeletConfiguration) (*server.TLSOptions, error) &#123; // 1. kc.ServerTLSBootstrap：ServerTLSBootstrap是Kubeconfiguration中的一个字段，用来指示启用服务器证书引导 // 系统不再使用自签名的服务证书， kubelet 会调用certificates.k8s.io API 来请求证书，需要有一个批复人来批准证书签名请求（CSR） // 设置此字段时，RotateKubeletServerCertificate特性必须被启用。 // // 2. kc.TLSCertFile &amp;&amp; kc.TLSPrivateKeyFile：TLSCertFile是包含 HTTPS 所需要的 x509 证书的文件 （如果有 CA 证书，会串接到服务器证书之后）。X509证书也就是HTTPS证书，比如说SSL/TLS证书。 // 如果tlsCertFile 和tlsPrivateKeyFile都没有设置，则系统会为节点的公开地址生成自签名的证书和私钥，并将其保存到 kubelet --cert-dir参数所指定的目录下。 if !kc.ServerTLSBootstrap &amp;&amp; kc.TLSCertFile == &quot;&quot; &amp;&amp; kc.TLSPrivateKeyFile == &quot;&quot; &#123; // --cert-dir即为CertDirectory，命令行指定的证书目录 // 这里拿到两个证书的路径 kc.TLSCertFile = path.Join(kf.CertDirectory, &quot;kubelet.crt&quot;) kc.TLSPrivateKeyFile = path.Join(kf.CertDirectory, &quot;kubelet.key&quot;) // CanReadCertAndKey用来判断证书文件是否存在并且可以读取 canReadCertAndKey, err := certutil.CanReadCertAndKey(kc.TLSCertFile, kc.TLSPrivateKeyFile) if err != nil &#123; return nil, err &#125; // 如果证书文件不存在或者不可以读取，则生成自签名证书并写入路径 if !canReadCertAndKey &#123; // nodeutil.GetHostname命令行设置的主机名，如果不存在，则使用os.Hostname获取node的hostname，最终转换为小写 hostName, err := nodeutil.GetHostname(kf.HostnameOverride) if err != nil &#123; return nil, err &#125; // 证书不存在，生成自签名证书 cert, key, err := certutil.GenerateSelfSignedCertKey(hostName, nil, nil) if err != nil &#123; return nil, fmt.Errorf(&quot;unable to generate self signed cert: %w&quot;, err) &#125; // 将cert和key写到kc.TLSCertFile和kc.TLSPrivateKeyFile路径 ... &#125; &#125; // kc.TLSCipherSuites是一个字符串数组，用来指定TLS连接时所使用的加密套件，见https://golang.org/pkg/crypto/tls/#pkg-constants // 这个方法会将TLSCipherSuites字符串数组转换为加密套件数组 tlsCipherSuites, err := cliflag.TLSCipherSuites(kc.TLSCipherSuites) if err != nil &#123; return nil, err &#125; // 如果加密套件数组不为空，判断使用的加密套件是否有安全问题，如果有，打印出来 if len(tlsCipherSuites) &gt; 0 &#123; // 返回tls/ssl实现的加密套件中具有安全问题的那部分 insecureCiphers := cliflag.InsecureTLSCiphers() for i := 0; i &lt; len(tlsCipherSuites); i++ &#123; for cipherName, cipherID := range insecureCiphers &#123; if tlsCipherSuites[i] == cipherID &#123; klog.InfoS(&quot;Use of insecure cipher detected.&quot;, &quot;cipher&quot;, cipherName) &#125; &#125; &#125; &#125; // TLSMinVersion 是支持的最低 TLS 版本 minTLSVersion, err := cliflag.TLSVersion(kc.TLSMinVersion) if err != nil &#123; return nil, err &#125; // 打印警告“不可配置最低的TLS版本为TLS1.3” if minTLSVersion == tls.VersionTLS13 &#123; if len(tlsCipherSuites) != 0 &#123; klog.InfoS(&quot;Warning: TLS 1.3 cipher suites are not configurable, ignoring --tls-cipher-suites&quot;) &#125; &#125; // 创建一个server.TLSOptions对象，包含了TLS配置信息 tlsOptions := &amp;server.TLSOptions&#123; Config: &amp;tls.Config&#123; MinVersion: minTLSVersion, CipherSuites: tlsCipherSuites, &#125;, CertFile: kc.TLSCertFile, KeyFile: kc.TLSPrivateKeyFile, &#125; // kc.Authentication.X509指定如何对 Kubelet 服务器的请求进行身份验证 // 三个属性： // X509：x509 包含与 x509 客户端证书身份验证相关的设置。 // clientCAFile 是 PEM 编码证书捆绑包的路径。 // 如果设置了该选项，则任何提交由该证书中的一个机构签署的客户证书的请求， // 都会使用与客户证书中的 CommonName 对应的用户名和与组织对应的组进行身份验证。 // webhook：包含与 webhook 承载令牌身份验证相关的设置。 // enabled：启用允许由 tokenreviews.authentication.k8s.io API 支持的不记名令牌身份验证 // cacheTTL：启用身份验证结果的缓存 // anonymous：包含与匿名身份验证相关的设置 // enabled：允许向 kubelet 服务器发起匿名请求。未被其他身份验证方法拒绝的请求将被视为匿名请求。 // 匿名请求的用户名是 system:anonymous，组名是 system:unauthenticated if len(kc.Authentication.X509.ClientCAFile) &gt; 0 &#123; // NewPoolFromBytes 返回一个 x509.CertPool，其中包含给定 PEM 编码字节中的证书。 // 如果无法读取文件、无法解析证书或文件不包含任何证书，则返回错误 clientCAs, err := certutil.NewPool(kc.Authentication.X509.ClientCAFile) if err != nil &#123; return nil, fmt.Errorf(&quot;unable to load client CA file %s: %w&quot;, kc.Authentication.X509.ClientCAFile, err) &#125; // Specify allowed CAs for client certificates tlsOptions.Config.ClientCAs = clientCAs // Populate PeerCertificates in requests, but don&#x27;t reject connections without verified certificates tlsOptions.Config.ClientAuth = tls.RequestClientCert &#125; return tlsOptions, nil&#125;func GenerateSelfSignedCertKeyWithFixtures(host string, alternateIPs []net.IP, alternateDNS []string, fixtureDirectory string) ([]byte, []byte, error) &#123; // 生效时间：当前时间的前一个小时，避免时钟偏差的影响 // 一年的有效期 validFrom := time.Now().Add(-time.Hour) // valid an hour earlier to avoid flakes due to clock skew maxAge := time.Hour * 24 * 365 // one year self-signed certs // hostname_ip_dns baseName := fmt.Sprintf(&quot;%s_%s_%s&quot;, host, strings.Join(ipsToStrings(alternateIPs), &quot;-&quot;), strings.Join(alternateDNS, &quot;-&quot;)) certFixturePath := filepath.Join(fixtureDirectory, baseName+&quot;.crt&quot;) keyFixturePath := filepath.Join(fixtureDirectory, baseName+&quot;.key&quot;) ... // 生成密钥对，实际上下面要生成中间证书 caKey, err := rsa.GenerateKey(cryptorand.Reader, 2048) caTemplate := x509.Certificate&#123; SerialNumber: big.NewInt(1), Subject: pkix.Name&#123; CommonName: fmt.Sprintf(&quot;%s-ca@%d&quot;, host, time.Now().Unix()), &#125;, NotBefore: validFrom, NotAfter: validFrom.Add(maxAge), KeyUsage: x509.KeyUsageKeyEncipherment | x509.KeyUsageDigitalSignature | x509.KeyUsageCertSign, BasicConstraintsValid: true, IsCA: true, &#125; // 构建一个Certificate结构体，代表Certificate证书，即创建证书模版 caTemplate := x509.Certificate&#123; SerialNumber: big.NewInt(1), //该号码表示CA颁发的唯一序列号，在此使用一个数来代表 Subject: pkix.Name&#123; CommonName: fmt.Sprintf(&quot;%s-ca@%d&quot;, host, time.Now().Unix()), &#125;, NotBefore: validFrom, NotAfter: validFrom.Add(maxAge), // Key Encipherment (x509.KeyUsageKeyEncipherment)：表示证书的密钥可用于加密数据 // Digital Signature (x509.KeyUsageDigitalSignature)：表示证书的密钥可用于数字签名 // Certificate Signing (x509.KeyUsageCertSign)：表示证书的密钥可用于签发其他证书，通常用于证书链中的中间或根证书 KeyUsage: x509.KeyUsageKeyEncipherment | x509.KeyUsageDigitalSignature | x509.KeyUsageCertSign, //表示该证书是用来做服务端认证的 BasicConstraintsValid: true, IsCA: true, &#125; // CreateCertificate 根据模板创建新的 X.509 v3 证书，返回DER类型的编码 // 查阅文档 https://pkg.go.dev/crypto/x509@go1.21.0#CreateCertificate caDERBytes, err := x509.CreateCertificate(cryptorand.Reader, &amp;caTemplate, &amp;caTemplate, &amp;caKey.PublicKey, caKey) if err != nil &#123; return nil, nil, err &#125; // 从DER类型的数据中解析Certificate证书 中间证书 caCertificate, err := x509.ParseCertificate(caDERBytes) if err != nil &#123; return nil, nil, err &#125; // 生成密钥对 priv, err := rsa.GenerateKey(cryptorand.Reader, 2048) if err != nil &#123; return nil, nil, err &#125; template := x509.Certificate&#123; SerialNumber: big.NewInt(2), //该号码表示CA颁发的唯一序列号，在此使用一个数来代表 Subject: pkix.Name&#123; CommonName: fmt.Sprintf(&quot;%s@%d&quot;, host, time.Now().Unix()), &#125;, NotBefore: validFrom, NotAfter: validFrom.Add(maxAge), KeyUsage: x509.KeyUsageKeyEncipherment | x509.KeyUsageDigitalSignature, ExtKeyUsage: []x509.ExtKeyUsage&#123;x509.ExtKeyUsageServerAuth&#125;, BasicConstraintsValid: true, &#125; // 获取IP地址 if ip := netutils.ParseIPSloppy(host); ip != nil &#123; template.IPAddresses = append(template.IPAddresses, ip) &#125; else &#123; template.DNSNames = append(template.DNSNames, host) &#125; template.IPAddresses = append(template.IPAddresses, alternateIPs...) template.DNSNames = append(template.DNSNames, alternateDNS...) // CreateCertificate 根据模板创建新的 X.509 v3 证书，返回DER类型的编码 derBytes, err := x509.CreateCertificate(cryptorand.Reader, &amp;template, caCertificate, &amp;priv.PublicKey, caKey) if err != nil &#123; return nil, nil, err &#125; // 生成中间证书+CA证书 certBuffer := bytes.Buffer&#123;&#125; if err := pem.Encode(&amp;certBuffer, &amp;pem.Block&#123;Type: CertificateBlockType, Bytes: derBytes&#125;); err != nil &#123; return nil, nil, err &#125; if err := pem.Encode(&amp;certBuffer, &amp;pem.Block&#123;Type: CertificateBlockType, Bytes: caDERBytes&#125;); err != nil &#123; return nil, nil, err &#125; // 将私钥中的密钥对放入pem.Block结构体中 keyBuffer := bytes.Buffer&#123;&#125; if err := pem.Encode(&amp;keyBuffer, &amp;pem.Block&#123;Type: keyutil.RSAPrivateKeyBlockType, Bytes: x509.MarshalPKCS1PrivateKey(priv)&#125;); err != nil &#123; return nil, nil, err &#125; ... return certBuffer.Bytes(), keyBuffer.Bytes(), nil&#125; initConfigz(&amp;s.KubeletConfiguration)123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361func initConfigz(kc *kubeletconfiginternal.KubeletConfiguration) error &#123; cz, err := configz.New(&quot;kubeletconfig&quot;) if err != nil &#123; klog.ErrorS(err, &quot;Failed to register configz&quot;) return err &#125; if err := setConfigz(cz, kc); err != nil &#123; klog.ErrorS(err, &quot;Failed to register config&quot;) return err &#125; return nil&#125;// configz.New的实现// 1. 先加锁，configsGuard.Lock()使用了sync.RWMutex，sync.RWMutex允许多个读锁或一个写锁存在，适用于读频繁但是写不频繁的场景。// golang的map不是线程安全的，在查找、赋值、遍历、删除都会检查写标志，当写标志存在时，会直接panic// 2. 保证配置只被写入一次，先赋值为将一个空的Config结构体：// type Config struct &#123;// val interface&#123;&#125;// &#125;// configs是一个全局变量，用于存储name和Config句柄的映射func New(name string) (*Config, error) &#123; configsGuard.Lock() // configsGuard sync.RWMutex defer configsGuard.Unlock() if _, found := configs[name]; found &#123; return nil, fmt.Errorf(&quot;register config %q twice&quot;, name) &#125; newConfig := Config&#123;&#125; configs[name] = &amp;newConfig return &amp;newConfig, nil&#125;// 一些前置的结构体说明// 用来标识一种资源类型，它表示类型，版本和组type GroupVersionKind struct &#123; Group string Version string Kind string&#125;// 用来将字段选择器转换成内部表示type FieldLabelConversionFunc func(label, value string) (internalLabel, internalValue string, err error)// func setConfigz(cz *configz.Config, kc *kubeletconfiginternal.KubeletConfiguration) error &#123; scheme, _, err := kubeletscheme.NewSchemeAndCodecs() if err != nil &#123; return err &#125; versioned := kubeletconfigv1beta1.KubeletConfiguration&#123;&#125; if err := scheme.Convert(kc, &amp;versioned, nil); err != nil &#123; return err &#125; cz.Set(versioned) return nil&#125;// 用来标志一种API的版本和类型type GroupVersionKind struct &#123; Group string Version string Kind string&#125;// schema注册了api对象的序列化和反序列化对象。它将组，版本和kind信息转换成Go Schema和不同版本之间的映射。它是版本化API和版本化配置的基础。// 在schema里面，Type是一个特定的结构，Version是该类型特定的时间点标识符，通常向后兼容。对于特定版本的Type，Kind具有唯一的名称。// Group用来标识一系列随着时间变化的Version，Kind和Type。（实际上是类型的“v1”，不期望将来被打破）// 以上实际上是Kubernetes用来管理不同的API版本的方式，它存储了API的元信息。例如：// apiVersion: v1// kind: Pod// Kind为Pod，v1是Version，Group是core// 这个对象不应该被修改，在注册后它是线程安全的type Scheme struct &#123; // 通过API版本信息和名称找出Type，GroupVersionKind结构体已经说明 gvkToType map[schema.GroupVersionKind]reflect.Type // 通过Type找出版本 typeToGVK map[reflect.Type][]schema.GroupVersionKind // unversionedTypes无需转换 unversionedTypes map[reflect.Type]schema.GroupVersionKind // unversionedKinds是Kind的名称，可以在任何组/版本的上下文被创建 unversionedKinds map[string]reflect.Type // 从版本和资源映射到相应的方法，该方法将资源的label字段映射成内部版本 fieldLabelConversionFuncs map[schema.GroupVersionKind]FieldLabelConversionFunc // defaulterFuncs 使用对象调用，以实现默认功能的映射 defaulterFuncs map[reflect.Type]func(interface&#123;&#125;) // 存储已转换的函数 converter *conversion.Converter // versionPriority 组到版本的有序列表，指示默认列表中的优先级 versionPriority map[string][]string // observedVersions 跟踪在类型注册期间我们看到的版本的顺序 observedVersions []schema.GroupVersion // schemeName 是schema的名称。不指定名称则使用NewSchema的调用堆栈 schemeName string&#125;func NewSchemeAndCodecs(mutators ...serializer.CodecFactoryOptionsMutator) (*runtime.Scheme, *serializer.CodecFactory, error) &#123; scheme := runtime.NewScheme() // 新建一个Schema的默认对象 // AddToScheme主要调用kubeletconfig register.go的Register函数列表，注册和转换为Schema元信息，见下文register.go和scheme_builder.go if err := kubeletconfig.AddToScheme(scheme); err != nil &#123; return nil, nil, err &#125; if err := kubeletconfigv1beta1.AddToScheme(scheme); err != nil &#123; return nil, nil, err &#125; if err := kubeletconfigv1.AddToScheme(scheme); err != nil &#123; return nil, nil, err &#125; codecs := serializer.NewCodecFactory(scheme, mutators...) return scheme, &amp;codecs, nil&#125;下面我们先看kubeletconfig.AddToScheme(scheme)，看看是怎么将kubeletconfig注册到Schema的。/** kubelet的注册文件register.go*/// kubelet Group名称const GroupName = &quot;kubelet.config.k8s.io&quot;// APIVersionInternal为__internal，表示一个不稳定的内部对象var SchemeGroupVersion = schema.GroupVersion&#123;Group: GroupName, Version: runtime.APIVersionInternal&#125;var ( // SchemeBuilder is the scheme builder with scheme init functions to run for this API package SchemeBuilder = runtime.NewSchemeBuilder(addKnownTypes) // AddToScheme is a global function that registers this API group &amp; version to a scheme AddToScheme = SchemeBuilder.AddToScheme)// addKnownTypes registers known types to the given schemefunc addKnownTypes(scheme *runtime.Scheme) error &#123; // 见下文vendor/k8s.io/apimachinery/pkg/runtime/scheme.go scheme.AddKnownTypes(SchemeGroupVersion, &amp;KubeletConfiguration&#123;&#125;, &amp;SerializedNodeConfigSource&#123;&#125;, &amp;CredentialProviderConfig&#123;&#125;, ) return nil&#125;/* scheme_builder.go*/// AddToScheme applies all the stored functions to the scheme. A non-nil error// indicates that one function failed and the attempt was abandoned.func (sb *SchemeBuilder) AddToScheme(s *Scheme) error &#123; for _, f := range *sb &#123; if err := f(s); err != nil &#123; return err &#125; &#125; return nil&#125;// Register adds a scheme setup function to the list.func (sb *SchemeBuilder) Register(funcs ...func(*Scheme) error) &#123; for _, f := range funcs &#123; *sb = append(*sb, f) &#125;&#125;// NewSchemeBuilder calls Register for you.func NewSchemeBuilder(funcs ...func(*Scheme) error) SchemeBuilder &#123; var sb SchemeBuilder sb.Register(funcs...) return sb&#125;/* vendor/k8s.io/apimachinery/pkg/runtime/scheme.go*/// 判断如果为非内部类型并且不在observedVersions（记录注册的版本顺序）中，则加进去func (s *Scheme) addObservedVersion(version schema.GroupVersion) &#123; if len(version.Version) == 0 || version.Version == APIVersionInternal &#123; return &#125; for _, observedVersion := range s.observedVersions &#123; if observedVersion == version &#123; return &#125; &#125; s.observedVersions = append(s.observedVersions, version)&#125;// 构建GroupVersionKind结构体func (gv GroupVersion) WithKind(kind string) GroupVersionKind &#123; return GroupVersionKind&#123;Group: gv.Group, Version: gv.Version, Kind: kind&#125;&#125;// GroupVersionKind构建GroupVersion结构体func (gvk GroupVersionKind) GroupVersion() GroupVersion &#123; return GroupVersion&#123;Group: gvk.Group, Version: gvk.Version&#125;&#125;func (s *Scheme) AddKnownTypes(gv schema.GroupVersion, types ...Object) &#123; s.addObservedVersion(gv) // 注册到schema.observedVersions for _, obj := range types &#123; // 检查，types必须为结构体指针列表 t := reflect.TypeOf(obj) if t.Kind() != reflect.Pointer &#123; panic(&quot;All types must be pointers to structs.&quot;) &#125; t = t.Elem() // 获取结构体 s.AddKnownTypeWithName(gv.WithKind(t.Name()), obj) &#125;&#125;func (s *Scheme) AddKnownTypeWithName(gvk schema.GroupVersionKind, obj Object) &#123; s.addObservedVersion(gvk.GroupVersion()) // 将上面的types ...Object添加到Schema t := reflect.TypeOf(obj) // 一些检查，跟上面的检查项一样，主要检查版本是否为空，obj是否是结构体指针 ... t = t.Elem() // 唯一性检查，不能注册obj两次，t在这里是GroupVersionKind结构体指针 if oldT, found := s.gvkToType[gvk]; found &amp;&amp; oldT != t &#123; panic(fmt.Sprintf(&quot;Double registration of different types for %v: old=%v.%v, new=%v.%v in scheme %q&quot;, gvk, oldT.PkgPath(), oldT.Name(), t.PkgPath(), t.Name(), s.schemeName)) &#125; // 添加到Schema的gvkToType映射当中 s.gvkToType[gvk] = t for _, existingGvk := range s.typeToGVK[t] &#123; if existingGvk == gvk &#123; return &#125; &#125; // 添加obj对应的GroupVersionKind添加到typeToGVK中，t在这里是Object结构体即上文的KubeletConfiguration&#123;&#125;等，gvk是GroupVersionKind结构体 s.typeToGVK[t] = append(s.typeToGVK[t], gvk)&#125;// 好了，回过头我们再看func addKnownTypes(scheme *runtime.Scheme) error &#123; scheme.AddKnownTypes(SchemeGroupVersion, &amp;KubeletConfiguration&#123;&#125;, &amp;SerializedNodeConfigSource&#123;&#125;, &amp;CredentialProviderConfig&#123;&#125;, ) return nil&#125;// scheme.AddKnownTypes将SchemeGroupVersion，KubeletConfiguration，SerializedNodeConfigSource和CredentialProviderConfig注册到Schema元信息当中。并且，在Schema.observedVersions当中它是按顺序的。// 其中 // KubeletConfiguration包含了Kubelet的配置// SerializedNodeConfigSource为Kubelet 在内部使用此类型来跟踪检查点动态配置，从1.22起已经废弃// CredentialProviderConfig凭证提供者的信息。从磁盘读取并且此配置并启用。回头来看kubeletconfigv1beta1.AddToScheme(scheme)，依然是注册了kubeletconfigv1beta1版本的配置信息不过，下面不一样的是，它注册了一个addDefaultingFuncs函数，在kubeletconfigv1beta1.AddToScheme(scheme)被调用// GroupName is the group name used in this packageconst GroupName = &quot;kubelet.config.k8s.io&quot;// SchemeGroupVersion is group version used to register these objectsvar SchemeGroupVersion = schema.GroupVersion&#123;Group: GroupName, Version: &quot;v1beta1&quot;&#125;var ( // localSchemeBuilder extends the SchemeBuilder instance with the external types. In this package, // defaulting and conversion init funcs are registered as well. localSchemeBuilder = &amp;kubeletconfigv1beta1.SchemeBuilder // AddToScheme is a global function that registers this API group &amp; version to a scheme AddToScheme = localSchemeBuilder.AddToScheme)func init() &#123; // We only register manually written functions here. The registration of the // generated functions takes place in the generated files. The separation // makes the code compile even when the generated files are missing. localSchemeBuilder.Register(addDefaultingFuncs)&#125;---// 向Schema.defaulterFuncs添加默认函数func (s *Scheme) AddTypeDefaultingFunc(srcType Object, fn func(interface&#123;&#125;)) &#123; s.defaulterFuncs[reflect.TypeOf(srcType)] = fn&#125;func addDefaultingFuncs(scheme *kruntime.Scheme) error &#123; return RegisterDefaults(scheme)&#125;// 添加了一个默认函数，这个函数的参数为*v1beta1.KubeletConfiguration，// 实际上将*v1beta1.KubeletConfiguration作为参数调用SetObjectDefaults_KubeletConfigurationfunc RegisterDefaults(scheme *runtime.Scheme) error &#123; scheme.AddTypeDefaultingFunc(&amp;v1beta1.KubeletConfiguration&#123;&#125;, func(obj interface&#123;&#125;) &#123; SetObjectDefaults_KubeletConfiguration(obj.(*v1beta1.KubeletConfiguration)) &#125;) return nil&#125;func SetObjectDefaults_KubeletConfiguration(in *v1beta1.KubeletConfiguration) &#123; // 这个函数比较简单，就是当KubeletConfiguration配置为空时设置v1beta1.KubeletConfiguration结构体的默认参数 SetDefaults_KubeletConfiguration(in) for i := range in.ReservedMemory &#123; a := &amp;in.ReservedMemory[i] v1.SetDefaults_ResourceList(&amp;a.Limits) &#125;&#125;// v1beta1.KubeletConfigurationtype KubeletConfiguration struct &#123; ... // NUMA节点的保留内存配置 // 相关信息可以查看https://kubernetes.io/docs/tasks/administer-cluster/reserve-compute-resources/#node-allocatable ReservedMemory []MemoryReservation `json:&quot;reservedMemory,omitempty&quot;` ...&#125;同样的，kubeletconfigv1.AddToScheme(scheme)也是用来注册v1版本的kubeletconfig信息至于下面这一样，是初始化一系列的序列化器，将配置序列化成json，yaml等格式的数据codecs := serializer.NewCodecFactory(scheme, mutators...)回到setConfigz方法：func setConfigz(cz *configz.Config, kc *kubeletconfiginternal.KubeletConfiguration) error &#123; scheme, _, err := kubeletscheme.NewSchemeAndCodecs() if err != nil &#123; return err &#125; versioned := kubeletconfigv1beta1.KubeletConfiguration&#123;&#125; if err := scheme.Convert(kc, &amp;versioned, nil); err != nil &#123; return err &#125; cz.Set(versioned) return nil&#125;type Config struct &#123; val interface&#123;&#125;&#125;首先，使用kubeletscheme.NewSchemeAndCodecs()注册了kubeletconfig，kubeletconfigv1beta1，kubeletconfigv1的配置信息；然后，实例化一个kubeletconfigv1beta1.KubeletConfiguration结构体接着，使用scheme.Convert将kubeletconfiginternal.KubeletConfiguration转换为kubeletconfigv1beta1.KubeletConfiguration最后，将转换后的kubeletconfigv1beta1.KubeletConfiguration设置到Config.val再重新看看这块的代码：func initConfigz(kc *kubeletconfiginternal.KubeletConfiguration) error &#123; cz, err := configz.New(&quot;kubeletconfig&quot;) if err != nil &#123; klog.ErrorS(err, &quot;Failed to register configz&quot;) return err &#125; if err := setConfigz(cz, kc); err != nil &#123; klog.ErrorS(err, &quot;Failed to register config&quot;) return err &#125; return nil&#125; 这块的代码就是注册一些可用的API版本，然后将kc转换为kubeletconfigv1beta1.KubeletConfiguration类型，并将他以kubeletconfig为key，kubeletconfigv1beta1.KubeletConfiguration句柄为值，保存到一个名为configs的map全局变量中 参考文档 Quality-of-Service for Memory Resources KEP-2570: Support Memory QoS with cgroups v2 Cgroups v2 Kubernetes 服务质量 Qos 解析 Reserve Compute Resources for System Daemons","categories":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"http://jerryblogs.com/categories/Kubernetes/"},{"name":"源码分析","slug":"Kubernetes/源码分析","permalink":"http://jerryblogs.com/categories/Kubernetes/%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"}],"tags":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"http://jerryblogs.com/tags/Kubernetes/"}]},{"title":"cobra-一个强大的cli构建工具","slug":"cobra-一个强大的cli构建工具","date":"2023-06-29T09:24:50.000Z","updated":"2023-06-29T11:36:28.202Z","comments":true,"path":"2023/06/29/cobra-一个强大的cli构建工具/","link":"","permalink":"http://jerryblogs.com/2023/06/29/cobra-%E4%B8%80%E4%B8%AA%E5%BC%BA%E5%A4%A7%E7%9A%84cli%E6%9E%84%E5%BB%BA%E5%B7%A5%E5%85%B7/","excerpt":"","text":"概述Cobra用于构建一个强大的现代cli应用程序的工具，它被Kubernetes、github cli、Hugo等著名的项目所使用。 作为一个使用广泛的工具，Cobra支持智能建议（app srver… did you mean app server?），自动生成shell&#x2F;bash&#x2F;sh等，自动生成帮助信息…可以说是一个非常强大的工具。 该项目的项目地址如下： https://github.com/spf13/cobra 如何使用安装1go get -u github.com/spf13/cobra@latest 导入 1import &quot;github.com/spf13/cobra&quot; 使用生成模版文件先安装cobra-cli: 1go install github.com/spf13/cobra-cli@latest 在根目录下，执行 1cobra-cli init 添加标记首先将程序名称设置为artifact，然后修改命令介绍。 1234567891011rootCmd = &amp;cobra.Command&#123; Use: &quot;artifact&quot;, Short: &quot;artifact是制品包下载器工具&quot;, Long: `artifact是制品包的下载工具，可用于从文件服务器拉取制品包，然后从文件服务器下载连接器。输入制品包版本号或者时间戳，arfact将自动检测包是否存在，并按照指定的连接器列表下载连接器，最后将他们打包成最终的制品包文件。`, // Uncomment the following line if your bare application // has an action associated with it: Run: func(cmd *cobra.Command, args []string) &#123; fmt.Println() &#125;,&#125; 在init函数增加记录： 12345678var version stringfunc init() &#123; rootCmd.PersistentFlags().StringVarP(&amp;version, &quot;version&quot;, &quot;v&quot;, &quot;&quot;, &quot;版本号&quot;) // 1 rootCmd.MarkPersistentFlagRequired(&quot;version&quot;) // 2 viper.BindPFlag(&quot;version&quot;, rootCmd.PersistentFlags().Lookup(&quot;version&quot;)) // 3&#125; 定义version标记，可以使用快捷命令”-v”，或者长命令”–version”，定义默认值为空，而且帮助信息为“版本号”。 设置version为必填，这里表示必须在命令行指定，在配置文件中指定是不行的 将version与配置文件中的version关联起来，这里命令行优先级大于配置文件 在根目录下运行go run main.go -h，即可输出帮助信息。 添加子命令运行cobra-cli add list，表示列出制品包列表，这样会在cmd目录下生成一个list目录，配置方法与上文类似，这里不再多说。 具体配置方法可以参照官方说明文档。 For complete details on using the Cobra-CLI generator, please read The Cobra Generator READMEFor complete details on using the Cobra library, please read the The Cobra User Guide.","categories":[{"name":"Golang","slug":"Golang","permalink":"http://jerryblogs.com/categories/Golang/"}],"tags":[{"name":"Golang","slug":"Golang","permalink":"http://jerryblogs.com/tags/Golang/"}]},{"title":"一个简单的chatgpt-go库，支持自定义角色和保留上下文","slug":"Chatgpt-go","date":"2023-06-26T01:08:21.000Z","updated":"2023-06-26T01:17:03.896Z","comments":true,"path":"2023/06/26/Chatgpt-go/","link":"","permalink":"http://jerryblogs.com/2023/06/26/Chatgpt-go/","excerpt":"","text":"发现现在chatgpt go库比较少，几个月前自己简单手撸了一个，可以比较方便的使用和集成到您的项目当中。 项目地址 可以比较方便的集成到您的项目当中，使用方式比较简单： 12345678910111213var MongkeyKing = ChatMode&#123; ModeName: &quot;孙悟空&quot;, PreMessages: []Message&#123;&#123; // 美猴王孙悟空的角色 Role: chat.SystemRole, Content: &quot;请你扮演孙悟空的角色和我对话&quot;, &#125;&#125;,&#125;c := chat.NewChatGptProxy(MongkeyKing, secretKey, proxyUrl) // secretKey密钥, proxyUrl本地代理地址chatSay, err := c.Chat(chat.Message&#123; // chatgpt返回的文本 Role: chat.UserRole, Content: userSay,&#125;) 角色名称和prompt可以在rules.go文件自定义。","categories":[{"name":"Chatgpt","slug":"Chatgpt","permalink":"http://jerryblogs.com/categories/Chatgpt/"}],"tags":[{"name":"Chatgpt","slug":"Chatgpt","permalink":"http://jerryblogs.com/tags/Chatgpt/"}]},{"title":"Docker推拉自建仓库比较慢的问题","slug":"Docker推拉自建仓库比较慢的问题","date":"2023-05-25T03:34:46.000Z","updated":"2023-05-26T04:35:40.064Z","comments":true,"path":"2023/05/25/Docker推拉自建仓库比较慢的问题/","link":"","permalink":"http://jerryblogs.com/2023/05/25/Docker%E6%8E%A8%E6%8B%89%E8%87%AA%E5%BB%BA%E4%BB%93%E5%BA%93%E6%AF%94%E8%BE%83%E6%85%A2%E7%9A%84%E9%97%AE%E9%A2%98/","excerpt":"","text":"背景及描述内网是一个Kubernetes集群，在集群中部署一个自建的私有镜像仓库。启动一个业务Pod，拉取该私有镜像仓库的镜像（该镜像2GB左右，20层，其中有一层是1.73GB），发现该镜像拉取比较慢，需要3分钟左右。 结论自建私有镜像仓库和业务pod运行在同一个pod，理论上拉取速度会比较快，但是同一时刻存在多个镜像正在被拉取，镜像的拉取过程需要先把镜像下载下来，然后再解压缩该镜像。在这个过程中可能由于节点资源受限而造成拉取镜像慢的现象。Docker镜像的拉取是并发拉取镜像的每一层，但是对于单独的一层是单线程进行解压缩，Docker使用gzip库进行压缩和解压缩。在镜像存在一个比较大的层的情况下，解压缩的效率比较慢会导致镜像推送和拉取比较慢。 社区解决方案一篇比较早的解决方案： 使用pgzip库进行多线程压缩和解压缩，在这个Issue中提到，当使用了pgzip后，带宽利用效率提升3倍，推送时间可至少缩减至1&#x2F;3。 作者随后提交了PR，将gzip替换为pgzip，但最终这个PR被废弃，被废弃的主要原因有几个： pgzip相比gzip消耗更大，引入gzip会增加更多的依赖，给后续的维护带来更大的不确定性和风险 测试不足，缺乏更多平台和场景下的测试结果 gzip仅限一个CPU内核，pgzip会用光所有的CPU资源容易影响其他服务运行，要控制它需要做多余的工作 compress&#x2F;gzip作者表示，他将会进行压缩优化以加快拉取镜像速度。","categories":[{"name":"Docker","slug":"Docker","permalink":"http://jerryblogs.com/categories/Docker/"}],"tags":[{"name":"Docker","slug":"Docker","permalink":"http://jerryblogs.com/tags/Docker/"}]},{"title":"Kubernetes-Scheduler的插件","slug":"Kubernetes-Scheduler的插件","date":"2023-04-25T13:42:08.000Z","updated":"2023-05-10T09:55:26.391Z","comments":true,"path":"2023/04/25/Kubernetes-Scheduler的插件/","link":"","permalink":"http://jerryblogs.com/2023/04/25/Kubernetes-Scheduler%E7%9A%84%E6%8F%92%E4%BB%B6/","excerpt":"","text":"概述 插件是如何注册的？ Scheduler中有哪些调度器？ 如何写一个调度器？ 源码解析注册过程根据上一篇文章的代码： 1234567891011121314151617181920212223242526272829303132333435363738394041func (r Registry) Register(name string, factory PluginFactory) error &#123; if _, ok := r[name]; ok &#123; return fmt.Errorf(&quot;a plugin named %v already exists&quot;, name) &#125; r[name] = factory return nil&#125;func (r Registry) Merge(in Registry) error &#123; for name, factory := range in &#123; if err := r.Register(name, factory); err != nil &#123; return err &#125; &#125; return nil&#125;func New(ctx context.Context, opts *options.Options, outOfTreeRegistryOptions ...Option) (*schedulerserverconfig.CompletedConfig, *scheduler.Scheduler, error) &#123; ... // 内置插件注册表，里面提供了一系列的特性开关 registry := frameworkplugins.NewInTreeRegistry() // 合并外置插件注册表 if err := registry.Merge(options.frameworkOutOfTreeRegistry); err != nil &#123; return nil, err &#125; ... profiles, err := profile.NewMap(options.profiles, registry, recorderFactory, stopCh, frameworkruntime.WithComponentConfigVersion(options.componentConfigVersion), frameworkruntime.WithClientSet(client), frameworkruntime.WithKubeConfig(options.kubeConfig), frameworkruntime.WithInformerFactory(informerFactory), frameworkruntime.WithSnapshotSharedLister(snapshot), frameworkruntime.WithCaptureProfile(frameworkruntime.CaptureProfile(options.frameworkCapturer)), frameworkruntime.WithClusterEventMap(clusterEventMap), frameworkruntime.WithClusterEventMap(clusterEventMap), frameworkruntime.WithParallelism(int(options.parallelism)), frameworkruntime.WithExtenders(extenders), frameworkruntime.WithMetricsRecorder(metricsRecorder), ) ...&#125; 上述代码是Scheduler的实例化过程，我们可以看到它注册了内置插件，然后合并了自定义插件，然后放到profile中。 frameworkplugins.NewInTreeRegistry我们先来看看Register: 12345// Register是一个可用插件的集合，框架使用registry来启用并且初始化插件的配置。// 所有的插件在初始化框架之前必须在register里面type Registry map[string]PluginFactory// PluginFactory是一个构建插件的方法type PluginFactory = func(configuration runtime.Object, f framework.Handle) (framework.Plugin, error) 可以看到，Register是一个map，value为插件的名称(Name)，值为插件的工厂方法。 上文调用了一个frameworkplugins.NewInTreeRegistry()方法： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124// Features被不同的插件使用，携带features gate值// 这个结构允许我们打破插件对内部k8s features pkg的依赖。type Features struct &#123; // 启用动态资源分配 EnableDynamicResourceAllocation bool // 启用 ReadWriteOnce (RWO) 存储卷绑定限制 // 用于限制 Pod 只能使用支持 ReadWriteOnce 访问模式的存储卷 EnableReadWriteOncePod bool // 启用卷容量优先级 // 在调度Pod时考虑存储卷的容量情况，优先将Pod调度到卷容量足够的节点 EnableVolumeCapacityPriority bool // 启用Pod拓扑传播中的最小域限制 // 在调度过程中限制同一拓扑域中Pod的数量 // 拓扑域用来表示集群中节点之间的物理分区或逻辑分区的概念 // 物理分区比如机架，节点组，区域等；逻辑分区比如自定义标签或注解等 // 通过将Pod分布在不同的拓扑域当中，可以降低单点故障的风险，提高应用程序的高可用性 EnableMinDomainsInPodTopologySpread bool // 启用Pod拓扑传播中的节点包含策略 // 用于在调度过程中根据节点的标签信息限制Pod的调度 EnableNodeInclusionPolicyInPodTopologySpread bool // 启用Pod拓扑传播中的标签键匹配策略 // 在调度中限制Pod调度到具有特定标签的节点上面 EnableMatchLabelKeysInPodTopologySpread bool // 启用Pod调度就绪性检查 // 只将就绪的Pod调度到节点上 EnablePodSchedulingReadiness bool // 启用Pod中断检查 // 限制Pod调度到可能中断的节点上面 EnablePodDisruptionConditions bool // 启用Pod垂直扩缩容 // 调度过程中根据Pod的资源使用情况调整Pod的请求和限制，实现Pod自动扩缩容 EnableInPlacePodVerticalScaling bool&#125;// FeatureGate 指定是否启用特定的特性type FeatureGate interface &#123; // Enabled returns true if the key is enabled. Enabled(key Feature) bool // KnownFeatures returns a slice of strings describing the FeatureGate&#x27;s known features. KnownFeatures() []string // DeepCopy returns a deep copy of the FeatureGate object, such that gates can be // set on the copy without mutating the original. This is useful for validating // config against potential feature gate changes before committing those changes. DeepCopy() MutableFeatureGate&#125;// featureGate实现了FeatureGate接口（默认实现，也就是下面的DefaultFeatureGate），以及用于标志解析的 pflag.Valuetype featureGate struct &#123; featureGateName string special map[Feature]func(map[Feature]FeatureSpec, map[Feature]bool, bool) // lock guards writes to known, enabled, and reads/writes of closed lock sync.Mutex // known holds a map[Feature]FeatureSpec known *atomic.Value // enabled holds a map[Feature]bool enabled *atomic.Value // closed is set to true when AddFlag is called, and prevents subsequent calls to Add closed bool&#125;// 获取特性的配置，如果有则从enabled中获取，如果没有则从known中获取默认值func (f *featureGate) Enabled(key Feature) bool &#123; if v, ok := f.enabled.Load().(map[Feature]bool)[key]; ok &#123; return v &#125; if v, ok := f.known.Load().(map[Feature]FeatureSpec)[key]; ok &#123; return v.Default &#125; panic(fmt.Errorf(&quot;feature %q is not registered in FeatureGate %q&quot;, key, f.featureGateName))&#125;func FactoryAdapter(fts plfeature.Features, withFts PluginFactoryWithFts) PluginFactory &#123; return func(plArgs runtime.Object, fh framework.Handle) (framework.Plugin, error) &#123; return withFts(plArgs, fh, fts) &#125;&#125;// NewInTreeRegistry使用内置插件构建注册表register// 运行自定义插件的scheduler可以通过WithFrameworkOutOfTreeRegistry选项注册自定义插件func NewInTreeRegistry() runtime.Registry &#123; fts := plfeature.Features&#123; EnableDynamicResourceAllocation: feature.DefaultFeatureGate.Enabled(features.DynamicResourceAllocation), EnableReadWriteOncePod: feature.DefaultFeatureGate.Enabled(features.ReadWriteOncePod), EnableVolumeCapacityPriority: feature.DefaultFeatureGate.Enabled(features.VolumeCapacityPriority), EnableMinDomainsInPodTopologySpread: feature.DefaultFeatureGate.Enabled(features.MinDomainsInPodTopologySpread), EnableNodeInclusionPolicyInPodTopologySpread: feature.DefaultFeatureGate.Enabled(features.NodeInclusionPolicyInPodTopologySpread), EnableMatchLabelKeysInPodTopologySpread: feature.DefaultFeatureGate.Enabled(features.MatchLabelKeysInPodTopologySpread), EnablePodSchedulingReadiness: feature.DefaultFeatureGate.Enabled(features.PodSchedulingReadiness), EnablePodDisruptionConditions: feature.DefaultFeatureGate.Enabled(features.PodDisruptionConditions), EnableInPlacePodVerticalScaling: feature.DefaultFeatureGate.Enabled(features.InPlacePodVerticalScaling), &#125; registry := runtime.Registry&#123; dynamicresources.Name: runtime.FactoryAdapter(fts, dynamicresources.New), selectorspread.Name: selectorspread.New, imagelocality.Name: imagelocality.New, tainttoleration.Name: tainttoleration.New, nodename.Name: nodename.New, nodeports.Name: nodeports.New, nodeaffinity.Name: nodeaffinity.New, podtopologyspread.Name: runtime.FactoryAdapter(fts, podtopologyspread.New), nodeunschedulable.Name: nodeunschedulable.New, noderesources.Name: runtime.FactoryAdapter(fts, noderesources.NewFit), noderesources.BalancedAllocationName: runtime.FactoryAdapter(fts, noderesources.NewBalancedAllocation), volumebinding.Name: runtime.FactoryAdapter(fts, volumebinding.New), volumerestrictions.Name: runtime.FactoryAdapter(fts, volumerestrictions.New), volumezone.Name: volumezone.New, nodevolumelimits.CSIName: runtime.FactoryAdapter(fts, nodevolumelimits.NewCSI), nodevolumelimits.EBSName: runtime.FactoryAdapter(fts, nodevolumelimits.NewEBS), nodevolumelimits.GCEPDName: runtime.FactoryAdapter(fts, nodevolumelimits.NewGCEPD), nodevolumelimits.AzureDiskName: runtime.FactoryAdapter(fts, nodevolumelimits.NewAzureDisk), nodevolumelimits.CinderName: runtime.FactoryAdapter(fts, nodevolumelimits.NewCinder), interpodaffinity.Name: interpodaffinity.New, queuesort.Name: queuesort.New, defaultbinder.Name: defaultbinder.New, defaultpreemption.Name: runtime.FactoryAdapter(fts, defaultpreemption.New), schedulinggates.Name: runtime.FactoryAdapter(fts, schedulinggates.New), &#125; return registry&#125; Features结构体是Kubernetes调度器中的一些特性开关。每个字段对应一个开关，代表该特性是否启用。关于每个字段的说明已经在上述代码中标注。 featureGate结构体内有一个enabled属性，类型为*atomic.Value，*atomic.Value主要是用来对其中的值进行原子化写入操作，具体可以看看这篇文章，其值为一个map类型，因为map不是线程安全的。 registry := runtime.Registry实例化了一个Registry结构体，runtime.FactoryAdapter主要是为了兼容旧版本插件的New方法 profile.NewMap()1234567891011121314151617181920212223242526type cfgValidator struct &#123; m Map queueSort string queueSortArgs runtime.Object&#125;// key为scheduler名称，值为frameworktype Map map[string]framework.Frameworkfunc NewMap(cfgs []config.KubeSchedulerProfile, r frameworkruntime.Registry, recorderFact RecorderFactory, stopCh &lt;-chan struct&#123;&#125;, opts ...frameworkruntime.Option) (Map, error) &#123; m := make(Map) v := cfgValidator&#123;m: m&#125; for _, cfg := range cfgs &#123; p, err := newProfile(cfg, r, recorderFact, stopCh, opts...) if err != nil &#123; return nil, fmt.Errorf(&quot;creating profile for scheduler name %s: %v&quot;, cfg.SchedulerName, err) &#125; if err := v.validate(cfg, p); err != nil &#123; return nil, err &#125; m[cfg.SchedulerName] = p &#125; return m, nil&#125; newProfile解析配置，然后将KubeSchedulerConfiguration的配置加载到新建的Framework结构体（可以视为一个Scheduler，包含了拓展点及其插件列表），最后返回一个key为SchedulerName，value为Scheduler的结构Map。在newProfile内通过，实例化了一个Framework结构体，并调用了Register的工厂方法，实例化插件。 一些调度器内置的代码可以在kubernetes的源代码库找到，其他的一些外置的插件，可以在这里找到。 NodeName它是一个非常常见的插件，它的作用是指定Pod所要调度的节点的名称，即spec.NodeName。他的代码很短，我们来看看： 123456789101112131415161718192021222324252627282930313233343536373839type NodeName struct&#123;&#125;...const ( // Name is the name of the plugin used in the plugin registry and configurations. Name = names.NodeName // ErrReason returned when node name doesn&#x27;t match. ErrReason = &quot;node(s) didn&#x27;t match the requested node name&quot;)...// Name returns name of the plugin. It is used in logs, etc.func (pl *NodeName) Name() string &#123; return Name&#125;// Filter invoked at the filter extension point.func (pl *NodeName) Filter(ctx context.Context, _ *framework.CycleState, pod *v1.Pod, nodeInfo *framework.NodeInfo) *framework.Status &#123; if nodeInfo.Node() == nil &#123; return framework.NewStatus(framework.Error, &quot;node not found&quot;) &#125; if !Fits(pod, nodeInfo) &#123; return framework.NewStatus(framework.UnschedulableAndUnresolvable, ErrReason) &#125; return nil&#125;// Fits actually checks if the pod fits the node.func Fits(pod *v1.Pod, nodeInfo *framework.NodeInfo) bool &#123; return len(pod.Spec.NodeName) == 0 || pod.Spec.NodeName == nodeInfo.Node().Name&#125;// New initializes a new plugin and returns it.func New(_ runtime.Object, _ framework.Handle) (framework.Plugin, error) &#123; return &amp;NodeName&#123;&#125;, nil&#125; 首先，NodeName是一个空的结构体，它主要是在Filter拓展点实现了Filter方法。 Filter方法调用了Fits方法，该方法用pod.Spec.NodeName和nodeInfo.Node().Name相比较，如果相等，则返回true 当pod.Spec.NodeName和nodeInfo.Node().Name不想等，则返回一个Status状态，说明Node不匹配的原因 当Pod匹配时，返回nil NodeAffinityNodeAffinity是比较常见的一个插件，我们知道，Affinity有两种类型，一种表示偏好，即preferredDuringSchedulingIgnoredDuringExecution，一种表示必须满足，即requiredDuringSchedulingIgnoredDuringExecution。 所以我们来看NodeAffinity的源码，我们可以看到： 12345type NodeAffinity struct &#123; handle framework.Handle addedNodeSelector *nodeaffinity.NodeSelector addedPrefSchedTerms *nodeaffinity.PreferredSchedulingTerms&#125; 其中addedNodeSelector代表requiredDuringSchedulingIgnoredDuringExecution，addedPrefSchedTerms代表requiredDuringSchedulingIgnoredDuringExecution。 NodeAffinity实现了PreFilter,Filter,PreScore,Score拓展点，我们按顺序来看PreFilter： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051func (pl *NodeAffinity) PreFilter(ctx context.Context, cycleState *framework.CycleState, pod *v1.Pod) (*framework.PreFilterResult, *framework.Status) &#123; affinity := pod.Spec.Affinity noNodeAffinity := (affinity == nil || affinity.NodeAffinity == nil || affinity.NodeAffinity.RequiredDuringSchedulingIgnoredDuringExecution == nil) if noNodeAffinity &amp;&amp; pl.addedNodeSelector == nil &amp;&amp; pod.Spec.NodeSelector == nil &#123; // NodeAffinity Filter has nothing to do with the Pod. return nil, framework.NewStatus(framework.Skip) &#125; state := &amp;preFilterState&#123;requiredNodeSelectorAndAffinity: nodeaffinity.GetRequiredNodeAffinity(pod)&#125; cycleState.Write(preFilterStateKey, state) if noNodeAffinity || len(affinity.NodeAffinity.RequiredDuringSchedulingIgnoredDuringExecution.NodeSelectorTerms) == 0 &#123; return nil, nil &#125; // Check if there is affinity to a specific node and return it. terms := affinity.NodeAffinity.RequiredDuringSchedulingIgnoredDuringExecution.NodeSelectorTerms var nodeNames sets.Set[string] for _, t := range terms &#123; var termNodeNames sets.Set[string] for _, r := range t.MatchFields &#123; if r.Key == metav1.ObjectNameField &amp;&amp; r.Operator == v1.NodeSelectorOpIn &#123; // The requirements represent ANDed constraints, and so we need to // find the intersection of nodes. s := sets.New(r.Values...) if termNodeNames == nil &#123; termNodeNames = s &#125; else &#123; termNodeNames = termNodeNames.Intersection(s) &#125; &#125; &#125; if termNodeNames == nil &#123; // If this term has no node.Name field affinity, // then all nodes are eligible because the terms are ORed. return nil, nil &#125; nodeNames = nodeNames.Union(termNodeNames) &#125; // If nodeNames is not nil, but length is 0, it means each term have conflicting affinity to node.Name; // therefore, pod will not match any node. if nodeNames != nil &amp;&amp; len(nodeNames) == 0 &#123; return nil, framework.NewStatus(framework.UnschedulableAndUnresolvable, errReasonConflict) &#125; else if len(nodeNames) &gt; 0 &#123; return &amp;framework.PreFilterResult&#123;NodeNames: nodeNames&#125;, nil &#125; return nil, nil&#125; 这个方法的实现类似于NodeSelector，首先是判断是否定义了RequiredDuringSchedulingIgnoredDuringExecution字段，如果没有则进入下一个拓展点 接着，将标签选择和节点选择的信息写到cycleState方便后续的拓展点处理 最后，判断是否存在节点选择的信息，即Key为meta.name，匹配模式为IN，如果有则返回选择的节点列表，如果没有则直接进入下一步处理 然后是Filter： 1234567891011121314151617181920212223242526func (pl *NodeAffinity) Filter(ctx context.Context, state *framework.CycleState, pod *v1.Pod, nodeInfo *framework.NodeInfo) *framework.Status &#123; node := nodeInfo.Node() if node == nil &#123; return framework.NewStatus(framework.Error, &quot;node not found&quot;) &#125; // 查找RequiredDuringSchedulingIgnoredDuringExecution是否能找到相应的节点 if pl.addedNodeSelector != nil &amp;&amp; !pl.addedNodeSelector.Match(node) &#123; return framework.NewStatus(framework.UnschedulableAndUnresolvable, errReasonEnforced) &#125; // 获取PreFilter的cycleState结构体（Pod指定的nodeselector和nodeaffinity） s, err := getPreFilterState(state) if err != nil &#123; // Fallback to calculate requiredNodeSelector and requiredNodeAffinity // here when PreFilter is disabled. s = &amp;preFilterState&#123;requiredNodeSelectorAndAffinity: nodeaffinity.GetRequiredNodeAffinity(pod)&#125; &#125; // Pod指定的nodeselector和nodeaffinity是否冲突 match, _ := s.requiredNodeSelectorAndAffinity.Match(node) if !match &#123; return framework.NewStatus(framework.UnschedulableAndUnresolvable, ErrReasonPod) &#125; return nil&#125; 首先查找RequiredDuringSchedulingIgnoredDuringExecution是否能找到相应的节点 然后获取PreFilter的cycleState结构体，这个是已经在PreFilter计算过并保存的结构体，这个结构体中保存了NodeSelector和NodeAffinity的信息 最后判断NodeSelector和NodeAffinity的信息是否冲突 同样的，PreScore也是对Score进行预处理，将preScoreState存入cycleState结构中： 1234567891011121314func (pl *NodeAffinity) PreScore(ctx context.Context, cycleState *framework.CycleState, pod *v1.Pod, nodes []*v1.Node) *framework.Status &#123; if len(nodes) == 0 &#123; return nil &#125; preferredNodeAffinity, err := getPodPreferredNodeAffinity(pod) if err != nil &#123; return framework.AsStatus(err) &#125; state := &amp;preScoreState&#123; preferredNodeAffinity: preferredNodeAffinity, &#125; cycleState.Write(preScoreStateKey, state) return nil&#125; Score： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849// 当matchExpressions和matchFields匹配的时候，将weight加入计算，最终返回weight之和func (t *PreferredSchedulingTerms) Score(node *v1.Node) int64 &#123; var score int64 nodeLabels := labels.Set(node.Labels) nodeFields := extractNodeFields(node) for _, term := range t.terms &#123; // parse errors are reported in NewPreferredSchedulingTerms. if ok, _ := term.match(nodeLabels, nodeFields); ok &#123; score += int64(term.weight) &#125; &#125; return score&#125;// 返回节点权重之和func (pl *NodeAffinity) Score(ctx context.Context, state *framework.CycleState, pod *v1.Pod, nodeName string) (int64, *framework.Status) &#123; nodeInfo, err := pl.handle.SnapshotSharedLister().NodeInfos().Get(nodeName) if err != nil &#123; return 0, framework.AsStatus(fmt.Errorf(&quot;getting node %q from Snapshot: %w&quot;, nodeName, err)) &#125; node := nodeInfo.Node() // 通过AddedAffinity规则计算节点分数 var count int64 if pl.addedPrefSchedTerms != nil &#123; count += pl.addedPrefSchedTerms.Score(node) &#125; // 获取PreScore的preScoreState结构，如果没有则重新计算 s, err := getPreScoreState(state) if err != nil &#123; // Fallback to calculate preferredNodeAffinity here when PreScore is disabled. preferredNodeAffinity, err := getPodPreferredNodeAffinity(pod) if err != nil &#123; return 0, framework.AsStatus(err) &#125; s = &amp;preScoreState&#123; preferredNodeAffinity: preferredNodeAffinity, &#125; &#125; // 通过preferredNodeAffinity规则计算节点分数 if s.preferredNodeAffinity != nil &#123; count += s.preferredNodeAffinity.Score(node) &#125; return count, nil&#125; 该方法对匹配的节点规则进行weight的累加 AddedAffinity是在调度规则中指定的，对指定的Pod生效(通过schedulerName)，可以看这个文章 preferredNodeAffinity规则在Pod的yaml中指定 总结本文从源码角度概述了调度器插件的注册过程，介绍了NodeSelector插件和NodeAffinity插件的实现方法。","categories":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"http://jerryblogs.com/categories/Kubernetes/"},{"name":"源码分析","slug":"Kubernetes/源码分析","permalink":"http://jerryblogs.com/categories/Kubernetes/%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"}],"tags":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"http://jerryblogs.com/tags/Kubernetes/"}]},{"title":"Kubernetes中的SchedulingQueue","slug":"Kubernetes中的SchedulingQueue","date":"2023-04-24T17:48:50.000Z","updated":"2023-04-24T20:34:59.111Z","comments":true,"path":"2023/04/25/Kubernetes中的SchedulingQueue/","link":"","permalink":"http://jerryblogs.com/2023/04/25/Kubernetes%E4%B8%AD%E7%9A%84SchedulingQueue/","excerpt":"","text":"概述从Scheduler启动主循环的函数，我们可以看到，使用了sched.SchedulingQueue.Run()，调用了SchedulingQueue的Run方法，我们来看看SchedulingQueue做了什么 SchedulingQueue1234567891011121314151617181920212223242526272829303132//SchedulingQueue 是队列的接口，用于存储等待调度的 pod。//该接口遵循类似于 cache.FIFO 和 cache.Heap 的模式和//使得将这些数据结构用作 SchedulingQueue 变得容易。type SchedulingQueue interface &#123; framework.PodNominator Add(pod *v1.Pod) error //Activate 将给定的 pod 移动到 activeQ，前提是它们在 unschedulablePods 或 backoffQ 中 //传入的 Pod 最初是由想要激活 Pod 的插件编译而来的， //通过保留的 CycleState 结构 (PodsToActivate) 注入 pod Activate(pods map[string]*v1.Pod) //AddUnschedulableIfNotPresent 将不可调度的 pod 添加回调度队列 //podSchedulingCycle 表示当前的调度周期数，可以是 //通过调用 SchedulingCycle() 返回 AddUnschedulableIfNotPresent(pod *framework.QueuedPodInfo, podSchedulingCycle int64) error //SchedulingCycle 返回当前的调度周期数，即 //通过调度队列缓存。通常，每当 //弹出一个 pod（例如调用 Pop()）就足够了 SchedulingCycle() int64 //Pop 移除队列的头部并将其返回。如果 //队列为空，等待直到有新项目添加到队列中 Pop() (*framework.QueuedPodInfo, error) Update(oldPod, newPod *v1.Pod) error Delete(pod *v1.Pod) error MoveAllToActiveOrBackoffQueue(event framework.ClusterEvent, preCheck PreEnqueueCheck) AssignedPodAdded(pod *v1.Pod) AssignedPodUpdated(pod *v1.Pod) PendingPods() ([]*v1.Pod, string) //Close 关闭 SchedulingQueue 以便等待弹出项目的goroutine可以优雅地退出 Close() // Run启动管理队列的 goroutines Run()&#125; 我们可以看到，SchedulingQueue这个接口定义了一些列的方法，在k8s，默认的实现是PriorityQueue。 123456func NewSchedulingQueue( lessFn framework.LessFunc, informerFactory informers.SharedInformerFactory, opts ...Option) SchedulingQueue &#123; return NewPriorityQueue(lessFn, informerFactory, opts...)&#125; PriorityQueue先来看看PriorityQueue的结构： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051// PriorityQueue实现了一个调度队列// PriorityQueue 的头部是最高优先级的pending pod。 // 这个结构有两个子队列和一个可选的数据结构，即activeQ，backoffQ和unschedulablePods。// - activeQ 持有正在考虑调度的Pod// - backoffQ 持有从unschedulablePods移动到的pod，并将在他们的backoffQ退避期结束时移动到activeQ// - unschedulablePods 持有那些已经被尝试过调度并且当前被确定不可调度的podtype PriorityQueue struct &#123; *nominator // 关闭队列的通道 stop chan struct&#123;&#125; // 用于获取当前时间的时钟 clock clock.Clock // Pod 初始的回退时长，用于实现 backoff 机制 podInitialBackoffDuration time.Duration // Pod 最大的回退时长，用于实现 backoff 机制 podMaxBackoffDuration time.Duration // Pod 在 unschedulablePods 中最大的停留时长 podMaxInUnschedulablePodsDuration time.Duration // 用于实现条件变量的 sync.Cond cond sync.Cond // 一个基于堆的结构，用于存储调度器正在主动查看以找到要调度的 Pod // 堆的头部是最高优先级的 Pod activeQ *heap.Heap // 一个基于堆的结构，按照回退过期时间排序的 Pod 队列 // 用于存储完成回退的 Pod，在调度器查看 activeQ 之前，从该堆中弹出 Pod podBackoffQ *heap.Heap // 存储已经尝试过但无法调度的 Pod 的结构 unschedulablePods *UnschedulablePods // 调度周期的序列号，每次从队列中弹出一个 Pod 时会递增 schedulingCycle int64 // 缓存在接收到移动请求时的调度周期的序列号 // 如果在接收到移动请求时正在尝试调度的 Pod 则会被放回 activeQ moveRequestCycle int64 // 用于存储集群事件和对应的 Pod 集合的映射 clusterEventMap map[framework.ClusterEvent]sets.Set[string] // 存储注册的preEnqueue plugins插件，以配置文件名为键 preEnqueuePluginMap map[string][]framework.PreEnqueuePlugin // 表示队列是否已关闭 closed bool // 用于从缓存中获取 Namespace 列表的 Listers 接口 nsLister listersv1.NamespaceLister // 用于记录调度器指标的异步记录器 metricsRecorder metrics.MetricAsyncRecorder // 插件指标抽样百分比，用于控制插件指标的采样率 pluginMetricsSamplePercent int&#125; 我们来看看PriorityQueue.Run()方法： 12345// Run starts the goroutine to pump from podBackoffQ to activeQfunc (p *PriorityQueue) Run() &#123; go wait.Until(p.flushBackoffQCompleted, 1.0*time.Second, p.stop) go wait.Until(p.flushUnschedulablePodsLeftover, 30*time.Second, p.stop)&#125; 我们可以看到，Run()方法开启了两个goroutine，分别是 flushBackoffQCompleted 和 flushUnschedulablePodsLeftover。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950// flushBackoffQCompleted 将所有已完成退避的 pod 从 backoffQ 移动到 activeQfunc (p *PriorityQueue) flushBackoffQCompleted() &#123; p.lock.Lock() defer p.lock.Unlock() activated := false for &#123; rawPodInfo := p.podBackoffQ.Peek() if rawPodInfo == nil &#123; break &#125; pInfo := rawPodInfo.(*framework.QueuedPodInfo) pod := pInfo.Pod if p.isPodBackingoff(pInfo) &#123; break &#125; _, err := p.podBackoffQ.Pop() if err != nil &#123; klog.ErrorS(err, &quot;Unable to pop pod from backoff queue despite backoff completion&quot;, &quot;pod&quot;, klog.KObj(pod)) break &#125; if added, _ := p.addToActiveQ(pInfo); added &#123; klog.V(5).InfoS(&quot;Pod moved to an internal scheduling queue&quot;, &quot;pod&quot;, klog.KObj(pod), &quot;event&quot;, BackoffComplete, &quot;queue&quot;, activeQName) metrics.SchedulerQueueIncomingPods.WithLabelValues(&quot;active&quot;, BackoffComplete).Inc() activated = true &#125; &#125; if activated &#123; p.cond.Broadcast() &#125;&#125;// flushUnschedulablePodsLeftover移动留在unschedulablePods中且超过podMaxInUnschedulablePodsDuration时间的Pod到backoffQ或activeQfunc (p *PriorityQueue) flushUnschedulablePodsLeftover() &#123; p.lock.Lock() defer p.lock.Unlock() var podsToMove []*framework.QueuedPodInfo currentTime := p.clock.Now() for _, pInfo := range p.unschedulablePods.podInfoMap &#123; lastScheduleTime := pInfo.Timestamp if currentTime.Sub(lastScheduleTime) &gt; p.podMaxInUnschedulablePodsDuration &#123; podsToMove = append(podsToMove, pInfo) &#125; &#125; if len(podsToMove) &gt; 0 &#123; p.movePodsToActiveOrBackoffQueue(podsToMove, UnschedulableTimeout) &#125;&#125; 我们可以看到两个方法都是加锁的： flushBackoffQCompleted： 对Pod进行PreEnqueue检查，如果Gated字段为 true，表示该 Pod 没有通过预调度插件的检查，被禁止调度，将该Pod添加到unschedulablePods中 如果 Gated 字段为 false，表示该Pod通过了预调度插件的检查，可以继续被调度，将调用activeQ的Add方法将QueuedPodInfo对象添加到activeQ中，作为一个优先级队列（heap） flushUnschedulablePodsLeftover： 检查是否有Pod在unschedulablePods中且超过podMaxInUnschedulablePodsDuration时间 如果有，则根据Pod的状态是否为podBackoff，如果是则放入activeQ，如不是则放入podBackoffQ 当Pod调度失败当Filter阶段调度失败时，会调用AddUnschedulableIfNotPresent方法： 123456789101112131415161718192021222324252627282930313233343536373839func (p *PriorityQueue) AddUnschedulableIfNotPresent(pInfo *framework.QueuedPodInfo, podSchedulingCycle int64) error &#123; p.lock.Lock() defer p.lock.Unlock() pod := pInfo.Pod if p.unschedulablePods.get(pod) != nil &#123; return fmt.Errorf(&quot;Pod %v is already present in unschedulable queue&quot;, klog.KObj(pod)) &#125; if _, exists, _ := p.activeQ.Get(pInfo); exists &#123; return fmt.Errorf(&quot;Pod %v is already present in the active queue&quot;, klog.KObj(pod)) &#125; if _, exists, _ := p.podBackoffQ.Get(pInfo); exists &#123; return fmt.Errorf(&quot;Pod %v is already present in the backoff queue&quot;, klog.KObj(pod)) &#125; // Refresh the timestamp since the pod is re-added. pInfo.Timestamp = p.clock.Now() // If a move request has been received, move it to the BackoffQ, otherwise move // it to unschedulablePods. for plugin := range pInfo.UnschedulablePlugins &#123; metrics.UnschedulableReason(plugin, pInfo.Pod.Spec.SchedulerName).Inc() &#125; if p.moveRequestCycle &gt;= podSchedulingCycle &#123; if err := p.podBackoffQ.Add(pInfo); err != nil &#123; return fmt.Errorf(&quot;error adding pod %v to the backoff queue: %v&quot;, klog.KObj(pod), err) &#125; klog.V(5).InfoS(&quot;Pod moved to an internal scheduling queue&quot;, &quot;pod&quot;, klog.KObj(pod), &quot;event&quot;, ScheduleAttemptFailure, &quot;queue&quot;, backoffQName) metrics.SchedulerQueueIncomingPods.WithLabelValues(&quot;backoff&quot;, ScheduleAttemptFailure).Inc() &#125; else &#123; p.unschedulablePods.addOrUpdate(pInfo) klog.V(5).InfoS(&quot;Pod moved to an internal scheduling queue&quot;, &quot;pod&quot;, klog.KObj(pod), &quot;event&quot;, ScheduleAttemptFailure, &quot;queue&quot;, unschedulablePods) metrics.SchedulerQueueIncomingPods.WithLabelValues(&quot;unschedulable&quot;, ScheduleAttemptFailure).Inc() &#125; p.addNominatedPodUnlocked(pInfo.PodInfo, nil) return nil&#125; 具体逻辑如下： 获取待添加的 Pod 信息：从传入的 QueuedPodInfo 对象中获取 Pod 对象。 检查 Pod 是否已经存在于不可调度队列、活跃队列或者后退队列中，如果是则返回错误，表示 Pod 已经存在于相应的队列中。 更新 Pod 的时间戳：将 QueuedPodInfo 对象中的时间戳更新为当前时间，表示 Pod 被重新添加到队列中。 根据 Pod 的 move request 判断放入哪个队列：根据 Pod 的 move request 时间与当前调度轮次（podSchedulingCycle）的比较，决定将 Pod 放入后退队列（backoffQ）还是不可调度队列（unschedulablePods）。如果 Pod 的 move request 时间早于或等于当前调度轮次，则将 Pod 放入后退队列（backoffQ）；否则，将 Pod 放入不可调度队列（unschedulablePods）。 更新队列信息和计数器：根据放入的队列类型，更新相应队列的信息和计数器。同时，调用 addNominatedPodUnlocked 方法，将 Pod 加入到优先队列的 nominatedPods 中。 返回错误或 nil：根据操作的结果，返回相应的错误（如果有）或者 nil，表示成功添加 Pod 到队列中。 总结根据三篇文章，可以画出此流程图：","categories":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"http://jerryblogs.com/categories/Kubernetes/"},{"name":"源码分析","slug":"Kubernetes/源码分析","permalink":"http://jerryblogs.com/categories/Kubernetes/%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"}],"tags":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"http://jerryblogs.com/tags/Kubernetes/"}]},{"title":"Kubernetes如何启动Scheduler","slug":"Kubernetes的scheduler插件","date":"2023-04-24T15:00:31.000Z","updated":"2023-04-24T20:03:06.166Z","comments":true,"path":"2023/04/24/Kubernetes的scheduler插件/","link":"","permalink":"http://jerryblogs.com/2023/04/24/Kubernetes%E7%9A%84scheduler%E6%8F%92%E4%BB%B6/","excerpt":"","text":"概述 Kubernetes如何启动Scheduler 源码解析Scheduler上一篇文章文章我们讲了Schuduler的主循环，这一节我们讲讲kube-scheduler如何启动。 Setup1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859/ Setup creates a completed config and a scheduler based on the command args and optionsfunc Setup(ctx context.Context, opts *options.Options, outOfTreeRegistryOptions ...Option) (*schedulerserverconfig.CompletedConfig, *scheduler.Scheduler, error) &#123; if cfg, err := latest.Default(); err != nil &#123; return nil, nil, err &#125; else &#123; opts.ComponentConfig = cfg &#125; if errs := opts.Validate(); len(errs) &gt; 0 &#123; return nil, nil, utilerrors.NewAggregate(errs) &#125; c, err := opts.Config(ctx) if err != nil &#123; return nil, nil, err &#125; // Get the completed config cc := c.Complete() outOfTreeRegistry := make(runtime.Registry) for _, option := range outOfTreeRegistryOptions &#123; if err := option(outOfTreeRegistry); err != nil &#123; return nil, nil, err &#125; &#125; recorderFactory := getRecorderFactory(&amp;cc) completedProfiles := make([]kubeschedulerconfig.KubeSchedulerProfile, 0) // Create the scheduler. sched, err := scheduler.New(cc.Client, cc.InformerFactory, cc.DynInformerFactory, recorderFactory, ctx.Done(), scheduler.WithComponentConfigVersion(cc.ComponentConfig.TypeMeta.APIVersion), scheduler.WithKubeConfig(cc.KubeConfig), scheduler.WithProfiles(cc.ComponentConfig.Profiles...), scheduler.WithPercentageOfNodesToScore(cc.ComponentConfig.PercentageOfNodesToScore), scheduler.WithFrameworkOutOfTreeRegistry(outOfTreeRegistry), scheduler.WithPodMaxBackoffSeconds(cc.ComponentConfig.PodMaxBackoffSeconds), scheduler.WithPodInitialBackoffSeconds(cc.ComponentConfig.PodInitialBackoffSeconds), scheduler.WithPodMaxInUnschedulablePodsDuration(cc.PodMaxInUnschedulablePodsDuration), scheduler.WithExtenders(cc.ComponentConfig.Extenders...), scheduler.WithParallelism(cc.ComponentConfig.Parallelism), scheduler.WithBuildFrameworkCapturer(func(profile kubeschedulerconfig.KubeSchedulerProfile) &#123; // Profiles are processed during Framework instantiation to set default plugins and configurations. Capturing them for logging completedProfiles = append(completedProfiles, profile) &#125;), ) if err != nil &#123; return nil, nil, err &#125; if err := options.LogOrWriteConfig(klog.FromContext(ctx), opts.WriteConfigTo, &amp;cc.ComponentConfig, completedProfiles); err != nil &#123; return nil, nil, err &#125; return &amp;cc, sched, nil&#125; 根据注释，我们可以看到这个函数主要是用来根据命令行参数和选项创建配置文件和Scheduler。 1.使用了last.Default()函数，我们可以得到一个默认的config。会使用最新API版本的配置默认值。默认配置包括： metav1.TypeMeta，包括Kind和APIVersion Parallelism，并行度定义了用于调度 Pod 的算法中的并行度。必须大于 0。默认为 16 LeaderElection，定义了选举领导者的配置，具体配置可以查看LeaderElectionConfiguration结构体 ClientConnection，指定使用kubeconfig与apiserver通信时的配置 componentbaseconfigv1alpha1.DebuggingConfiguration，调试相关的功能和配置 PercentageOfNodesToScore，发现可行性节点的配置 PodInitialBackoffSeconds，不可调度Pod的初始退避 PodMaxBackoffSeconds，不可调度Pod的最大退避 Profiles，Pod指定的调度程序的名称 Extenders，拓展器列表 2.opts.Validate对用户的配置进行检查 3.c.Complete获取完整的配置 4.使用scheduler.New创建一个Scheduler结构体实例 scheduler.New()scheduler.New()用来创建一个Scheduler实例： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134func New(client clientset.Interface, informerFactory informers.SharedInformerFactory, dynInformerFactory dynamicinformer.DynamicSharedInformerFactory, recorderFactory profile.RecorderFactory, stopCh &lt;-chan struct&#123;&#125;, opts ...Option) (*Scheduler, error) &#123; // 停止信号通道默认值 stopEverything := stopCh if stopEverything == nil &#123; stopEverything = wait.NeverStop &#125; // 创建默认的配置，opt为用来给scheduler赋值的工厂函数 options := defaultSchedulerOptions for _, opt := range opts &#123; opt(&amp;options) &#125; if options.applyDefaultProfile &#123; var versionedCfg configv1.KubeSchedulerConfiguration scheme.Scheme.Default(&amp;versionedCfg) cfg := schedulerapi.KubeSchedulerConfiguration&#123;&#125; if err := scheme.Scheme.Convert(&amp;versionedCfg, &amp;cfg, nil); err != nil &#123; return nil, err &#125; options.profiles = cfg.Profiles &#125; // 内置插件注册表，里面提供了一系列的特性开关 registry := frameworkplugins.NewInTreeRegistry() // 合并外置插件注册表 if err := registry.Merge(options.frameworkOutOfTreeRegistry); err != nil &#123; return nil, err &#125; // 指标注册 metrics.Register() // 构建拓展器 extenders, err := buildExtenders(options.extenders, options.profiles) if err != nil &#123; return nil, fmt.Errorf(&quot;couldn&#x27;t build extenders: %w&quot;, err) &#125; // 创建 Pod 和 Node 的 Lister，用于访问 Informer 中的缓存数据 podLister := informerFactory.Core().V1().Pods().Lister() nodeLister := informerFactory.Core().V1().Nodes().Lister() // 创建调度器使用的快照、集群事件映射和度量指标记录器 snapshot := internalcache.NewEmptySnapshot() clusterEventMap := make(map[framework.ClusterEvent]sets.Set[string]) metricsRecorder := metrics.NewMetricsAsyncRecorder(1000, time.Second, stopCh) // 初始化调度器的配置文件，并创建调度器使用的配置文件映射 profiles, err := profile.NewMap(options.profiles, registry, recorderFactory, stopCh, frameworkruntime.WithComponentConfigVersion(options.componentConfigVersion), frameworkruntime.WithClientSet(client), frameworkruntime.WithKubeConfig(options.kubeConfig), frameworkruntime.WithInformerFactory(informerFactory), frameworkruntime.WithSnapshotSharedLister(snapshot), frameworkruntime.WithCaptureProfile(frameworkruntime.CaptureProfile(options.frameworkCapturer)), frameworkruntime.WithClusterEventMap(clusterEventMap), frameworkruntime.WithClusterEventMap(clusterEventMap), frameworkruntime.WithParallelism(int(options.parallelism)), frameworkruntime.WithExtenders(extenders), frameworkruntime.WithMetricsRecorder(metricsRecorder), ) if err != nil &#123; return nil, fmt.Errorf(&quot;initializing profiles: %v&quot;, err) &#125; if len(profiles) == 0 &#123; return nil, errors.New(&quot;at least one profile is required&quot;) &#125; // 创建预处理插件映射，用于在调度前执行预处理插件 preEnqueuePluginMap := make(map[string][]framework.PreEnqueuePlugin) for profileName, profile := range profiles &#123; preEnqueuePluginMap[profileName] = profile.PreEnqueuePlugins() &#125; // 创建 Pod 的调度队列，设置队列的排序函数、初始和最大回退时间、Lister、集群事件映射、最大未调度 Pod 持续时间等 podQueue := internalqueue.NewSchedulingQueue( // 指定 Pod 调度队列中的排序函数，用于决定 Pod 的调度顺序。这里使用了一个配置文件中指定的调度器名称对应的排序函数 profiles[options.profiles[0].SchedulerName].QueueSortFunc(), // 用于创建 Informer 对象的工厂，这些 Informer 对象用于监听 Kubernetes API Server 中的资源变更事件，以便调度器能够实时获取最新的资源状态 informerFactory, // 设置 Pod 的初始退避时间和最大退避时间，用于在调度队列中处理失败的调度尝试时进行重试 internalqueue.WithPodInitialBackoffDuration(time.Duration(options.podInitialBackoffSeconds)*time.Second), internalqueue.WithPodMaxBackoffDuration(time.Duration(options.podMaxBackoffSeconds)*time.Second), // 设置用于获取 Pod 列表的 Lister 接口，用于从本地缓存中获取 Pod 的最新信息，以提高调度队列的性能 internalqueue.WithPodLister(podLister), // 设置用于处理集群级事件的映射，这些事件包括节点状态变更、节点资源变更等 internalqueue.WithClusterEventMap(clusterEventMap), // 设置 Pod 在调度队列中最大的未调度时长，用于控制长时间未调度的 Pod 的处理方式 internalqueue.WithPodMaxInUnschedulablePodsDuration(options.podMaxInUnschedulablePodsDuration), // 设置预调度插件（PreFilter、Filter 和 Score）的映射，用于在将 Pod 加入调度队列之前进行预处理，例如资源检查、节点亲和性/亲和性反亲和性检查等 internalqueue.WithPreEnqueuePluginMap(preEnqueuePluginMap), // 设置插件指标采样的百分比，用于控制插件指标的采样率，以降低对性能的影响 internalqueue.WithPluginMetricsSamplePercent(pluginMetricsSamplePercent), // 设置用于记录调度队列指标的 MetricsRecorder，用于监控和度量调度队列的性能 internalqueue.WithMetricsRecorder(*metricsRecorder), ) // 遍历所有配置文件，为每个配置文件设置 Pod 的提名器 for _, fwk := range profiles &#123; fwk.SetPodNominator(podQueue) &#125; // 创建调度器缓存，用于存储调度器运行时状态 schedulerCache := internalcache.New(durationToExpireAssumedPod, stopEverything) // 创建缓存调试器，用于监听停止信号并进行调试 debugger := cachedebugger.New(nodeLister, podLister, schedulerCache, podQueue) debugger.ListenForSignal(stopEverything) // 创建调度器实例，并应用默认的事件处理函数 sched := &amp;Scheduler&#123; Cache: schedulerCache, client: client, nodeInfoSnapshot: snapshot, percentageOfNodesToScore: options.percentageOfNodesToScore, Extenders: extenders, NextPod: internalqueue.MakeNextPodFunc(podQueue), StopEverything: stopEverything, SchedulingQueue: podQueue, Profiles: profiles, &#125; sched.applyDefaultHandlers() // 添加所有的事件处理函数到调度器中，包括 Informer 和动态 Informer 的事件处理函数 addAllEventHandlers(sched, informerFactory, dynInformerFactory, unionedGVKs(clusterEventMap)) return sched, nil&#125; 这段代码的输入参数为： client: clientset.Interface 类型，表示与 Kubernetes API 通信的客户端 informerFactory: informer.SharedInformerFactory 类型，表示用于创建 Informer 的工厂，用于从 Kubernetes API 接收事件并维护本地缓存 dynInformerFactory: dynamicinformer.DynamicSharedInformerFactory 类型，表示用于创建动态 Informer 的工厂，用于从 Kubernetes API 接收动态资源事件并维护本地缓存 recorderFactory: profile.RecorderFactory 类型，表示用于创建事件记录器的工厂，用于记录调度器产生的事件 stopCh: &lt;-chan struct{} 类型，表示一个停止信号通道，用于通知调度器停止运行 opts …Option: 可选的选项参数，用于配置调度器的行为 sched.applyDefaultHandlers()在New一个Scheduler的时候会给scheduler结构体实例添加默认的Handler，即sched.applyDefaultHandlers()： 1234func (s *Scheduler) applyDefaultHandlers() &#123; s.SchedulePod = s.schedulePod s.FailureHandler = s.handleSchedulingFailure&#125; 我们可以看到给scheduler添加了两个方法，即schedulePod和handleSchedulingFailure。 schedulePod123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263// 尝试调度一个给定的Pod到节点列表中的节点// 如果成功返回节点名称// 失败返回FitErrorfunc (sched *Scheduler) schedulePod(ctx context.Context, fwk framework.Framework, state *framework.CycleState, pod *v1.Pod) (result ScheduleResult, err error) &#123; // 创建了一个名为 &quot;Scheduling&quot; 的追踪（trace）对象，用于在调度过程中记录相关的跟踪信息。 // 这个追踪对象包含两个字段：一个是 &quot;namespace&quot;，值为 Pod 的命名空间（pod.Namespace）， // 另一个是 &quot;name&quot;，值为 Pod 的名称（pod.Name）。 // 这些字段将被用于在跟踪信息中记录 Pod 的命名空间和名称，以便在调度过程中进行监控和日志记录 trace := utiltrace.New(&quot;Scheduling&quot;, utiltrace.Field&#123;Key: &quot;namespace&quot;, Value: pod.Namespace&#125;, utiltrace.Field&#123;Key: &quot;name&quot;, Value: pod.Name&#125;) // 调度过程中检查追踪对象 trace 是否超时（超过 100 毫秒） defer trace.LogIfLong(100 * time.Millisecond) // 更新调度器的缓存快照 if err := sched.Cache.UpdateSnapshot(sched.nodeInfoSnapshot); err != nil &#123; return result, err &#125; trace.Step(&quot;Snapshotting scheduler cache and node infos done&quot;) // 调度器的节点信息快照（nodeInfoSnapshot）中节点的数量是否为0。如果节点数量为0，说明没有可用的节点进行调度 if sched.nodeInfoSnapshot.NumNodes() == 0 &#123; return result, ErrNoNodesAvailable &#125; // 查找满足 pod 调度条件的节点，并获取调度诊断信息，以便后续的调度决策和处理 feasibleNodes, diagnosis, err := sched.findNodesThatFitPod(ctx, fwk, state, pod) if err != nil &#123; return result, err &#125; trace.Step(&quot;Computing predicates done&quot;) // 没有满足条件的节点 if len(feasibleNodes) == 0 &#123; return result, &amp;framework.FitError&#123; Pod: pod, NumAllNodes: sched.nodeInfoSnapshot.NumNodes(), Diagnosis: diagnosis, &#125; &#125; // 只有一个节点，直接使用这个节点 if len(feasibleNodes) == 1 &#123; return ScheduleResult&#123; SuggestedHost: feasibleNodes[0].Name, EvaluatedNodes: 1 + len(diagnosis.NodeToStatusMap), FeasibleNodes: 1, &#125;, nil &#125; // 进行优先级排序，选择最优的节点 priorityList, err := prioritizeNodes(ctx, sched.Extenders, fwk, state, pod, feasibleNodes) if err != nil &#123; return result, err &#125; // 从priorityList找出得分最高的节点，如果多个节点得分一样，则从得分一致的节点列表随机选择一个 host, err := selectHost(priorityList) trace.Step(&quot;Prioritizing done&quot;) return ScheduleResult&#123; SuggestedHost: host, EvaluatedNodes: len(feasibleNodes) + len(diagnosis.NodeToStatusMap), FeasibleNodes: len(feasibleNodes), &#125;, err&#125; 从代码可以看到，有两个比较关键的函数sched.findNodesThatFitPod和prioritizeNodes： sched.findNodesThatFitPod： 使用过滤器插件和过滤器拓展器来查找可用的节点。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273func (sched *Scheduler) findNodesThatFitPod(ctx context.Context, fwk framework.Framework, state *framework.CycleState, pod *v1.Pod) ([]*v1.Node, framework.Diagnosis, error) &#123; // 记录详细信息以诊断调度失败 diagnosis := framework.Diagnosis&#123; NodeToStatusMap: make(framework.NodeToStatusMap), UnschedulablePlugins: sets.New[string](), &#125; // 从节点信息快照中获取所有的节点信息 allNodes, err := sched.nodeInfoSnapshot.NodeInfos().List() if err != nil &#123; return nil, diagnosis, err &#125; // 运行 PreFilterPlugins 插件 preRes, s := fwk.RunPreFilterPlugins(ctx, state, pod) // 失败 if !s.IsSuccess() &#123; // 不为不可调度 if !s.IsUnschedulable() &#123; return nil, diagnosis, s.AsError() &#125; // Record the messages from PreFilter in Diagnosis.PreFilterMsg. msg := s.Message() diagnosis.PreFilterMsg = msg klog.V(5).InfoS(&quot;Status after running PreFilter plugins for pod&quot;, &quot;pod&quot;, klog.KObj(pod), &quot;status&quot;, msg) // Status satisfying IsUnschedulable() gets injected into diagnosis.UnschedulablePlugins. if s.FailedPlugin() != &quot;&quot; &#123; diagnosis.UnschedulablePlugins.Insert(s.FailedPlugin()) &#125; return nil, diagnosis, nil &#125; // 存在已经被提名的节点，即类似nodeSelector，Node Affinity等 if len(pod.Status.NominatedNodeName) &gt; 0 &#123; feasibleNodes, err := sched.evaluateNominatedNode(ctx, pod, fwk, state, diagnosis) if err != nil &#123; klog.ErrorS(err, &quot;Evaluation failed on nominated node&quot;, &quot;pod&quot;, klog.KObj(pod), &quot;node&quot;, pod.Status.NominatedNodeName) &#125; // Nominated node passes all the filters, scheduler is good to assign this node to the pod. if len(feasibleNodes) != 0 &#123; return feasibleNodes, diagnosis, nil &#125; &#125; nodes := allNodes if !preRes.AllNodes() &#123; nodes = make([]*framework.NodeInfo, 0, len(preRes.NodeNames)) for n := range preRes.NodeNames &#123; nInfo, err := sched.nodeInfoSnapshot.NodeInfos().Get(n) if err != nil &#123; return nil, diagnosis, err &#125; nodes = append(nodes, nInfo) &#125; &#125; // 根据Filter插件列表查找适合过滤器插件的节点 feasibleNodes, err := sched.findNodesThatPassFilters(ctx, fwk, state, pod, diagnosis, nodes) // 用于记录下一次开始调度的起始节点的索引值。 // 在调度过程中，调度器（scheduler）通常会遍历所有的节点进行评估，然后选择一个合适的节点进行调度。 // 为了避免每次都从同一个节点开始评估，可以使用 nextStartNodeIndex 变量记录下一次开始评估的节点索引值， // 以便能够在下一次调度过程中从下一个节点开始，从而实现节点的轮询（Round-robin）策略 processedNodes := len(feasibleNodes) + len(diagnosis.NodeToStatusMap) sched.nextStartNodeIndex = (sched.nextStartNodeIndex + processedNodes) % len(nodes) if err != nil &#123; return nil, diagnosis, err &#125; // 根据Filter拓展器列表查找可用的节点列表 feasibleNodes, err = findNodesThatPassExtenders(sched.Extenders, pod, feasibleNodes, diagnosis.NodeToStatusMap) if err != nil &#123; return nil, diagnosis, err &#125; return feasibleNodes, diagnosis, nil&#125; prioritizeNodes 运行评分插件来确定节点的优先级，从调用 RunScorePlugins() 返回每个节点的分数。来自每个插件的分数被加在一起以得到该节点的分数，然后任何扩展程序也会运行。最后将所有分数合并（相加）得到所有节点的总加权分数。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112func prioritizeNodes( ctx context.Context, extenders []framework.Extender, fwk framework.Framework, state *framework.CycleState, pod *v1.Pod, nodes []*v1.Node,) ([]framework.NodePluginScores, error) &#123; //如果未提供优先级配置，则所有节点的得分均为 1。 //这是生成所需格式的优先级列表所必需的 if len(extenders) == 0 &amp;&amp; !fwk.HasScorePlugins() &#123; result := make([]framework.NodePluginScores, 0, len(nodes)) for i := range nodes &#123; result = append(result, framework.NodePluginScores&#123; Name: nodes[i].Name, TotalScore: 1, &#125;) &#125; return result, nil &#125; // Run PreScore plugins. preScoreStatus := fwk.RunPreScorePlugins(ctx, state, pod, nodes) if !preScoreStatus.IsSuccess() &#123; return nil, preScoreStatus.AsError() &#125; // Run the Score plugins. nodesScores, scoreStatus := fwk.RunScorePlugins(ctx, state, pod, nodes) if !scoreStatus.IsSuccess() &#123; return nil, scoreStatus.AsError() &#125; // Additional details logged at level 10 if enabled. klogV := klog.V(10) if klogV.Enabled() &#123; for _, nodeScore := range nodesScores &#123; for _, pluginScore := range nodeScore.Scores &#123; klogV.InfoS(&quot;Plugin scored node for pod&quot;, &quot;pod&quot;, klog.KObj(pod), &quot;plugin&quot;, pluginScore.Name, &quot;node&quot;, nodeScore.Name, &quot;score&quot;, pluginScore.Score) &#125; &#125; &#125; if len(extenders) != 0 &amp;&amp; nodes != nil &#123; // allNodeExtendersScores has all extenders scores for all nodes. // It is keyed with node name. allNodeExtendersScores := make(map[string]*framework.NodePluginScores, len(nodes)) var mu sync.Mutex var wg sync.WaitGroup for i := range extenders &#123; if !extenders[i].IsInterested(pod) &#123; continue &#125; wg.Add(1) go func(extIndex int) &#123; metrics.SchedulerGoroutines.WithLabelValues(metrics.PrioritizingExtender).Inc() metrics.Goroutines.WithLabelValues(metrics.PrioritizingExtender).Inc() defer func() &#123; metrics.SchedulerGoroutines.WithLabelValues(metrics.PrioritizingExtender).Dec() metrics.Goroutines.WithLabelValues(metrics.PrioritizingExtender).Dec() wg.Done() &#125;() prioritizedList, weight, err := extenders[extIndex].Prioritize(pod, nodes) if err != nil &#123; // Prioritization errors from extender can be ignored, let k8s/other extenders determine the priorities klog.V(5).InfoS(&quot;Failed to run extender&#x27;s priority function. No score given by this extender.&quot;, &quot;error&quot;, err, &quot;pod&quot;, klog.KObj(pod), &quot;extender&quot;, extenders[extIndex].Name()) return &#125; mu.Lock() defer mu.Unlock() for i := range *prioritizedList &#123; nodename := (*prioritizedList)[i].Host score := (*prioritizedList)[i].Score if klogV.Enabled() &#123; klogV.InfoS(&quot;Extender scored node for pod&quot;, &quot;pod&quot;, klog.KObj(pod), &quot;extender&quot;, extenders[extIndex].Name(), &quot;node&quot;, nodename, &quot;score&quot;, score) &#125; // MaxExtenderPriority may diverge from the max priority used in the scheduler and defined by MaxNodeScore, // therefore we need to scale the score returned by extenders to the score range used by the scheduler. finalscore := score * weight * (framework.MaxNodeScore / extenderv1.MaxExtenderPriority) if allNodeExtendersScores[nodename] == nil &#123; allNodeExtendersScores[nodename] = &amp;framework.NodePluginScores&#123; Name: nodename, Scores: make([]framework.PluginScore, 0, len(extenders)), &#125; &#125; allNodeExtendersScores[nodename].Scores = append(allNodeExtendersScores[nodename].Scores, framework.PluginScore&#123; Name: extenders[extIndex].Name(), Score: finalscore, &#125;) allNodeExtendersScores[nodename].TotalScore += finalscore &#125; &#125;(i) &#125; // wait for all go routines to finish wg.Wait() for i := range nodesScores &#123; if score, ok := allNodeExtendersScores[nodes[i].Name]; ok &#123; nodesScores[i].Scores = append(nodesScores[i].Scores, score.Scores...) nodesScores[i].TotalScore += score.TotalScore &#125; &#125; &#125; if klogV.Enabled() &#123; for i := range nodesScores &#123; klogV.InfoS(&quot;Calculated node&#x27;s final score for pod&quot;, &quot;pod&quot;, klog.KObj(pod), &quot;node&quot;, nodesScores[i].Name, &quot;score&quot;, nodesScores[i].TotalScore) &#125; &#125; return nodesScores, nil&#125; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112func prioritizeNodes( ctx context.Context, extenders []framework.Extender, fwk framework.Framework, state *framework.CycleState, pod *v1.Pod, nodes []*v1.Node,) ([]framework.NodePluginScores, error) &#123; // If no priority configs are provided, then all nodes will have a score of one. // This is required to generate the priority list in the required format if len(extenders) == 0 &amp;&amp; !fwk.HasScorePlugins() &#123; result := make([]framework.NodePluginScores, 0, len(nodes)) for i := range nodes &#123; result = append(result, framework.NodePluginScores&#123; Name: nodes[i].Name, TotalScore: 1, &#125;) &#125; return result, nil &#125; // Run PreScore plugins. preScoreStatus := fwk.RunPreScorePlugins(ctx, state, pod, nodes) if !preScoreStatus.IsSuccess() &#123; return nil, preScoreStatus.AsError() &#125; // Run the Score plugins. nodesScores, scoreStatus := fwk.RunScorePlugins(ctx, state, pod, nodes) if !scoreStatus.IsSuccess() &#123; return nil, scoreStatus.AsError() &#125; // Additional details logged at level 10 if enabled. klogV := klog.V(10) if klogV.Enabled() &#123; for _, nodeScore := range nodesScores &#123; for _, pluginScore := range nodeScore.Scores &#123; klogV.InfoS(&quot;Plugin scored node for pod&quot;, &quot;pod&quot;, klog.KObj(pod), &quot;plugin&quot;, pluginScore.Name, &quot;node&quot;, nodeScore.Name, &quot;score&quot;, pluginScore.Score) &#125; &#125; &#125; if len(extenders) != 0 &amp;&amp; nodes != nil &#123; // allNodeExtendersScores has all extenders scores for all nodes. // It is keyed with node name. allNodeExtendersScores := make(map[string]*framework.NodePluginScores, len(nodes)) var mu sync.Mutex var wg sync.WaitGroup for i := range extenders &#123; if !extenders[i].IsInterested(pod) &#123; continue &#125; wg.Add(1) go func(extIndex int) &#123; metrics.SchedulerGoroutines.WithLabelValues(metrics.PrioritizingExtender).Inc() metrics.Goroutines.WithLabelValues(metrics.PrioritizingExtender).Inc() defer func() &#123; metrics.SchedulerGoroutines.WithLabelValues(metrics.PrioritizingExtender).Dec() metrics.Goroutines.WithLabelValues(metrics.PrioritizingExtender).Dec() wg.Done() &#125;() prioritizedList, weight, err := extenders[extIndex].Prioritize(pod, nodes) if err != nil &#123; // Prioritization errors from extender can be ignored, let k8s/other extenders determine the priorities klog.V(5).InfoS(&quot;Failed to run extender&#x27;s priority function. No score given by this extender.&quot;, &quot;error&quot;, err, &quot;pod&quot;, klog.KObj(pod), &quot;extender&quot;, extenders[extIndex].Name()) return &#125; mu.Lock() defer mu.Unlock() for i := range *prioritizedList &#123; nodename := (*prioritizedList)[i].Host score := (*prioritizedList)[i].Score if klogV.Enabled() &#123; klogV.InfoS(&quot;Extender scored node for pod&quot;, &quot;pod&quot;, klog.KObj(pod), &quot;extender&quot;, extenders[extIndex].Name(), &quot;node&quot;, nodename, &quot;score&quot;, score) &#125; // MaxExtenderPriority may diverge from the max priority used in the scheduler and defined by MaxNodeScore, // therefore we need to scale the score returned by extenders to the score range used by the scheduler. finalscore := score * weight * (framework.MaxNodeScore / extenderv1.MaxExtenderPriority) if allNodeExtendersScores[nodename] == nil &#123; allNodeExtendersScores[nodename] = &amp;framework.NodePluginScores&#123; Name: nodename, Scores: make([]framework.PluginScore, 0, len(extenders)), &#125; &#125; allNodeExtendersScores[nodename].Scores = append(allNodeExtendersScores[nodename].Scores, framework.PluginScore&#123; Name: extenders[extIndex].Name(), Score: finalscore, &#125;) allNodeExtendersScores[nodename].TotalScore += finalscore &#125; &#125;(i) &#125; // wait for all go routines to finish wg.Wait() for i := range nodesScores &#123; if score, ok := allNodeExtendersScores[nodes[i].Name]; ok &#123; nodesScores[i].Scores = append(nodesScores[i].Scores, score.Scores...) nodesScores[i].TotalScore += score.TotalScore &#125; &#125; &#125; if klogV.Enabled() &#123; for i := range nodesScores &#123; klogV.InfoS(&quot;Calculated node&#x27;s final score for pod&quot;, &quot;pod&quot;, klog.KObj(pod), &quot;node&quot;, nodesScores[i].Name, &quot;score&quot;, nodesScores[i].TotalScore) &#125; &#125; return nodesScores, nil&#125; handleSchedulingFailure12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364func (sched *Scheduler) handleSchedulingFailure(ctx context.Context, fwk framework.Framework, podInfo *framework.QueuedPodInfo, status *framework.Status, nominatingInfo *framework.NominatingInfo, start time.Time) &#123; ... pod := podInfo.Pod err := status.AsError() errMsg := status.Message() if err == ErrNoNodesAvailable &#123; klog.V(2).InfoS(&quot;Unable to schedule pod; no nodes are registered to the cluster; waiting&quot;, &quot;pod&quot;, klog.KObj(pod), &quot;err&quot;, err) &#125; else if fitError, ok := err.(*framework.FitError); ok &#123; // Inject UnschedulablePlugins to PodInfo, which will be used later for moving Pods between queues efficiently. podInfo.UnschedulablePlugins = fitError.Diagnosis.UnschedulablePlugins klog.V(2).InfoS(&quot;Unable to schedule pod; no fit; waiting&quot;, &quot;pod&quot;, klog.KObj(pod), &quot;err&quot;, errMsg) &#125; else if apierrors.IsNotFound(err) &#123; // 错误是 NotFound 错误，可能是因为节点未找到，将会输出日志信息并等待，并尝试从调度器缓存中移除节点 klog.V(2).InfoS(&quot;Unable to schedule pod, possibly due to node not found; waiting&quot;, &quot;pod&quot;, klog.KObj(pod), &quot;err&quot;, errMsg) if errStatus, ok := err.(apierrors.APIStatus); ok &amp;&amp; errStatus.Status().Details.Kind == &quot;node&quot; &#123; nodeName := errStatus.Status().Details.Name // when node is not found, We do not remove the node right away. Trying again to get // the node and if the node is still not found, then remove it from the scheduler cache. _, err := fwk.ClientSet().CoreV1().Nodes().Get(context.TODO(), nodeName, metav1.GetOptions&#123;&#125;) if err != nil &amp;&amp; apierrors.IsNotFound(err) &#123; node := v1.Node&#123;ObjectMeta: metav1.ObjectMeta&#123;Name: nodeName&#125;&#125; if err := sched.Cache.RemoveNode(&amp;node); err != nil &#123; klog.V(4).InfoS(&quot;Node is not found; failed to remove it from the cache&quot;, &quot;node&quot;, node.Name) &#125; &#125; &#125; &#125; else &#123; klog.ErrorS(err, &quot;Error scheduling pod; retrying&quot;, &quot;pod&quot;, klog.KObj(pod)) &#125; // 检查 Pod 是否存在于 Informer 缓存中。如果 Pod 不存在于缓存中，将会输出日志信息 podLister := fwk.SharedInformerFactory().Core().V1().Pods().Lister() cachedPod, e := podLister.Pods(pod.Namespace).Get(pod.Name) if e != nil &#123; klog.InfoS(&quot;Pod doesn&#x27;t exist in informer cache&quot;, &quot;pod&quot;, klog.KObj(pod), &quot;err&quot;, e) &#125; else &#123; // 如果 Pod 存在于缓存中，首先检查缓存中的 Pod 是否已经被分配到节点上，如果是则会输出日志信息并不将其添加回队列中，否则会将缓存中的 Pod 进行深拷贝并添加到调度队列中 if len(cachedPod.Spec.NodeName) != 0 &#123; klog.InfoS(&quot;Pod has been assigned to node. Abort adding it back to queue.&quot;, &quot;pod&quot;, klog.KObj(pod), &quot;node&quot;, cachedPod.Spec.NodeName) &#125; else &#123; //由于 &lt;cachedPod&gt; 来自 SharedInformer，我们需要在这里做一个 DeepCopy()。 //忽略这个错误，因为 apiserver 没有正确验证关联项 //而且我们无法修复向后兼容性的验证。 podInfo.PodInfo, _ = framework.NewPodInfo(cachedPod.DeepCopy()) if err := sched.SchedulingQueue.AddUnschedulableIfNotPresent(podInfo, sched.SchedulingQueue.SchedulingCycle()); err != nil &#123; klog.ErrorS(err, &quot;Error occurred&quot;) &#125; &#125; &#125; ... // 更新调度状态为调度失败 if err := updatePod(ctx, sched.client, pod, &amp;v1.PodCondition&#123; Type: v1.PodScheduled, Status: v1.ConditionFalse, Reason: reason, Message: errMsg, &#125;, nominatingInfo); err != nil &#123; klog.ErrorS(err, &quot;Error updating pod&quot;, &quot;pod&quot;, klog.KObj(pod)) &#125;&#125; Run()代码位于cmd&#x2F;kube-scheduler&#x2F;server.go 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104// 通过给定的配置运行scheduler，只在上下文发生时返回错误或者在上下文终止时返回。func Run(ctx context.Context, cc *schedulerserverconfig.CompletedConfig, sched *scheduler.Scheduler) error &#123; logger := klog.FromContext(ctx) // 调试，输出版本信息 logger.Info(&quot;Starting Kubernetes Scheduler&quot;, &quot;version&quot;, version.Get()) logger.Info(&quot;Golang settings&quot;, &quot;GOGC&quot;, os.Getenv(&quot;GOGC&quot;), &quot;GOMAXPROCS&quot;, os.Getenv(&quot;GOMAXPROCS&quot;), &quot;GOTRACEBACK&quot;, os.Getenv(&quot;GOTRACEBACK&quot;)) // 创建 configz 实例，Configz 是一个用于暴露组件配置信息的工具 if cz, err := configz.New(&quot;componentconfig&quot;); err == nil &#123; cz.Set(cc.ComponentConfig) &#125; else &#123; return fmt.Errorf(&quot;unable to register configz: %s&quot;, err) &#125; // cc.EventBroadcaster.StartRecordingToSink(ctx.Done()) 用于启动事件广播器并将其配置为将事件记录到一个接收器中，接收器的生命周期与 ctx.Done() 关联，即在 ctx 完成时自动关闭 // 事件广播器用于将 Kubernetes 调度器产生的事件广播给外部监听器，例如日志记录器、监控系统等，以便进行事件处理和分析 cc.EventBroadcaster.StartRecordingToSink(ctx.Done()) defer cc.EventBroadcaster.Shutdown() // 设置用于健康检查的检查器 var checks []healthz.HealthChecker if cc.ComponentConfig.LeaderElection.LeaderElect &#123; checks = append(checks, cc.LeaderElection.WatchDog) &#125; // 判断当前调度器是否为领导者的函数 isLeader()。 // 首先，通过创建一个无缓冲的通道 waitingForLeader，用于等待领导者的信号。 // 然后，通过 select 语句来判断当前调度器是否为领导者 waitingForLeader := make(chan struct&#123;&#125;) isLeader := func() bool &#123; select &#123; case _, ok := &lt;-waitingForLeader: // 首先尝试从 waitingForLeader 通道接收数据，并将结果赋值给变量 ok。如果通道已关闭（ok 为 false），表示当前调度器是领导者，isLeader() 函数返回 true return !ok default: // waitingForLeader 通道上无法接收到数据（即通道中没有数据可接收），则 select 语句会立即执行 default 分支，表示当前调度器正在等待领导者的选举结果，isLeader() 函数返回 false return false &#125; &#125; // 在安全模式下启动一个 HTTP 服务器，并将指定的 handler 注册为处理请求的处理链 if cc.SecureServing != nil &#123; handler := buildHandlerChain(newHealthzAndMetricsHandler(&amp;cc.ComponentConfig, cc.InformerFactory, isLeader, checks...), cc.Authentication.Authenticator, cc.Authorization.Authorizer) // TODO: handle stoppedCh and listenerStoppedCh returned by c.SecureServing.Serve if _, _, err := cc.SecureServing.Serve(handler, 0, ctx.Done()); err != nil &#123; // fail early for secure handlers, removing the old error loop from above return fmt.Errorf(&quot;failed to start secure server: %v&quot;, err) &#125; &#125; // 启动 cc.InformerFactory 对象的 informer，以监听 Kubernetes API 资源的变化并触发事件 cc.InformerFactory.Start(ctx.Done()) // DynInformerFactory can be nil in tests. if cc.DynInformerFactory != nil &#123; cc.DynInformerFactory.Start(ctx.Done()) &#125; // 等待 InformerFactory 和 DynInformerFactory 对象的缓存同步完成 cc.InformerFactory.WaitForCacheSync(ctx.Done()) // DynInformerFactory can be nil in tests. if cc.DynInformerFactory != nil &#123; cc.DynInformerFactory.WaitForCacheSync(ctx.Done()) &#125; // 进行 LeaderElection 选举时，设置 LeaderElection 相关的回调函数、创建 LeaderElector 对象并启动选举 if cc.LeaderElection != nil &#123; cc.LeaderElection.Callbacks = leaderelection.LeaderCallbacks&#123; // 成功获得 Leader 锁并开始作为 Leader 运行时的回调函数，其中会关闭 waitingForLeader 通道，并启动调度器 sched OnStartedLeading: func(ctx context.Context) &#123; close(waitingForLeader) sched.Run(ctx) &#125;, // 失去 Leader 锁并停止作为 Leader 运行时的回调函数，其中会检查是否接收到取消信号 ctx.Done()，如果接收到则正常退出，否则输出错误日志并以非零退出码强制退出 OnStoppedLeading: func() &#123; select &#123; case &lt;-ctx.Done(): // We were asked to terminate. Exit 0. logger.Info(&quot;Requested to terminate, exiting&quot;) os.Exit(0) default: // We lost the lock. logger.Error(nil, &quot;Leaderelection lost&quot;) klog.FlushAndExit(klog.ExitFlushTimeout, 1) &#125; &#125;, &#125; // 启动 LeaderElection 选举过程，该方法会在后台启动 goroutine 进行选举，并且会阻塞当前 goroutine 直到选举结束 leaderElector, err := leaderelection.NewLeaderElector(*cc.LeaderElection) if err != nil &#123; return fmt.Errorf(&quot;couldn&#x27;t create leader elector: %v&quot;, err) &#125; leaderElector.Run(ctx) return fmt.Errorf(&quot;lost lease&quot;) &#125; // waitingForLeader 是一个 chan struct&#123;&#125; 类型的通道，用于在 LeaderElection 过程中的回调函数中进行信号通知。通过 close(waitingForLeader) 可以关闭这个通道，从而向其他等待该通道的 goroutine 发送一个关闭的信号 close(waitingForLeader) sched.Run(ctx) return fmt.Errorf(&quot;finished without leader elect&quot;)&#125; 根据代码注释，可以知道，他首先启动事件广播器，健康检查器，然后开启一个HTTP服务器，监听API事件变化，然后启动选举过程，最后启动Scheduler的主循环，处理调度。 总结 文档参考 kube-scheduler 源码阅读 ChatGpt","categories":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"http://jerryblogs.com/categories/Kubernetes/"},{"name":"源码分析","slug":"Kubernetes/源码分析","permalink":"http://jerryblogs.com/categories/Kubernetes/%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"}],"tags":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"http://jerryblogs.com/tags/Kubernetes/"}]},{"title":"kubernetes中如何进行调度","slug":"kubernetes中如何进行调度","date":"2023-04-22T10:55:53.000Z","updated":"2023-04-24T17:25:21.400Z","comments":true,"path":"2023/04/22/kubernetes中如何进行调度/","link":"","permalink":"http://jerryblogs.com/2023/04/22/kubernetes%E4%B8%AD%E5%A6%82%E4%BD%95%E8%BF%9B%E8%A1%8C%E8%B0%83%E5%BA%A6/","excerpt":"","text":"概述 K8s中是如何调度的？ K8s中调度的具体实现？ K8s如何调度PodPod的调度主要分为两个阶段，主要是调度周期和绑定周期，调度周期决定Pod被调度到哪一个节点，绑定周期将决策运用到集群。调度周期和绑定周期成为“调度上下文”。 调度周期 发现未被调度的Pod：调度器通过K8s的watch机制来发现集群中新创建且尚未被调度到节点的Pod。kube-scheduler位于集群控制面，是k8s的默认调度器，主要负责将一个pod调度到一个节点上。 选择节点：在选择节点时包含过滤和打分两个步骤来选择合适的节点进行调度。 在过滤阶段，调度器会选择一系列符合pod调度需求的节点 在打分阶段，调度器会对可调度节点队列中的每一个节点进行打分 最终pod会被调度到分数最高的节点上面 调度框架-调度器插件调度框架是k8s调度框架的一种插件架构，提供了一系列API插件来实现大部分的调度功能，这些调度插件被注册后在一个或多个拓展点被调用。 内置调度器插件可以启用或禁用，大部分默认调度器都默认启用。如果要启用调度器，可以修改配置文件： 123456apiVersion: kubescheduler.config.k8s.io/v1alpha1kind: KubeSchedulerConfigurationenablePlugins: PodTopologySpread: truedisablePlugins: NodeAffinity: true 默认配置文件地址为在/etc/kubernetes/scheduler.conf。 除了默认的插件，可以实现自己的插件，并和默认插件一起配置。你可以访问 scheduler-plugins 了解更多信息。 也可以通过KubeSchedulerConfiguration进行配置。 源码分析Scheduler结构体123456789101112131415161718192021222324252627// Scheduler监听新的Pod，试图找到适合的节点并且绑定写回api servertype Scheduler struct &#123; // 调度器缓存，通过对其修改可以观察到NodeLister和Algorithm的变化 Cache internalcache.Cache // 调度器使用的拓展器列表 Extenders []framework.Extender // 阻塞并等待下一个待调度的Pod，使用函数而不是通道的原因是，调度一个 Pod 可能需要一定的时间，而我们不希望 Pod 在通道中等待时变得过期 NextPod func() *framework.QueuedPodInfo // 调度失败时调用的处理函数 FailureHandler FailureHandlerFn // 尝试将给定的 Pod 调度到节点列表中的一个节点。如果成功，返回一个包含建议的主机名称的 ScheduleResult 结构体；否则，返回带有失败原因的 FitError SchedulePod func(ctx context.Context, fwk framework.Framework, state *framework.CycleState, pod *v1.Pod) (ScheduleResult, error) // 用于关闭调度器的通道 StopEverything &lt;-chan struct&#123;&#125; // 用于保存待调度的 Pod 的调度队列 SchedulingQueue internalqueue.SchedulingQueue // 调度配置文件（scheduling profiles）的映射 Profiles profile.Map // 用于与 Kubernetes API 服务器通信的客户端 client clientset.Interface // 节点信息的快照，用于存储节点的状态信息 nodeInfoSnapshot *internalcache.Snapshot // 评分节点的百分比 percentageOfNodesToScore int32 // 下一个开始节点的索引，用于决定从节点列表中的哪个节点开始调度 nextStartNodeIndex int&#125; Scheduler启动主循环Scheduler结构体定义了一个方法Run，它用来启动结构体的主循环，即启动调度器： 1234567891011// 开始监听和调度Unschedule状态的Pod，阻塞式的，直到上下文关闭func (sched *Scheduler) Run(ctx context.Context) &#123; sched.SchedulingQueue.Run() // 我们需要在专用的Goroutine中启动专用的schedulerOne循环，该方法会在函数获取下一项时被SchedulingQueue挂起。 // 如果没有新的Pod可以调度，将会被挂起，如果这个Goroutine中完成，将阻塞SchedulingQueue的关闭 go wait.UntilWithContext(ctx, sched.scheduleOne, 0) &lt;-ctx.Done() sched.SchedulingQueue.Close()&#125; wait.UntilWithContext函数在指定的上下文中循环执行给定的函数，ctx为传入的context，用来当ctx.Cancle或ctx.Done之后，会执行第10行的代码，即关闭SchedulingQueue。sched.scheduleOne为循环执行的函数，0即为执行一次。 我们来看下Scheduler.scheduleOne函数： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354// scheduleOne 对单个 pod 进行完整的调度工作流程，它在调度算法的主机拟合（host fitting）上进行串行化处理func (sched *Scheduler) scheduleOne(ctx context.Context) &#123; podInfo := sched.NextPod() // 当 schedulerQueue 被关闭时，pod 可能为 nil if podInfo == nil || podInfo.Pod == nil &#123; return &#125; pod := podInfo.Pod fwk, err := sched.frameworkForPod(pod) if err != nil &#123; // 这种情况不应该发生，因为我们只接受指定与配置文件中的调度器名称匹配的 pod 进行调度 klog.ErrorS(err, &quot;Error occurred&quot;) return &#125; if sched.skipPodSchedule(fwk, pod) &#123; return &#125; klog.V(3).InfoS(&quot;Attempting to schedule pod&quot;, &quot;pod&quot;, klog.KObj(pod)) // 同步地尝试为 pod 找到合适的节点。 start := time.Now() state := framework.NewCycleState() state.SetRecordPluginMetrics(rand.Intn(100) &lt; pluginMetricsSamplePercent) // 初始化一个空的 podsToActivate 结构，它将由插件填充或保持为空。 podsToActivate := framework.NewPodsToActivate() state.Write(framework.PodsToActivateKey, podsToActivate) schedulingCycleCtx, cancel := context.WithCancel(ctx) defer cancel() scheduleResult, assumedPodInfo, status := sched.schedulingCycle(schedulingCycleCtx, state, fwk, podInfo, start, podsToActivate) if !status.IsSuccess() &#123; sched.FailureHandler(schedulingCycleCtx, fwk, assumedPodInfo, status, scheduleResult.nominatingInfo, start) return &#125; // 异步地将 pod 绑定到其节点（我们可以这样做是因为上面的假设步骤）。 go func() &#123; bindingCycleCtx, cancel := context.WithCancel(ctx) defer cancel() metrics.SchedulerGoroutines.WithLabelValues(metrics.Binding).Inc() defer metrics.SchedulerGoroutines.WithLabelValues(metrics.Binding).Dec() metrics.Goroutines.WithLabelValues(metrics.Binding).Inc() defer metrics.Goroutines.WithLabelValues(metrics.Binding).Dec() status := sched.bindingCycle(bindingCycleCtx, state, fwk, scheduleResult, assumedPodInfo, start, podsToActivate) if !status.IsSuccess() &#123; sched.handleBindingCycleError(bindingCycleCtx, state, fwk, assumedPodInfo, start, scheduleResult, status) &#125; &#125;()&#125; host fitting，即主机拟合，是指将一个容器化的应用程序（通常是一个 Pod）调度到集群中的一个合适的节点（即主机）上运行的过程。 这个函数比较长，执行过程主要是： 1.阻塞式获取下一个Pod，当schedulerQueue被关闭时退出 2.sched.frameworkForPod方法用来获取Pod中指定的调度器Pod.Spec.SchedulerName用来指定Pod使用的调度器名称，默认是default。Kubernetes可以支持从头开始编写Scheduler，如果是自己实现的Scheduler，就可以在这个字段为Pod指定调度器。 3.调度是否能被跳过： 如果Pod正在被删除即跳过 如果某个 Pod 在调度队列中被取出进行调度尝试之前，收到了更新事件（如 Pod 的 Spec 发生了变化），那么该 Pod 将会被标记为假设状态，并不会进行实际的调度尝试，以避免重复调度。 4.概率性开启调度器插件统计指标的flag标志位 插件指标是指在 Kubernetes Scheduler 中使用的调度插件（例如调度算法、优先级函数等）生成的性能指标或统计信息。这些指标通常用于监控和度量调度器的性能、效果和健康状态，以便在需要时进行调优和故障排除。 在调度循环中，调度器可能会调用多个调度插件来评估节点的可用性、计算权重、比较优先级等。这些插件可能会在执行时生成各种指标，例如调度时间、节点利用率、调度决策等。记录这些插件指标可以帮助管理员和开发人员深入了解调度器的行为和性能，并进行优化和故障排除。 rand.Intn是 Go 语言标准库 math&#x2F;rand 包中的一个函数，用于生成一个指定范围内的随机整数 5.尝试调度单个Pod，如果调度失败，则调用FailureHandler函数进行处理 6.调度成功，开启一个新的Goroutine 使用Prometheus记录监控指标，metrics.SchedulerGoroutines 和 metrics。Goroutines，用于度量调度器的 Goroutine 数量 调用sched.bindingCycle将Pod绑定到节点上，绑定失败，则使用sched.handleBindingCycleError处理错误 Scheduler调度我们看到上述代码中sched.schedulingCycle用来调度Pod，我们看看代码： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104// schedulingCycle尝试调度单个Podfunc (sched *Scheduler) schedulingCycle( ctx context.Context, state *framework.CycleState, fwk framework.Framework, podInfo *framework.QueuedPodInfo, start time.Time, podsToActivate *framework.PodsToActivate,) (ScheduleResult, *framework.QueuedPodInfo, *framework.Status) &#123; pod := podInfo.Pod scheduleResult, err := sched.SchedulePod(ctx, fwk, state, pod) if err != nil &#123; if err == ErrNoNodesAvailable &#123; status := framework.NewStatus(framework.UnschedulableAndUnresolvable).WithError(err) return ScheduleResult&#123;nominatingInfo: clearNominatedNode&#125;, podInfo, status &#125; fitError, ok := err.(*framework.FitError) if !ok &#123; klog.ErrorS(err, &quot;Error selecting node for pod&quot;, &quot;pod&quot;, klog.KObj(pod)) return ScheduleResult&#123;nominatingInfo: clearNominatedNode&#125;, podInfo, framework.AsStatus(err) &#125; // SchedulePod() may have failed because the pod would not fit on any host, so we try to // preempt, with the expectation that the next time the pod is tried for scheduling it // will fit due to the preemption. It is also possible that a different pod will schedule // into the resources that were preempted, but this is harmless. if !fwk.HasPostFilterPlugins() &#123; klog.V(3).InfoS(&quot;No PostFilter plugins are registered, so no preemption will be performed&quot;) return ScheduleResult&#123;&#125;, podInfo, framework.NewStatus(framework.Unschedulable).WithError(err) &#125; // Run PostFilter plugins to attempt to make the pod schedulable in a future scheduling cycle. result, status := fwk.RunPostFilterPlugins(ctx, state, pod, fitError.Diagnosis.NodeToStatusMap) msg := status.Message() fitError.Diagnosis.PostFilterMsg = msg if status.Code() == framework.Error &#123; klog.ErrorS(nil, &quot;Status after running PostFilter plugins for pod&quot;, &quot;pod&quot;, klog.KObj(pod), &quot;status&quot;, msg) &#125; else &#123; klog.V(5).InfoS(&quot;Status after running PostFilter plugins for pod&quot;, &quot;pod&quot;, klog.KObj(pod), &quot;status&quot;, msg) &#125; var nominatingInfo *framework.NominatingInfo if result != nil &#123; nominatingInfo = result.NominatingInfo &#125; return ScheduleResult&#123;nominatingInfo: nominatingInfo&#125;, podInfo, framework.NewStatus(framework.Unschedulable).WithError(err) &#125; metrics.SchedulingAlgorithmLatency.Observe(metrics.SinceInSeconds(start)) // Tell the cache to assume that a pod now is running on a given node, even though it hasn&#x27;t been bound yet. // This allows us to keep scheduling without waiting on binding to occur. assumedPodInfo := podInfo.DeepCopy() assumedPod := assumedPodInfo.Pod // assume modifies `assumedPod` by setting NodeName=scheduleResult.SuggestedHost err = sched.assume(assumedPod, scheduleResult.SuggestedHost) if err != nil &#123; // This is most probably result of a BUG in retrying logic. // We report an error here so that pod scheduling can be retried. // This relies on the fact that Error will check if the pod has been bound // to a node and if so will not add it back to the unscheduled pods queue // (otherwise this would cause an infinite loop). return ScheduleResult&#123;nominatingInfo: clearNominatedNode&#125;, assumedPodInfo, framework.AsStatus(err) &#125; // Run the Reserve method of reserve plugins. if sts := fwk.RunReservePluginsReserve(ctx, state, assumedPod, scheduleResult.SuggestedHost); !sts.IsSuccess() &#123; // trigger un-reserve to clean up state associated with the reserved Pod fwk.RunReservePluginsUnreserve(ctx, state, assumedPod, scheduleResult.SuggestedHost) if forgetErr := sched.Cache.ForgetPod(assumedPod); forgetErr != nil &#123; klog.ErrorS(forgetErr, &quot;Scheduler cache ForgetPod failed&quot;) &#125; return ScheduleResult&#123;nominatingInfo: clearNominatedNode&#125;, assumedPodInfo, sts &#125; // Run &quot;permit&quot; plugins. runPermitStatus := fwk.RunPermitPlugins(ctx, state, assumedPod, scheduleResult.SuggestedHost) if !runPermitStatus.IsWait() &amp;&amp; !runPermitStatus.IsSuccess() &#123; // trigger un-reserve to clean up state associated with the reserved Pod fwk.RunReservePluginsUnreserve(ctx, state, assumedPod, scheduleResult.SuggestedHost) if forgetErr := sched.Cache.ForgetPod(assumedPod); forgetErr != nil &#123; klog.ErrorS(forgetErr, &quot;Scheduler cache ForgetPod failed&quot;) &#125; return ScheduleResult&#123;nominatingInfo: clearNominatedNode&#125;, assumedPodInfo, runPermitStatus &#125; // At the end of a successful scheduling cycle, pop and move up Pods if needed. if len(podsToActivate.Map) != 0 &#123; sched.SchedulingQueue.Activate(podsToActivate.Map) // Clear the entries after activation. podsToActivate.Map = make(map[string]*v1.Pod) &#125; return scheduleResult, assumedPodInfo, nil&#125; 依然这段代码比较长，但是总体内容比较简单： 1.使用sched.SchedulePod对Pod进行模拟调度，返回一个scheduleResult，记录了调度结果Node的信息。接下来是一系列错误判断： 没有可用的节点 断言为FitError类型的错误，即选择节点时出现了不符合条件的错误，没有可用节点 如果没有可用节点，且设置了PostFilterPlugins调度器插件，将实行抢占策略 运行RunPostFilter插件尝试使pod在未来可以被调度 2.记录metric指标，计算从开始时间 start 到当前时间的运行时延，并将其观测（Observe）到 metrics.SchedulingAlgorithmLatency 指标中 3.调用sched.assume告诉缓存假定 pod 现在正在给定节点上运行，即使它尚未绑定。这使我们可以继续进行调度，而无需等待绑定发生。 4.使用reserve插件的RunReservePluginsReserve方法，在pod实际绑定到节点之前，为Pod预留资源、标记节点或其他预留操作预留失败则调用RunReservePluginsUnreserve清理预留状态，然后用sched.Cache.ForgetPod(assumedPod) 方法将 assumedPod 从调度器的缓存中删除，以便后续的调度可以重新考虑这个Pod 5.接下来，调用了调度器中的许可插件的 RunPermitPlugins 方法。许可插件是调度器的一部分，用于检查是否允许将 Pod 绑定到某个节点 6.如果存在待激活的Pod，将其放入调度队列中，进行进一步的调度处理，并清空待激活的Pod列表 最后返回调度结果，Pod信息 “待激活”（Pending Activation）是指已经被调度器选择为将要在某个节点上运行的 Pod，但由于某些原因还没有被激活，即还没有被添加到调度队列中进行实际的调度处理。通常情况下，Pod 需要在被激活后才能被真正调度到节点上运行。 Scheduler绑定到节点123456789101112131415161718192021222324252627282930313233343536373839404142434445// bindingCycle tries to bind an assumed Pod.func (sched *Scheduler) bindingCycle( ctx context.Context, state *framework.CycleState, fwk framework.Framework, scheduleResult ScheduleResult, assumedPodInfo *framework.QueuedPodInfo, start time.Time, podsToActivate *framework.PodsToActivate) *framework.Status &#123; assumedPod := assumedPodInfo.Pod // Run &quot;permit&quot; plugins. if status := fwk.WaitOnPermit(ctx, assumedPod); !status.IsSuccess() &#123; return status &#125; // Run &quot;prebind&quot; plugins. if status := fwk.RunPreBindPlugins(ctx, state, assumedPod, scheduleResult.SuggestedHost); !status.IsSuccess() &#123; return status &#125; // Run &quot;bind&quot; plugins. if status := sched.bind(ctx, fwk, assumedPod, scheduleResult.SuggestedHost, state); !status.IsSuccess() &#123; return status &#125; // Calculating nodeResourceString can be heavy. Avoid it if klog verbosity is below 2. klog.V(2).InfoS(&quot;Successfully bound pod to node&quot;, &quot;pod&quot;, klog.KObj(assumedPod), &quot;node&quot;, scheduleResult.SuggestedHost, &quot;evaluatedNodes&quot;, scheduleResult.EvaluatedNodes, &quot;feasibleNodes&quot;, scheduleResult.FeasibleNodes) metrics.PodScheduled(fwk.ProfileName(), metrics.SinceInSeconds(start)) metrics.PodSchedulingAttempts.Observe(float64(assumedPodInfo.Attempts)) metrics.PodSchedulingDuration.WithLabelValues(getAttemptsLabel(assumedPodInfo)).Observe(metrics.SinceInSeconds(assumedPodInfo.InitialAttemptTimestamp)) // Run &quot;postbind&quot; plugins. fwk.RunPostBindPlugins(ctx, state, assumedPod, scheduleResult.SuggestedHost) // At the end of a successful binding cycle, move up Pods if needed. if len(podsToActivate.Map) != 0 &#123; sched.SchedulingQueue.Activate(podsToActivate.Map) // Unlike the logic in schedulingCycle(), we don&#x27;t bother deleting the entries // as `podsToActivate.Map` is no longer consumed. &#125; return nil&#125; 1.首先通过调用 fwk.WaitOnPermit(ctx, assumedPod) 方法等待 “permit” 插件的完成状态。 “permit” 插件通常用于在实际进行 Pod 调度之前进行进一步的验证和确认，例如检查节点资源是否满足要求、检查节点的亲和性和反亲和性等条件是否满足等。这些插件可以对 Pod 进行更细粒度的筛选和过滤，以确保 Pod 能够在合适的节点上运行。 2.执行 “prebind” 插件的逻辑，通过调用 fwk.RunPreBindPlugins(ctx, state, assumedPod, scheduleResult.SuggestedHost) 方法运行 “prebind” 插件。 “prebind” 插件通常用于在进行 Pod 节点绑定（binding）之前进行一些预处理操作，例如修改 Pod 的标签、注解、亲和性和反亲和性等信息，以及进行其他一些验证和准备工作。这些插件可以在节点绑定之前对 Pod 进行进一步的处理和调整，以确保 Pod 在节点上的调度和运行是合理和符合预期的。 3.调用 sched.bind(ctx, fwk, assumedPod, scheduleResult.SuggestedHost, state) 方法运行 “bind” 插件。 根据拓展器插件 &gt; 框架插件的优先级，调用extender.Bind方法，将Pod绑定到节点 “bind” 插件用于在进行 Pod 节点绑定（binding）时进行实际的节点绑定操作，将 Pod 绑定到指定的节点上，使其在该节点上运行。这些插件通常会处理节点资源的分配、Pod 节点状态的更新、调度事件的记录和报告等操作，以确保节点绑定的正确执行和状态的一致性。 4.执行 “postbind” 插件的逻辑，通过调用 fwk.RunPostBindPlugins(ctx, state, assumedPod, scheduleResult.SuggestedHost) 方法运行 “postbind” 插件。 “postbind” 插件用于在进行 Pod 节点绑定（binding）后进行一些后续的操作，例如更新节点状态、记录事件、发送通知等。这些插件通常会处理节点状态的更新、调度事件的记录和报告、通知相关组件等操作，以确保节点绑定后的一致性和完整性 Scheduler总结 参考文档 kubernetes调度器 调度框架 调度配置 scheduler-plugins调度器插件目录","categories":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"http://jerryblogs.com/categories/Kubernetes/"},{"name":"源码分析","slug":"Kubernetes/源码分析","permalink":"http://jerryblogs.com/categories/Kubernetes/%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"}],"tags":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"http://jerryblogs.com/tags/Kubernetes/"}]},{"title":"如何编译开发k8s","slug":"如何编译开发k8s","date":"2023-04-21T19:32:47.000Z","updated":"2023-04-21T20:54:28.584Z","comments":true,"path":"2023/04/22/如何编译开发k8s/","link":"","permalink":"http://jerryblogs.com/2023/04/22/%E5%A6%82%E4%BD%95%E7%BC%96%E8%AF%91%E5%BC%80%E5%8F%91k8s/","excerpt":"","text":"概述 如何获取代码？ 怎么跑起来（编译）？ 如何测试？ 目录结构是什么？ 代码地址kubernetes代码地址托管在Github上，代码地址为：https://github.com/kubernetes/kubernetes 如何编译主要有两种方式进行编译： 本地Go语言环境构建：12345mkdir -p $GOPATH/src/k8s.iocd $GOPATH/src/k8s.iogit clone https://github.com/kubernetes/kubernetescd kubernetesmake Docker构建，官方的Kubernetes构建采用此方式123 git clone https://github.com/kubernetes/kubernetes cd kubernetesmake quick-release 关于向官方提交代码和创建Issue的规范可以查看这篇文档。 Macos本地构建Kubernetes博主使用Macos M1进行本地构建。 安装GUN软件包首先安装编译所需的GUN软件包： 1find `brew --prefix`/opt -type d -follow -name gnubin -print 设置PATH到.zshrc: 12345678GNUBINS=&quot;$(find `brew --prefix`/opt -type d -follow -name gnubin -print)&quot;for bindir in $&#123;GNUBINS[@]&#125;do export PATH=$bindir:$PATHdoneexport PATH 安装etcd在本地安装etcd: 1brew install etcd 开始编译 支持按需编译所需的模块，通过make WHAT=cmd/&lt;subsystem&gt;的方式，比如：1make WHAT=cmd/kubectl 跨平台编译：12make crossmake cross KUBE_BUILD_PLATFORMS=windows/amd64 # 执行平台 编译成功后的产物将在_output/bin/kubectl。 我们选择编译所有的组件，使用make all在代码根目录下执行 需要 bash版本 &gt;&#x3D; 4.2，可以使用brew install bash来更新版本 执行完毕之后，可以在_output/bin看到如下输出： 123apiextensions-apiserver go-runner kube-controller-manager kube-scheduler kubectl-convert mountere2e.test kube-aggregator kube-log-runner kubeadm kubelet ncpuginkgo kube-apiserver kube-proxy kubectl kubemark 如何测试开发者提交的patch需要经过单元、集成和端到端测试并通过才能被合并。 提交前验证 运行以下命令进行提交代码前的一系列验证 1make verify 单元测试 针对一个特定的包进行测试： 1make test WHAT=./pkg/apis/core/helper GOFLAGS=-v 每次拉取请求都需要通过所有的单元测试，运行以下进行单元测试： 1make test 集成测试 运行集成测试需要以来etcd，可以运行以下执行集成测试： 1make test-integration 端到端测试 端到端测试需要花费很长时间，会启动，测试一个集群，最后删除一个集群： https://github.com/kubernetes/community/blob/master/contributors/devel/sig-testing/e2e-tests-kubetest2.md 目录结构├── api&#x2F; # Kubernetes API 定义│ ├── v1&#x2F; # Kubernetes API 版本 1│ ├── v1beta1&#x2F; # Kubernetes API 版本 1beta1│ └── …&#x2F;├── cmd&#x2F; # Kubernetes 命令行工具│ ├── kube-apiserver&#x2F; # Kubernetes API Server│ ├── kube-controller-manager&#x2F; # Kubernetes Controller Manager│ ├── kube-proxy&#x2F; # Kubernetes Proxy│ ├── kube-scheduler&#x2F; # Kubernetes Scheduler│ ├── kubeadm&#x2F; # Kubernetes 初始化工具│ ├── kubectl&#x2F; # Kubernetes 命令行客户端│ └── …&#x2F;├── pkg&#x2F; # Kubernetes 代码库│ ├── apis&#x2F; # Kubernetes API 实现│ ├── apiserver&#x2F; # Kubernetes API Server 实现│ ├── client&#x2F; # Kubernetes 客户端实现│ ├── controller&#x2F; # Kubernetes 控制器实现│ ├── kubelet&#x2F; # Kubernetes Kubelet 实现│ ├── proxy&#x2F; # Kubernetes Proxy 实现│ ├── scheduler&#x2F; # Kubernetes Scheduler 实现│ ├── util&#x2F; # Kubernetes 工具库│ └── …&#x2F;├── vendor&#x2F; # 第三方依赖├── Godeps&#x2F; # 过时的第三方依赖管理├── hack&#x2F; # 构建、测试、发布脚本├── docs&#x2F; # Kubernetes 文档├── test&#x2F; # 测试代码├── examples&#x2F; # Kubernetes 示例├── LICENSES&#x2F; # 第三方依赖许可证├── OWNERS # Kubernetes 代码库的所有者列表├── README.md # Kubernetes 代码库说明文档├── go.mod # Go 模块文件└── go.sum # Go 模块文件","categories":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"http://jerryblogs.com/categories/Kubernetes/"},{"name":"源码分析","slug":"Kubernetes/源码分析","permalink":"http://jerryblogs.com/categories/Kubernetes/%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"}],"tags":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"http://jerryblogs.com/tags/Kubernetes/"}]},{"title":"Go-Zero框架解析","slug":"Go-Zero框架解析","date":"2023-04-03T06:11:00.000Z","updated":"2023-04-20T18:06:21.544Z","comments":true,"path":"2023/04/03/Go-Zero框架解析/","link":"","permalink":"http://jerryblogs.com/2023/04/03/Go-Zero%E6%A1%86%E6%9E%B6%E8%A7%A3%E6%9E%90/","excerpt":"","text":"服务治理布隆过滤器关注的问题 布隆过滤器在微服务中能解决什么问题？ 布隆过滤器在go-zero中的实现 布隆过滤器在go-zero中如何使用 问题解答布隆过滤器在微服务中能解决什么问题？解决缓存穿透的问题。 什么是缓存穿透？ 缓存穿透是指一个缓存系统无法缓存某些查询请求，导致这些请求都需要从数据库或其他服务中获取数据，从而导致缓存系统的压力过大，甚至造成宕机。缓存穿透通常是由于恶意攻击或者查询条件错误等原因造成的。 微服务中为什么存在缓存穿透的问题？ 在微服务架构中，由于服务之间的调用频繁，缓存穿透的问题更加明显，会存在很多查询相同数据的情况。 布隆过滤器为什么能够解决这个问题？ 布隆过滤器是一种快速、高效的数据结构，可以用来判断一个元素是否存在于一个集合中。它可以使用非常小的空间来存储大量的元素，但是会存在一定的误判率。在缓存系统中，我们可以将每个查询条件都作为布隆过滤器中的一个元素，然后在查询时先使用布隆过滤器判断这个查询条件是否存在于布隆过滤器中，如果不存在则直接返回缓存未命中，避免了无效查询请求对缓存系统的影响。 布隆过滤器中为什么会存在误判率？ 布隆过滤器之所以存在误判率，是因为其使用的是一种基于哈希函数的概率算法，它只能判断元素是否“可能存在于集合中”，而不能100%确定元素是否存在于集合中。具体地说，当布隆过滤器将一个元素映射到多个位上时，可能会存在一种情况，即多个元素映射到的位上都已经被标记，从而导致这些元素被误判为存在于集合中。 误判率取决于什么？ 取决于布隆过滤器的大小和哈希函数的数量。 如何选择合适的误判率？ 一般来说，误判率越低，布隆过滤器所需要的空间和时间复杂度就越高。因此，在选择误判率时需要平衡误判率和布隆过滤器的性能。常见的误判率和建议使用的场景： 0.1%：适用于对数据准确性要求非常高的场景，如金融领域的交易系统； 1%：适用于大多数常规应用场景，如网站用户会话管理、垃圾邮件过滤等； 10%：适用于一些不那么关键的应用场景，如网站流量统计、爬虫去重等。 怎么检查一个布隆过滤器的误判率？ 准备测试数据集。测试数据集应该包含一些已知存在于集合中的元素和一些不存在于集合中的元素，可以从真实的数据集中提取，也可以生成一些随机数据。 使用布隆过滤器对测试数据集进行过滤。将测试数据集中的所有元素都添加到布隆过滤器中，然后再用测试数据集中的所有元素去查询布隆过滤器。如果查询到的元素在布隆过滤器中返回“存在”，否则返回“不存在”。 统计误判率。对于测试数据集中的所有不存在于集合中的元素，如果布隆过滤器返回了“存在”，则认为发生了误判。通过统计误判数和测试数据集中不存在元素的总数，可以计算出误判率。 布隆过滤器发生误判，会造成什么情况？ 误判率增加：布隆过滤器的误判率是在设计时确定的，如果误判率过高，则可能无法满足实际需求。当发生误判时，误判率会增加，这会降低布隆过滤器的准确性。 数据丢失：如果布隆过滤器用于缓存系统，当发生误判时，可能会导致一些缓存数据被丢失，从而影响系统的性能和正确性。 数据错误：如果布隆过滤器用于判断数据的存在性，当发生误判时，可能会导致数据的错误使用，从而影响系统的正确性。 布隆过滤器在go-zero中的实现1234567891011// 对元素进行hash 14次(const maps=14),每次都在元素后追加byte(0-13),然后进行hash.// 将locations[0-13] 进行取模,最终返回locations.func (f *BloomFilter) getLocations(data []byte) []uint &#123; locations := make([]uint, maps) for i := uint(0); i &lt; maps; i++ &#123; hashValue := hash.Hash(append(data, byte(i))) locations[i] = uint(hashValue % uint64(f.bits)) &#125; return locations&#125; 对元素进行hash 14次(const maps&#x3D;14),每次都在元素后追加byte(0-13),然后进行hash. 将locations[0-13] 进行取模,最终返回locations. 布隆过滤器在go-zero中如何使用12345678910111213141516// 初始化 redisBitSetstore := redis.New(&quot;redis 地址&quot;, func(r *redis.Redis) &#123; r.Type = redis.NodeType &#125;)// 声明一个bitSet, key=&quot;test_key&quot;名且bits是1024位bitSet := newRedisBitSet(store, &quot;test_key&quot;, 1024)// 判断第0位bit存不存在isSetBefore, err := bitSet.check([]uint&#123;0&#125;)// 对第512位设置为1err = bitSet.set([]uint&#123;512&#125;)// 3600秒后过期 err = bitSet.expire(3600)// 删除该bitSeterr = bitSet.del() 熔断的原理和实现关注的问题 熔断器在微服务领域可以解决什么问题？ 熔断器的原理 go-zero内置的熔断器如何实现 问题解答熔断器在微服务领域可以解决什么问题？在微服务中服务间依赖非常常见，比如评论服务依赖审核服务而审核服务又依赖反垃圾服务，当评论服务调用审核服务时，审核服务又调用反垃圾服务，而这时反垃圾服务超时了，由于审核服务依赖反垃圾服务，反垃圾服务超时导致审核服务逻辑一直等待，而这个时候评论服务又在一直调用审核服务，审核服务就有可能因为堆积了大量请求而导致服务宕机。 由此可见，在整个调用链中，中间的某一个环节出现异常就会引起上游调用服务出现一系列的问题，甚至导致整个调用链的服务都宕机，这是非常可怕的。因此一个服务作为调用方调用另一个服务时，为了防止被调用服务出现问题进而导致调用服务出现问题，所以调用服务需要进行自我保护，而保护的常用手段就是 熔断。 堆积大量请求为什么会导致服务宕机？ 资源耗尽：当服务接收到大量请求时，可能会占用过多的计算资源、内存资源或者网络带宽资源等，导致资源的快速耗尽。 线程阻塞：当服务接收到大量请求时，可能会导致线程池中的线程阻塞。如果阻塞的线程数量过多，那么就会导致新的请求无法被及时处理，从而导致服务宕机。 网络拥塞：当服务接收到大量请求时，可能会导致网络拥塞。如果网络拥塞严重，那么就会导致请求无法及时到达服务端，或者服务端无法及时响应请求，从而导致服务宕机 数据库连接池满：当服务接收到大量请求时，可能会占用过多的数据库连接资源。如果数据库连接池被占满，那么新的请求就无法获得数据库连接，从而导致服务无法响应请求，进而宕机。 熔断器的原理 熔断机制 指的是在发起服务调用的时候，如果被调用方返回的错误率超过一定的阈值，那么后续将不会真正地发起请求，而是在调用方直接返回错误 服务调用方为每一个调用服务(调用路径)维护一个状态机，在这个状态机中有三个状态： 关闭：在这种状态下，我们需要一个计数器来记录调用失败的次数和总的请求次数，如果在某个时间窗口内，失败的失败率达到预设的阈值，则切换到断开状态，此时开启一个超时时间，当到达该时间则切换到半关闭状态，该超时时间是给了系统一次机会来修正导致调用失败的错误，以回到正常的工作状态。在关闭状态下，调用错误是基于时间的，在特定的时间间隔内会重置，这能够防止偶然错误导致熔断器进入断开状态 打开：在该状态下，发起请求时会立即返回错误，一般会启动一个超时计时器，当计时器超时后，状态切换到半打开状态，也可以设置一个定时器，定期的探测服务是否恢复 半打开：在该状态下，允许应用程序一定数量的请求发往被调用服务，如果这些调用正常，那么可以认为被调用服务已经恢复正常，此时熔断器切换到关闭状态，同时需要重置计数。如果这部分仍有调用失败的情况，则认为被调用方仍然没有恢复，熔断器会切换到打开状态，然后重置计数器，半打开状态能够有效防止正在恢复中的服务被突然大量请求再次打垮 go-zero内置的熔断器如何实现？ zrpc根据Google sre过载保护算法，算法的原理如下： 请求数量：调用方发起的请求数量总和 请求接受数量：被调用方正常处理的请求数量 正常情况下，两个值相等，异常情况下，请求接受数量小于请求数量，当两者之间到达请求数量 &#x3D; K * 请求接受数量时，达到阈值，熔断器打开，新的请求在本地就会返回错误。概率的计算公式： 通过修改算法中K的值，可以调整熔断器的敏感度。 go-zero中对于上述的实现： 12345type googleBreaker struct &#123; k float64 // 倍值 默认1.5 stat *collection.RollingWindow // 滑动时间窗口，用来对请求失败和成功计数 proba *mathx.Proba // 动态概率&#125; 自适应熔断算法的实现： 123456789101112131415func (b *googleBreaker) accept() error &#123; accepts, total := b.history() // 请求接受数量和请求总量 weightedAccepts := b.k * float64(accepts) // 计算丢弃请求概率 dropRatio := math.Max(0, (float64(total-protection)-weightedAccepts)/float64(total+1)) if dropRatio &lt;= 0 &#123; return nil &#125; // 动态判断是否触发熔断 if b.proba.TrueOnProba(dropRatio) &#123; return ErrServiceUnavailable &#125; return nil&#125; 每次发起请求会调用doReq方法，在这个方法中首先通过accept效验是否触发熔断，acceptable用来判断哪些error会计入失败计数，定义如下： 12345678func Acceptable(err error) bool &#123; switch status.Code(err) &#123; case codes.DeadlineExceeded, codes.Internal, codes.Unavailable, codes.DataLoss: // 异常请求错误 return false default: return true &#125;&#125; 如果请求正常则通过markSuccess把请求数量和请求接受数量都加一，如果请求不正常则只有请求数量会加一 1234567891011121314151617181920212223242526272829303132func (b *googleBreaker) doReq(req func() error, fallback func(err error) error, acceptable Acceptable) error &#123; // 判断是否触发熔断 if err := b.accept(); err != nil &#123; if fallback != nil &#123; return fallback(err) &#125; else &#123; return err &#125; &#125; defer func() &#123; if e := recover(); e != nil &#123; b.markFailure() panic(e) &#125; &#125;() // 执行真正的调用 err := req() // 正常请求计数 if acceptable(err) &#123; b.markSuccess() &#125; else &#123; // 异常请求计数 b.markFailure() &#125; return err&#125;watch cicd is_cloud=True image_repo=registry.cn-beijing.aliyuncs.com d8da67f5-d160-433f-b258-99d4e50e9687 | deployByScripts env=dfs-tm-java profile=grayscale image_repo=registry.cn-beijing.aliyuncs.com % deployByScripts env=dfs-console profile=grayscale image_repo=registry.cn-beijing.aliyuncs.com % deployByScripts env=dfs-tcm profile=grayscale image_repo=registry.cn-beijing.aliyuncs.comwatch cicd is_cloud=True image_repo=registry.cn-beijing.aliyuncs.com 145854b6-6446-4e54-8726-ff74bc876401 原理介绍TimingWheelTimingWheel（时间轮）是一种计时器算法，常用于高并发场景中的延时任务调度，如定时任务、超时控制等。它通过将时间轴划分为若干个时刻，并将任务放入对应的时间轮槽中，实现快速的任务检索和触发。 TimingWheel 的核心是一个环形数组，其中每个元素代表一个时间槽，每个时间槽存放着需要在该时刻执行的任务。时间轮的基本单位是时间槽，每个时间槽的时间间隔是相等的，也就是说，整个时间轮可以被看做是一个周期为 $N$ 的定时器。 当有一个任务需要被添加到时间轮中时，首先计算它需要经过多少个时间槽才会被触发。例如，如果任务需要在 5 秒后被触发，而每个时间槽的时间间隔为 1 秒，则该任务需要经过 5 个时间槽才会被触发。接下来，将该任务插入到距离当前时间 $N$ 个时间槽之后的时间槽中，如果插入的位置超过了时间轮的尺寸，则需要对时间轮进行一次轮转。 在每个时间轮的槽中，可以存放多个任务。因此，当时间轮中的一个时间槽被触发时，需要遍历该时间槽中的所有任务，并将它们取出并执行。 TimingWheel 算法的优点是，在时间轮的基础上，可以支持很多高级功能，如分层时间轮、动态调整时间轮的精度、延迟队列等，可以满足不同场景下的需求。但是也存在一些缺点，例如时间轮的精度不高、任务的最大延迟时间有限等。","categories":[{"name":"Go","slug":"Go","permalink":"http://jerryblogs.com/categories/Go/"},{"name":"Go-Zero","slug":"Go/Go-Zero","permalink":"http://jerryblogs.com/categories/Go/Go-Zero/"}],"tags":[{"name":"Go","slug":"Go","permalink":"http://jerryblogs.com/tags/Go/"}]},{"title":"Grafana","slug":"Grafana","date":"2023-04-02T17:58:54.000Z","updated":"2023-04-20T18:01:33.904Z","comments":true,"path":"2023/04/03/Grafana/","link":"","permalink":"http://jerryblogs.com/2023/04/03/Grafana/","excerpt":"","text":"Grafana 是什么？ 怎么做到的？ 怎么用？ 1. 是什么？对指标、日志和metrics的查询，可视化和告警。 包括sms&#x2F;email等多种告警方式，可以通过钩子自定义告警规则和告警方式 支持dashboard模版，方便重用可视化模版 支持dashboard插件 支持 LDAP and OAuth认证方式 商业版和云版包括社区版没有的功能以及7 * 24 * 365全天候的grafana核心技术团队的支持。 2.怎么用？2.1 如何安装最小推荐规格255MB和1CPU。更多的功能需要更多的CPU和内存，需要更多资源的功能包括： 图像的服务端渲染 告警 数据源代理 需要数据库来保存配置、用户数据等，支持的数据库列表： sqlite3 mysql 5.7+ postgresql 10+ 默认情况下，在grafana安装时会和sqlite一起安装，并使用grafana。 Ubuntu&#x2F;Debian1234567sudo apt-get install -y apt-transport-httpssudo apt-get install -y software-properties-common wgetsudo wget -q -O /usr/share/keyrings/grafana.key https://apt.grafana.com/gpg.keyecho &quot;deb [signed-by=/usr/share/keyrings/grafana.key] https://apt.grafana.com stable main&quot; | sudo tee -a /etc/apt/sources.list.d/grafana.listecho &quot;deb [signed-by=/usr/share/keyrings/grafana.key] https://apt.grafana.com beta main&quot; | sudo tee -a /etc/apt/sources.list.d/grafana.listsudo apt-get updatesudo apt-get install grafana 通过Docker启动1docker run -d -p 3000:3000 --name grafana grafana/grafana-oss 2.2 启动服务通过systemd1234sudo systemctl daemon-reloadsudo systemctl start grafana-serversudo systemctl status grafana-serversudo systemctl enable grafana-server.service 通过init.d123sudo service grafana-server startsudo service grafana-server statussudo update-rc.d grafana-server defaults 二进制文件1./bin/grafana-server web 2.3 配置 配置文件地址 &#x2F;etc&#x2F;grafana&#x2F;grafana.ini 可以通过环境变量配置 通过；进行注释","categories":[{"name":"运维开发","slug":"运维开发","permalink":"http://jerryblogs.com/categories/%E8%BF%90%E7%BB%B4%E5%BC%80%E5%8F%91/"}],"tags":[{"name":"运维开发","slug":"运维开发","permalink":"http://jerryblogs.com/tags/%E8%BF%90%E7%BB%B4%E5%BC%80%E5%8F%91/"}]},{"title":"PEP-703","slug":"PEP-703","date":"2023-03-15T15:01:26.000Z","updated":"2023-04-20T18:05:48.691Z","comments":true,"path":"2023/03/15/PEP-703/","link":"","permalink":"http://jerryblogs.com/2023/03/15/PEP-703/","excerpt":"","text":"PEP703中讲到的nogilPEP703由nogil分支作者编写，其中描述了 biased reference counting（偏向引用计数），描述了引用计数的设计方式，该设计方式避免同一线程原子化对饮用计数的读写 本地引用计数(“local” reference count)：本线程所拥有的引用计数 共享引用计数(“shared” reference count)：其他线程使用原子化来访问该引用计数 interned string，小整数，静态分配的Python对象，True，False和None对象的本地引用计数将被置为-1，线程ID将被置为unsigned equivalent(UINTPTR_MAX)，保证这些对象不会被回收，一个比较常见的设计。 1234567struct _object &#123; _PyObject_HEAD_EXTRA uintptr_t ob_tid; Py_ssize_t ob_ref_local; Py_ssize_t ob_ref_shared; PyTypeObject *ob_type;&#125;; 描述了PyObject的对象。 内存分配：Python使用pymalloc，需要在gil锁之下保证线程安全；作者建议使用性能良好的线程安全分配器mimalloc。 禁用free list 使用stop-the-world保证线程安全 消除分代垃圾回收使用非分代垃圾回收 延迟引用计数和偏向引用计数集成 对列表，字典集合等对象的修改必须持有锁","categories":[{"name":"Python","slug":"Python","permalink":"http://jerryblogs.com/categories/Python/"},{"name":"Python PEP 合集","slug":"Python/Python-PEP-合集","permalink":"http://jerryblogs.com/categories/Python/Python-PEP-%E5%90%88%E9%9B%86/"}],"tags":[{"name":"Python","slug":"Python","permalink":"http://jerryblogs.com/tags/Python/"}]},{"title":"Github-Action-经验分享","slug":"Github-Action-经验分享","date":"2022-07-23T17:49:00.000Z","updated":"2023-04-20T17:54:49.945Z","comments":true,"path":"2022/07/24/Github-Action-经验分享/","link":"","permalink":"http://jerryblogs.com/2022/07/24/Github-Action-%E7%BB%8F%E9%AA%8C%E5%88%86%E4%BA%AB/","excerpt":"","text":"Github Action介绍Github Action是github提供的一个CICD平台，可以为托管在其上的项目提供持续集成和持续交付的能力，可用于自动构建、测试和部署。Github Action对于公有仓库是免费的，对于私有仓库用户提供了一定的免费额度： 对于运行在Linux上的作业，github提供了2000分钟的免费额度； Github提供了10G免费的缓存，并且会对7天内未被访问的key进行删除； 以上的使用限额，会在每个月进行重置； 关于Github更多的计费策略，可以点击跳转进行了解。对于存储和运行时间不敏感的项目，Github Action的免费额度其实是能够覆盖到的，即使是超额，Github的超额计费也是比较便宜的。1000分钟的运行时间只需要8美元，对于托管在Github私有仓库的中小公司来说可以说是相当合算的选择。Github 提供的2核7G的机器对于一般的项目来说也够用了。 为什么要用Github Action项目背景项目使用Java语言开发，用maven管理项目依赖。原先使用的方式是将代码托管于coding，自建的Jenkins服务将从coding拉取代码进行打包，再将制品放到k8s运行。Jenkins从coding拉取代码的速度是很慢的，往往需要将近10分钟才能将代码拉下来，再加上打包和上传镜像的时间，整个流程需要跑将近30分钟。而且如果打包失败，还需要重来，所以往往会出现整整一天才能把包打出来部署到测试环境的情况。项目存在开源版本，其中部分模块托管于Github开源仓库，部分模块是闭源。为了避免维护两份代码，将整个项目拆为3个仓库，分别是开源部分、闭源后端仓库和前端闭源仓库。如果仍然沿用Jenkins必然增加了不少的复杂度和运行时间。 成本现在我们线上的Jenkins服务经常会有莫名其妙的问题，如果沿用目前的方式，要得到一个稳定的服务，需要一个专业的运维人员维护整个的打包环境。目前github action提供的环境比较稳定，这么一段时间测试下来，基本上报错就是流程或者代码有问题，不会出现莫名其妙的Jenkins问题。 速度目前来讲，使用Github Action进行打包的时间是比较稳定的，基本稳定在14分钟左右。Github Action提供了缓存依赖的功能，能够自动的缓存node、java、python等各种依赖。在使用Jenkins之前前端原本需要2分钟的依赖安装，使用了缓存依赖仅需要7秒钟左右就能加载所有的依赖。对于代码拉取方面，由于目前闭源代码和开源的代码均托管在github，所以基本上三个代码仓库的拉取时间在7秒钟左右。相对于之前10分钟的拉取速度来讲，是一个极大的优化。对于加快构建，我们做了不少的努力，比如说将需要安装的比较大的依赖打包成docker镜像，以私有的方式放到Github Package，当构建镜像时自动拉取镜像，8秒钟左右容器就能跑起来用于构建。开源社区提供了丰富的插件，很多需要的功能不需要自己造轮子，在社区就能找到相应的插件。可以减少不少维护成本和调试速度。如果担心安全性问题，可以使用Github官方认证过的插件，某种程度上可以避免使用到”留后门“的插件。对于在构建期间没有依赖的模块，可以进行并发构建，可以提高不少时间。不过如果组件之间存在相互依赖关系还是不要并发构建了，强行并发会增加系统的复杂度。对于部署，Github上对于不同的云商，比如国内的阿里云腾讯云，国外的亚马逊谷歌云提供了不少轮子和模板，可以很方便的使用。另外，对于运行器有要求的作业，Github还提供了多平台支持，支持使用Linux、Windows和Macos运行器，不需要自己维护多套环境。 权限管理担心仓库被有心之人开源？这个其实不需要担心，只有owner有对仓库设置的权限，开发者对于仓库，对于制品（Github Package）是没有权限进行设置的。只要老板不手滑…对于账单计费方面，Github也提供了一个结算账号的角色，组织内的开发者是没有权限查看账单和使用额度的，仅有owner和结算账号能够查看账单和使用额度。 踩坑以及解决建议多仓库依赖如果打包过程需要依赖多个仓库，进行构建的仓库并不能很好的感知到被依赖仓库的变化。这种情况有两种解决方式： 使用重用工作流程使用rest api 远程调试在进行用Github配置cicd的过程中，还是踩了不少的坑。对于每次修改yml文件，就需要上传一次构建进行远程调试，这样的过程比较麻烦而且占用计费时间。这种情况下act是一个非常好用的工具，跳转到act仓库链接。 调试过程太麻烦我们在进行构建的过程中，由于一个依赖没有处理好，github action并不能很好的进行调试，我们一次次提交修改一次次看报错信息，导致团队搞了好几个小时都没发现问题所在。这个问题，这里推荐tmate插件，可以在运行器起一个tmate服务，在本地通过ssh连接上运行器查看相关信息，再进行调试会方便很多，这里依然提供快速跳转。相信有了上述两个工具，能够极大提升你的调试体验。如果你有更好的工具，请告诉我，不要藏私。","categories":[{"name":"运维开发","slug":"运维开发","permalink":"http://jerryblogs.com/categories/%E8%BF%90%E7%BB%B4%E5%BC%80%E5%8F%91/"}],"tags":[{"name":"运维开发","slug":"运维开发","permalink":"http://jerryblogs.com/tags/%E8%BF%90%E7%BB%B4%E5%BC%80%E5%8F%91/"}]},{"title":"用于弹性容器编排的Kubernetes中的水平Pod自动扩缩容","slug":"k8s-pod-scale","date":"2022-05-26T11:31:22.000Z","updated":"2023-04-21T20:56:27.227Z","comments":true,"path":"2022/05/26/k8s-pod-scale/","link":"","permalink":"http://jerryblogs.com/2022/05/26/k8s-pod-scale/","excerpt":"","text":"文章信息作者：Thanh-Tung Nguyen , Yu-Jin Yeom 1 , Taehong Kim 1,* , Dae-Heon Park 2,* and Sehan Kim原文：https://pdfs.semanticscholar.org/a6e5/53a4f59d737d802cebdce7e114c5d62f728c.pdf 摘要Kubernetes是一个开源的容器编排平台，通过自动扩缩容机制（水平、垂直和集群自动扩缩容）实现高可用性和可拓展性。其中HPA通过动态拓展和所见资源单元（Pod）的数量来帮助提供不间断的服务，而无需通过重新启动整个系统。Kubernetes监控默认的资源指标，包括主机及其Pod的CPU和内存使用情况。另一方面，有Prometheus等外部软件提供的自定义指标是可定制的，可以监控各种指标。在本文中，我们通过各种实验来研究HPA，以提供有关操作行为的关键知识。我们还讨论了Kubernetes资源指标（KRM）和Prometheus自定义指标（PCM）之间的本质区别，以及他们如何影响HPA的性能。最后，我们为将来使用Kubernetes的研究人员，开发人员和系统管理员提供了有关如何优化HPA性能的更深入的见解和经验教训。 前言近年来，随着云计算和后期边缘计算的快速出现，虚拟化技术已经成为学术研究和工业发展的轰动话题，因为他们使用Amazon Web Services、GCP、Microsoft Azure等云平台能实现大规模的弹性。新兴的虚拟化技术之一是容器化技术，其中配备随时可部署的应用程序组件的轻量级操作系统被打包到一个自给自足的容器中，准备在支持多租户的主机上运行。在主机系统中，不同的容器在同一主机操作系统和同一内核中一起运行，这有助于降低存储要求，并允许他们主机操作系统相比实现接近本机的性能。 由于容器可以大规模部署，因此对在部署、拓展和管理方面高度自动化的容器编排平台有着巨大的需求。在各种编排平台中，包括Docker Swarm、Amazon Elastic Container Service、Red Hat OpenShift Container Platform，Kubernetes已成为其受欢迎的事实上的标准。他是一个开源平台，可以轻松打包和运行容器化应用程序、工作负载和服务，并提供用于操作可拓展分部署系统的框架，此外，容器化应用程序具有在任何类型的操作系统和云基础架构上运行的可移植性。Kubernetes使用Docker作为基础环境来运行可以指且自给自足的容器，其实例化称为Docker镜像，它提供了一个控制平面，用于管理和安排这些容器在其主机集群（节点）上运行，具体取决于其可用资源和每个容器的特定要求。 在Kubernetes中，最重要的功能之一是自动伸缩，因为它允许容器化应用程序和服务在没有人干预的情况下弹性运行。Kubernetes提供了三种类型的自动伸缩器： HPA，根据各种要求调整执行和资源单元（Pod）的数量来支持高可用性。触发后HPA会出创建新的Pod来共享工作负载，而不会影响当前在集群内运行的现有Pod。 VPA，直接更改Pod的规格，例如请求的资源，并维护工作Pod的数量。因此，他需要重新启动这些Pod，从而中断应用程序和服务的连续性。 集群自动伸缩（CA），增加了不再能够在现有POD上调度POD的节点数。目前CA仅适用于GCP和AWS等商业云平台。 为了支持自动扩缩容，Kubernetes会监控Pod、应用程序、主机和集群统计信息（指标）。当这些指标达到特定阈值时，将自动触发扩缩容程序。虽然Kubernetes默认提供资源指标，但是他们的监控目标仅限于Pod和主机的CPU和内存的使用情况。因此，可以在外部软件的帮助下添加可定制（自定义）指标，提高HPA的性能和灵活性。在本文中，我们研究了Prometheus提供的自定义指标，Prometheus是由云原生计算基金会运行的开源项目。 已经有几项工作旨在提高Kubernetes自动扩缩容的性能。 例如，参考文献15，16中提出了改善CA和VPA的技术，参考文献17，18中则侧重于提高HPA和资源检测的性能。然而他们还没有深入基础知识。例如，需要解决诸如“Kubernetes HPA如何对不同的类型做出反应？”、“不同指标抓取周期对Kubernetes有何影响？”或“仅监视CPU和内存使用率是否满足HPA？”之类的问题。此外，有关Kubernetes及其自动扩缩器的文档可以在官方文档、网站和互联网的其他几个来源上找到。但是他们是从功能角度编写的，或者只是提供有关如何安装和运行Kubernetes的教程。对Kubernetes的运营行为缺乏全面的基本分析。因此，在本文中，我们专注于HPA，并寻求通过以下贡献来提高关于有关该主题的知识： 首先，我们通过测试平台上的各种实现，从拓展趋势，指标搜集，请求处理，集群大小，抓取时间和延迟等方面评估HPA。我们对结果的全面分析提供了官方网站和其他来源所没有的知识和见解。 其次，除了Kubernetes的默认资源指标外，我们还使用Prometheus Custom Metric评估HPA。通过了解这两种指标之间的差异，读者可以更牢固掌握HPA的运行行为。 最后，我们提供从实验和分析中获得的实践经验。它们可以作为基础知识，以便研究人员，开发人员和系统管理员可以做出明智的决策，以优化HPA的性能以及Kubernetes集群中的服务质量。 本文的其余部分组织如下。第 2 部分讨论了有关 Kubernetes 和自扩展研究的现有文献。第 3 部分分析了 Kubernetes 的体系结构，而第 4 部分则彻底讨论了 HPA、不同的指标以及 Reiness Probe 的使用，这有助于读者在进行性能分析之前更好地了解 Kubernetes 和 HPA。第5节从各种实验场景的结果中得出了有关HPA性能的经验教训。最后，第6节总结了本文。 相关工作Kubernetes最初由Google开发，后来转移到CNCF [14]，作为在云数据中心高效部署，管理和扩展容器化应用程序的解决方案。但是，由于它是一个开源项目，因此可以配置和修改Kubernetes作为坚实的基础，可以在其上构建和开发满足特定需求的其他平台。Reference [19] 的作者认为，当前版本的 Kubernetes 调度程序只考虑了可虚拟化的物理基础设施，包括 CPU 和内存使用率，这才有逻辑意义。但是，从公司的角度来看，为了提高数据中心的效率，还需要考虑其他条件，例如地理位置，电力基础设施和业务流程。因此，作者提出了一种名为Edgetic的增强调度程序，它可以在性能和功耗方面预测Pod的最佳位置。 在参考文献[15]中，瑟古德和列侬讨论了许多场景，包括智能家居环境，其中有大量的输入设备，而家庭成员等用户数量经常波动。这就意味着当所有现有节点都繁忙时，需要 HPA 和更高版本的 CA。然而，目前，默认的Kubernetes CA仅由GCP [2]等云平台提供商提供，因此他们为Kubernetes提出了一种称为自由&#x2F;开源软件（FOSS）的弹性CA解决方案。此解决方案采用 VMware ESXi 主机作为节点和虚拟机工具，包括用于 CA 操作的 vCenter、Foreman。具体而言，当任何节点达到 CPU 或内存阈值时，vCenter 服务器都会创建虚拟机警报，然后执行 bash 脚本，通过 Foreman 创建新的虚拟机节点。 在参考文献[16]中，作者提出了一种名为基于资源利用率的自动缩放系统（RUBAS）的无中断VPA解决方案，其中包含容器迁移。他们认为，资源可能被高估，导致利用率低下。因此，RUBAS 计算 VPA 的实际所需资源。此外，作者试图通过使用用户空间中的检查点还原（CRIU）创建检查点映像来解决必须在VPA中重新启动容器和容器的问题。Rossi [20]提出了一种用于水平和垂直自动缩放的强化学习模型。它旨在确保应用程序所需的响应时间。Reference [21]的作者开发了一种混合自适应自动缩放器Libra。它还考虑了采用传统HPA的应用的最佳资源分配。Libra本质上是VPA和HPA的控制回路。在第一阶段，Libra 使用金丝雀应用程序计算适当的 CPU 限制，并根据此新的 CPU 限制调整生产应用程序的 Pod 数量。之后，如果负载达到极限，则重复循环。 在参考文献[22，23]中，作者认为Kubernetes目前正在使用通过cAdvisor从&#x2F;cgroup虚拟文件系统收集的相对指标。这些指标可能与处理器中的实际 CPU 使用率不同，后者可以从 &#x2F;proc 文件系统中收集。这种差异可能导致低估所需资源。因此，作者提出了CPU密集型应用程序的相对指标和绝对指标之间的相关模型，用于纠正Kubernetes收集的相对指标，以提高HPA的性能。考虑到类似的目标，参考文献[24]的作者提出了几个影响因素，例如保守常数，它实际上为度量波动创造了一个缓冲区。仅当指标值超出此区域时，才会执行 HPA 操作。另一个因素是连续缩放操作之间的适应间隔。这样可以减少指标值波动时不必要的缩放。 在参考文献 [25，26] 中，作者将 Kubernetes 应用于容器化雾计算应用程序的资源配置中。提出一种网络感知调度算法，该算法考虑了节点的CPU和RAM容量、设备类型和地理位置等网络基础设施，以做出配置决策。例如，此算法在调度时间关键型应用程序的实例时可以考虑往返延迟。另一个基于 Kubernetes 的雾计算平台，用于管理地理上分散的容器，在参考文献 [27] 中提出。在本文中，作者设计了一种名为Autoscaling Broker（AS Broker）的服务，以获取原始指标并根据CPU和内存使用情况计算HPA的最佳副本数，同时减少应用程序的响应时间。在参考文献 [12] 中，Chang 等人介绍了一个基于 Kubernetes 的云监控平台，该平台提供基于资源利用率和应用程序 QoS 指标的动态资源配置算法。在此平台上，使用Heapster v.0.19.1 [28]，InfluxDB v.0.9.4.1 [29]和Grafana v.2.1.0 [30]收集并显示资源指标，而应用程序的响应时间则使用Apache JMeter计算[31]。此数据被聚合并输入到一个配置算法中，该算法实质上是计算和获取 Pod 的数量。Jin-Gang等人在参考文献中提出了一种用于统一通信服务器的预测HPA算法[18]。该算法的反应式扩展部分与 Kubernetes 的当前算法相同。另一方面，该算法还采用自动回归集成和移动平均（ARIMA）模型来预测未来的工作负载，或HTTP请求的数量[32]来触发HPA和升级。 Kubernetes HPA的另一个用例是在API网关系统上，如参考文献[17]中所述。该系统旨在简化与后端服务的内部连接。由于前端和后端服务的 Pod 在必要时都会进行水平自动缩放，因此它们的互连也会显著增加，从而导致对网关系统进行扩展的需求。在这项工作中，作者使用Prometheus自定义指标进行HPA操作。但是，没有提到哪些指标以及如何使用它们。以同样的方式，Dickel，Podolskiy和Gerndt [33]建议在有状态物联网网关上应用Kubernetes HPA。虽然无状态应用程序（如HTTP）可以很容易地水平扩展，但有状态的应用程序（包括WebSocket和MQTT）需要更多的关注。例如，在 HPA 之后，群集中将有多个网关实例。在以信息为中心的 IoT 网络上的发布-订阅模型中，客户端（订户）和服务器（发布者）需要通过同一网关进行连接。因此，作者使用WebSocket和MQTT协议为物联网网关设计了一个框架，专注于在客户端和服务器之间建立和监控活动连接。该论文还提到利用Prometheus Operator的HPA自定义指标[34]。但是，与前面提到的工作类似，它没有提供有关如何收集，计算和获取这些指标的具体信息。 值得注意的是，虽然上述大多数作品都研究Kubernetes及其功能HPA，但它们都未能详细描述HPA的工作原理及其在与不同类型的指标一起使用或在各种扩展配置（如抓取时间）下使用时的行为。这对于在 Kubernetes 中高效开发和管理容器化应用程序非常重要。因此，在本文中，我们首先讨论Kubernetes的架构，其组件及其内部通信，以建立坚实的主题基础，这将有助于读者牢牢掌握HPA相关概念，例如收集不同类型指标的方法，这将在随后进行解释。据我们所知，我们的论文是第一个完成这一任务的论文。最后，我们在各种场景中进行了严格的实验，以评估和分析HPA性能的各个方面。基于分析，我们就如何优化 Kubernetes HPA 提供深入的见解和建议，以帮助研究人员、开发人员和系统管理员做出明智的决策。 Kubernetes 的架构在本节中，我们首先介绍 Kubernetes 集群的架构 — 主要组件及其在集群内的相互通信。然后，我们仔细研究 Kubernetes 的服务如何使当前在集群内的 pod 上运行的应用程序能够作为网络服务工作。 Kubernetes集群如图 1a 所示，每个 Kubernetes 集群至少由一个主节点和几个工作节点组成。在实践中，可以有一个具有多个主节点的集群 [11]，通过复制主节点来确保集群的高可用性，因此在其中一个主节点发生故障的情况下，仍然存在一个仲裁来运行集群。 Figure 1. (a) The architecture of Kubernetes. (b,c) Examples of YAML code for application deployment in Kubernetes. Kubernetes 中最基本的执行和资源单元称为 pod，它包含一个容器或一组容器，以及有关如何操作这些容器的说明。每个 Pod 代表应用程序的一个实例，并且始终属于一个命名空间。此外，属于同一应用程序的 Pod 是相同的，并且具有相同的规格。从这个意义上说，Pod 也可以称为副本。部署应用程序时，需要指定所需的副本数以及请求的资源量。图 1b 显示了在 Namespace-1 中以 Application-A 的名义创建应用程序，并请求其每个 Pod 250Mi 和 250m，内存和 CPU 可用。“Mi”表示“兆字节”，“m”表示“毫核”，即等于 CPU 内核 1&#x2F;1000 的唯一单位。它被 Kubernetes 定义为一种测量 CPU 资源的细粒度方法，以便多个 pod 可以共享一个 CPU 核心。 此外，每个 Pod 在集群中都分配有一个唯一的 IP 地址 [11]，如图 1a 所示。这种设计允许 Kubernetes 水平扩展应用程序。例如，当应用程序需要更多计算资源时，用户无需调整现有 Pod 的规格，只需创建另一个相同的 Pod 即可共享负载。然后，这个额外的 Pod 的 IP 地址将包含在应用程序的服务中，该服务将传入流量路由到新 Pod 以及现有 Pod。这将再次更详细地讨论。 主节点主节点通过控制平面的四个主要组件（即 kube-apiserver、kube-controller-manager、kube-scheduler 和 etcd [11]）对集群进行全面控制，如图 1a 所示。 kube-controller-manager 监视并确保集群在所需状态下运行。例如，一个应用程序正在运行4个pod;但是，其中一个被逐出或丢失，kube-controller-manager 必须确保创建新的副本。 kube-scheduler 查找新创建和未调度的 Pod 以将它们分配给节点。它必须考虑几个因素，包括节点的资源可用性和亲和力规范。在前面的示例中，当新 pod 已创建且当前未调度时，kube 调度程序会在集群内搜索满足要求的节点，并分配 pod 在该节点上运行。 etcd 是包含集群所有配置数据的后备存储。 kube-apiserver是可以与所有其他组件通信的基础管理组件，对集群状态的每次更改都必须经过它。kube-apiserver还能够通过 kubelet 与工作节点交互，这将在后面讨论。此外，用户可以通过将 kubectl 命令传递给 kube-apiserver 来通过主服务器管理集群。在图 1 中，在运行命令 kubectl apply -f examA.yaml 之后，此文件中的规范通过 kube-apiserver 传递到 kube-controller-manager 以进行副本控制，并传递到 kube-scheduler 以在特定节点上调度 Pod。他们将回复 kube-apiserver，然后 kube-apiserver 会向这些节点发出信号，要求创建和运行 pod。这些配置也存储在 etcd 中。 工作节点工作节点以 Pod 的形式分配计算资源，并根据主节点的指令运行它们。 kubelet 是一个本地代理，它按照主节点的 kube-apiserver 的指示操作 Pod，并保持它们的健康和活动状态。 kube-proxy （KP） 允许与集群的 Pod 进行外部和内部通信。如前所述，每个 Pod 在创建时都会分配一个唯一的 IP 地址。KP 使用此 IP 地址将来自集群内部和外部的流量转发到 Pod。 容器运行时：Kubernetes 可以被认为是容器化应用程序的专用编排平台，因此需要所有节点（包括主节点）中的容器运行时才能实际运行容器。它可以在各种运行时上运行，包括Docker，CRI-O和Containerd。其中，Docker [8]被认为是Kubernetes最常见的一个。通过将容器打包到轻量级映像中，Docker 允许用户自动部署容器化应用程序。 CAdvisor（或容器顾问）[35]是一种工具，可提供本地主机或容器的统计运行数据，例如资源使用情况。这些数据可以导出到kubelet或管理工具，如Prometheus，用于监控目的。CAdvisor 对 Docker 具有本机支持，并与 Docker 一起安装在所有节点中，以便能够监视群集内的所有节点。 Kubernetes 服务在 Kubernetes 中，可以在内部访问每个 pod，因为它具有可在集群内访问的唯一 IP 地址。但是，由于 Pod 可以随时创建和死亡，因此使用单个 Pod 的 IP 地址并不是一个合理的解决方案。此外，无法从群集外部访问这些 IP 地址，这使得用户请求或部署在不同群集中的应用程序之间的通信变得不可能。 这些情况的解决方案是Kubernetes Service [11]，它是一个抽象对象，它公开了一组可以在内部和外部轻松访问的Pod。有三种类型的 Kubernetes 服务： ClusterIP 在创建时分配给服务，并在此服务的整个生存期内保持不变。只能在内部访问群集 IP。在图 1a 中，服务 A、B 和 C 分配了三个不同的内部 IP 地址，并分别公开了三个服务端口 9897、9898 和 9899。例如，当地址 10.98.32.199：9899（由 Service-C 的集群 IP 和公开的端口组成）在集群内被命中时，流量会自动重定向到 Application-C Pod 容器上的 targetPort 9899，如图 1c 中的关键字选择器在 YAML 文件中指定的那样。根据所选策略选择确切的目标 Pod。 NodePort 是每个节点上的服务的保留端口，该节点正在运行属于该服务的 Pod。在图 1b 的示例中，NodePort 31197 和服务端口 9897 几乎耦合在一起。当流量到达节点 A 上的 NodePort 31197 时，它将路由到端口 9897 上的服务 A。然后，与前面的示例类似，流量依次路由到 targetPort 9897 上的 Pod A-1 和 A-2。这使得甚至可以从集群外部访问 Pod。例如，如果节点A的外部IP地址130.211.11.131可以从互联网访问，则通过点击地址130.211.11.131：31197，用户实际上是在向Pod A-1和A-2发送请求。但是，很明显，直接访问节点的IP地址并不是一种有效的策略。 负载平衡由特定云服务提供商提供。当集群部署在 GCP [2]、Azure [3] 或 AWS [1] 等云平台上时，会为其提供负载均衡器，该负载均衡器可通过 URL 在外部轻松访问（www.my-example-app.com）。发往此 URL 的所有流量都将以与上一个示例类似的方式转发到 NodePort 31198 上的群集节点，如图 1a 所示。 水平自动伸缩在 Kubernetes 中，HPA 是一项强大的功能，它可以自动提高 Pod 的数量，从而提高应用程序的整体计算和处理能力，而无需停止应用程序当前正在运行的实例 [11]。成功创建后，这些新 Pod 能够与现有 Pod 共享传入负载。从技术角度来看，HPA是由kube-controller-manager实现的控制回路。默认情况下，每隔 15 秒（也称为同步周期）kube-controller-manager 都会将收集的指标与 HPA 配置中指定的阈值进行比较。图 2a 显示了 HPA 的配置。’’minReplicas’ 和 ‘’maxReplicas’ 是指应该在集群中运行的 pod 的最小和最大数量。在此示例中，minReplicas 和 maxReplicas 分别为 2 和 4。复制控制器是 kube-controller-manager 的一个组件，它跟踪副本集，并确保此应用程序的 Pod 始终在群集中运行不少于 2 个且不超过 4 个。用于此 HPA 的指标是 CPU 使用率。一旦 CPU 利用率的平均值达到预设阈值，HPA 就会通过计算以下内容自动增加 Pod 数： 1desiredReplicas = currentReplicas * currentMetricValue / desiredMetricValue 其中，desiredReplicas是扩展后的 Pod 数，currentReplicas 是当前运行的 Pod 数，currentMetricValue 是最新收集的指标值，所需的 desiredMetricValue 是目标阈值。desiredMetricValue实际上是图2a中的阈值目标AverageValue。 在此示例中，当currentMetricValue（在本例中为CPU使用率）达到150 m（高于阈值desiredMetricValue 60 m）时，desiredReplicas等于d2×（150&#x2F;60）e或5。但是，由于副本的最大数量只有 4 个，因此 kube-controller-manager 仅向 kube-apiserver 发出信号，要求再增加 2 个副本。在此之后，如果平均 CPU 使用率下降到 40 m，则desiredReplicas 为 d4 ×（40&#x2F;60）e 或 3。因此，将删除其中一个新创建的 Pod。但是，值得注意的是，为了避免重复创建和删除 Pod，因为指标可能会大幅波动，每个新创建的 Pod 在从集群中删除之前，至少会运行一段时间的降级延迟期。这段时间定为5分钟[11]。此外，HPA 可以使用多个指标，每个指标都有自己的阈值。当这些指标中的任何一个达到其阈值时，HPA 将以上述方式扩展群集。但是，对于缩减操作，所有这些指标都必须低于其阈值。 kube-controller-manager用于水平自动缩放的上述指标是Kubernetes的默认资源指标或Prometheus [13]，Microsoft Azure [3]等提供的外部自定义指标。在本文的范围内，我们只讨论Kubernetes资源指标和Prometheus Custom Metrics，这是HPA最受欢迎的。 Figure 2. (a) Examples of YAML code for configuring HPA and Readiness Probe in Kubernetes. (b) Horizontal Pod Autoscaling’s (HPA) architecture. Kubernetes 资源指标如图 2b 所示，cAdvisor 充当监控代理，收集主机和正在运行的 Pod 的 CPU、内存使用情况等核心指标，并通过 HTTP 端口发布这些指标。例如，在图 2b 中，cAdvisor 当前正在监视 4 个现有的 Pod A-1、A-2、B-1 和 B-2。在 kubelet 的帮助下，Metrics-Server 会定期抓取这些指标。默认抓取周期为 60 秒，可以通过更改 Metrics-Server 的部署配置进行调整。然后，Metrics-Server将它们作为各个Pod和节点的CPU和内存使用情况公开给kube-apiserver中的指标聚合器，其平均值将被计算并提取到HPA。这称为资源指标管道 [11]。Kubernetes Resource Metrics可以通过将命令kubectl top pod和kubectl top node传递给kube-apiserver来手动检查。 Prometheus 自定义指标Prometheus [13]允许灵活的监控，因为它将监控的目标公开为端点，并通过HTTP服务器定期提取其指标。它可以监视各种目标，包括节点，pod，服务甚至自身。其中每个目标的监视操作称为作业。虽然Prometheus的默认全局获取周期为60秒，但其工作可以有自己的周期。对于寿命太短而无法抓取的工作，Prometheus有一个名为Pushgateway的组件，这些工作可以在退出后直接推送其指标。然后，通过将Pushgateway也公开为端点，Prometheus可以在以后抓取这些指标，即使在作业终止之后也是如此。如图 2b 所示，Prometheus 使用一个名为 kubernetes-pods 的作业从现有的 pod A-1、A-2、B-1 和 B-2 中抓取指标。 然后，抓取的数据以时间序列的形式存储在时间序列数据库（TSDB）中，该数据库暴露给Prometheus Adapter[36]，该适配器是用PromQL（Prometheus Query Language）编写的 - 一种功能查询语言[13]，并且有几个查询来实际处理原始时间序列指标。例如，查询速率（http_requests_total[1m]）返回1分钟内收集的时间序列的每秒平均速率。时间序列的编号取决于抓取周期。 对于上述查询，15 秒的抓取周期总共会产生 4 个时间序列。 查询完成后，生成的指标将被发送到 kube-apiserver 中的 Metrics Aggregator，并通过该服务器提取到 HPA。虽然指标服务器只能监控CPU和内存使用情况，但Prometheus可以提供各种自定义指标。上一示例中的指标可用作 HTTP 请求的平均到达率，该比率可添加到图 2a 中的 HPA 中。因此，在本例中，如果有两个输入指标，则 HPA 会在任一指标达到其阈值时纵向扩展，并在两个指标都低于其阈值时缩减。 就绪性探头在大多数情况下，Pod 在创建后尚未准备好立即提供流量，因为它们可能必须加载数据或配置。因此，如果在这些新创建的 Pod 启动期间将流量发送到它们，则请求显然会失败。作为一种解决方案，Kubernetes 提供了一个名为 Readiness Probe [11] 的功能，该功能检查新 Pod 的状态，并且仅在准备就绪后才允许流量流向它们。在图 2a 中，初始的DelaySeconds定义了从创建 Pod 到首次检查 Pod 的就绪时间。如果在检查之后，Pod 仍未准备就绪，Kubernetes 将每隔一段时间定期检查一次。在这个例子中，一旦 Pod B-3 和 B-4 被创建，Kubernetes 就会给它 5 个时间来启动和准备就绪。第一次准备情况检查后，如果容器的状态为就绪，它将立即开始为传入流量提供服务。另一方面，如果不是，Kubernetes 每 10 秒检查一次 3 次，这由 failureThreshold 定义，然后放弃并决定根据预设配置重置或将 pod 视为未就绪。 性能评估在本节中，我们将详细展示和讨论评估结果，以确认我们对 Kubernetes 及其 HPA 功能的理解之前，我们将介绍我们的实验设置。我们还就如何优化HPA提供分析和深入见解。 实验设置我们在一台在英特尔（R） 酷睿 （TM） i7-8700 @ 3.20Ghz * 12 上运行的物理机内，设置了一个由 5 个节点组成的 Kubernetes 集群，该节点由 1 个主节点和 4 个工作节点组成。集群的每个节点都运行一个虚拟机，其中包含 Ubuntu 18.04.3 LTS 操作系统、Docker 版本 18.09.7、Kubernetes 版本 1.15.1。在计算能力方面，主节点分配了4个核心处理器和8GB RAM，而每个工作节点则分配了2个核心处理器和2GB RAM。此外，Gatling开源版本3.3.1 [37]被用作负载生成器，通过每个工作节点上的指定NodePort向我们的应用程序发送HTTP请求。 我们的应用程序被设计为 CPU 扩展。换句话说，一旦它成功接收到HTTP请求，它就会使用CPU资源，直到将响应发送回源。每个副本的 CPU 请求和限制分别为 100 m 和 200 m。副本数的范围从最小值 4（平均 1 个副本&#x2F;节点）到最大 24 个（平均每个节点 6 个副本）。 所有实验运行300秒。在前 100秒，Gatling 发送的平均传入请求速率约为 1800 个请求&#x2F;秒，而在接下来的 100 个请求中，它大约为 600 个请求&#x2F;秒，总共生成 240，000 个请求。我们将这两个时间段分别定义为高流量周期 （HTP） 和低流量周期 （LTP）。其余的模拟时间用于在没有到达请求的情况下观察指标的减少。在本文中，我们在7个不同的实验中测试了Kubernetes HPA的性能。每个实验重复10次，以确保其准确性。 实验结果默认 Kubernetes 资源指标的 HPA 性能设置。指标的抓取周期设置为默认值 60 秒。 目标。我们的目标是使用默认的 Kubernetes 资源指标 （KRM） 在默认抓取期内的 CPU 使用率、副本数和失败请求数方面评估 HPA 的性能。 如图 3a 所示，由于请求速率高，平均 CPU 使用率增加到极限。之后，随着副本数量的增加，它也会减少。可以观察到，最引人注目的一点是CPU使用率的指标值在每个抓取周期（60 s）都会发生变化。这是因为 Kubelet 只在抓取周期开始时从 cAdvisor 中抓取原始指标。然后，通过指标服务器报告指标，而不对指标聚合器进行任何修改。换句话说，指标的报告值与抓取的值完全相等。 图 3.HPA 使用默认的 Kubernetes Resource Metrics （KRM）。（a） 平均 CPU 使用率和副本集的缩放比例。（b） 失败请求的总数。（c） 失败请求的时间表。 将副本集扩展到 8 的第一个扩容操作发生在第 50 秒左右，因为指标值在第 35s和第 40 个秒之间增加。之后，当指标值达到最大值 200% 时，副本集将在第 65 秒再次扩展到 15。但是，第三个缩放操作发生在第 110s，即 55s 之后，即使 CPU 使用率仍然很高。这是由于HPA的一个非常重要的特征。默认情况下，HPA 每 15 秒检查一次指标值。在检查时，如果与上次检查相比，该值没有变化，则认为没有必要调整副本数。我们可以看到，从第 40 秒到第 100 秒，正好是一个抓取周期，CPU 使用率不会改变。在此之后，它下降到约180%，这实际上触发了对 21 个副本的第三次缩放操作。在这里，指标值再次保持稳定，直到第 160 秒，它开始下降，并且不会导致任何进一步的扩展。复制副本的数量在实验结束之前保持稳定，因为 HPA 必须等待 5 分钟，从上次放大到缩小。此设计旨在避免由连续缩放操作引起的抖动。 图3b，c显示了失败请求的数量和失败的时间。这些请求在扩展操作期间被拒绝，因为新创建的 Pod 尚未准备好为流量提供服务。在此期间路由到它们的任何请求都将失败。此外，我们可以看到，第一次扩展会导致大多数请求失败，因为它发生在 HTP 期间。高请求速率会导致更多请求被拒绝。与此相反，第三次纵向扩展仅会导致少量故障。 总之，需要注意的是，Kubernetes HPA 旨在定期检查指标值，有关扩展的决策取决于指标值是否与上次检查相比发生了变化。此外，由于 KRM 的值仅在每个抓取周期内更改，因此创建的副本数取决于此因素。因此，下一个实验分析抓取周期的长度对HPA性能的影响。 使用默认 Kubernetes 资源指标和不同抓取周期的 HPA 性能设置。KRM的抓取周期分别调整为15 s，30 s和60 s。 目标。我们的目标是研究不同抓取期对HPA性能的影响。 图 4 显示了使用 KRM 的 HPA 在 15 秒、30 秒和 60 秒三个不同抓取周期内的性能。在图 4a–c 中，我们可以看到指标值的趋势仍然遵循前面描述的模式。这些值会更改所有三种情况的每个抓取周期。换句话说，随着周期的延长，指标值将保持在同一水平更长的时间内。 Figure 4. HPA using default Kubernetes Resource Metrics (KRM). (a–c) The average CPU usage and the scaling of the replica set for scraping periods of 15 s, 30 s, and 60 s, respectively. (d–f) The total number of the failed requests for scraping periods of 15 s, 30 s, and 60 s, respectively. 但是，副本的最大数量往往会随着抓取周期的增加而减少。在图 4a、b 中，副本的最大数量分别为 24 和 23。另一方面，对于60秒抓取期的情况，它只有21。这是因为如果指标值没有变化，则不会触发扩容操作，如前所述。在这种情况下，由于值在较短的抓取周期内更改得更频繁，因此副本数也会更频繁地增加。此外，图 4d–f 显示了三种情况下失败的请求数。与副本的最大数量类似，失败请求的数量往往会随着爬取周期的延长而减少。这是因为未准备好的 Pod 获得的传入流量越多，被拒绝的请求就越多。请注意，进入未就绪 Pod 的请求的失败可以使用就绪情况调查来解决，其影响将在第 5.2.7 节中分析。 通过上述解释，可以得出结论，当在相同的负载下相同时间时，较长的抓取期会导致HPA产生较少数量的复制品。这有优点和缺点。一方面，较长的抓取期可能会通过触发相对缓慢的扩展操作来添加较少数量的副本，从而实现高效的资源分配。另一方面，如果传入负载变得过高，则可能导致所需资源不足。 HPA性能与Prometheus 自定义指标设置。Prometheus Custom Metrics 的抓取周期设置为默认值 60 s。 目标。我们的目标是使用 Prometheus 自定义指标评估 HPA 的性能，包括 CPU 使用率、副本数量和失败请求，以便与使用 KRM 的 HPA 进行比较。 从图 5a 可以看出，每次查询 PCM 的指标值时，都会非常频繁地变化。这与KRM的情况完全相反。其原因在于PCM的收集方式。尽管 Prometheus 也根据抓取周期来抓取 pod 的指标，但这些指标必须通过 Prometheus Adapter 的 rate（） 函数来计算它们每秒平均增加的次数 [13]。此外，重要的是要注意，rate（） 函数还在必要时根据指标的当前趋势执行外推，例如在缺少时间序列数据点的情况下。因此，在这种情况下，CPU 使用率在每个查询周期都会更改。反过来，这些频繁的变化导致HPA迅速将副本数量增加到最多24个，而KRM只有21个。因此，PCM 具有对指标值频繁变化的响应能力优势。快速增加副本数量或整体计算能力，使 HPA 能够处理传入负载的激增。但是，缺点是失败请求数较多，如图 5b，c 所示。 Figure 5. HPA using Prometheus Custom Metrics (PCM). (a) The average CPU usage and the scaling of the replica set. (b) The total number of the failed requests. (c) The timeline of the failed requests. HPA性能与普罗米修斯自定义指标和不同的抓取周期设置。指标抓取周期为15s，30s和60s。 目标。我们的目标是研究使用PCM的不同抓取周期对HPA性能的影响。 图 6 显示了 HPA 在 CPU 使用率方面的性能，以及具有三个不同抓取周期（分别为 15 秒、30 秒和 60 秒）的副本数。可以看出，在所有这三种情况下，图表的趋势都非常相似。如前所述，原因是Prometheus Adapter（负责将原始指标转换为自定义指标的实体）可以执行外推以在原始数据点丢失时提供指标。如果查询时刻处于抓取周期的中间，Prometheus Adapter（尤其是 rate（） 会根据之前收集的数据点计算指标。由于指标值的相似性，在同一时期内，三个事例的副本数以类似的模式增加。 Figure 6. HPA using Prometheus Custom Metrics (PCM). (a–c) The average CPU usage and the scaling of the replica set for scraping periods of 15 s, 30 s, and 60 s, respectively. (d–f) The total number of the failed requests for scraping periods of 15 s, 30 s, and 60 s, respectively. 我们可以得出结论，PCM不像KRM那样受到抓取期调整的强烈影响。可以选择更长的抓取期，因为它将减少收集和提取指标所需的计算和内部通信资源量。但是，值得注意的是，较长的抓取周期意味着数据点较少，这可能会降低速率函数的精度。 2-worker 集群和 4 Worker 集群中的 HPA 性能比较设置。设置了两个由 2 个和 4 个工作节点组成的集群。工作线程节点是相同的，并且两种情况下副本的最大和最小数量是相同的。 目标。我们的目标是调查和比较HPA在这两种情况下的表现。 图 7 显示了在 2 工作线程群集和 4 工作群集（PCM-2W 和 PCM-4W）中使用 PCM 的 HPA 之间的性能比较。从图 7a 中可以得出结论，两个 CPU 使用率值的总体趋势大致相似。CPU 使用率中唯一明显的差异出现在 30 和 70 秒之间。这是由于副本增加的差异造成的，如图 7b 所示，其中 PCM-2W 增加副本数量的速度比 PCM-4W 慢。这是因为，虽然两个 HPA 的最大副本数均为 24，但对于 4 个工作线程和 2 个工作线程集群，每个节点上的平均 Pod 数分别为 6 个和 12 个。它表明，与 4 工作线程群集中的节点相比，2 工作线程群集中的节点在扩展时必须处理更高的计算负载，这使得为其他 Pod 创建和分配资源的速度更慢。 Figure 7. Comparisons of HPA performances in 2-worker and 4-worker clusters. (a) The average CPU usage. (b) The scaling of the replica set. (c) The total number of the failed requests. (d) Response time of the successful requests. 不同自定义指标的HPA性能比较设置。与仅监视 CPU 和内存使用情况的 KRM 相反，PCM 可以监视其他指标，例如传入 HTTP 请求的速率。在第一种情况下，仅请求速率用于 HPA。另一方面，在第二种情况下，它与CPU使用率相结合。 目标。我们的目标是研究使用不同自定义指标对HPA性能的影响。 图 8 显示了使用 HTTP 请求 （PCM-H） 的 HPA 与使用 CPU 和 HTTP 请求 （PCM-CH） 的 HPA 之间的性能比较。PCM-H 提供基于传入请求速率的自动缩放。另一方面，PCM-CH 结合了速率和 CPU 使用率。指定多个指标时，如果任一指标达到其阈值，HPA 将纵向扩展。从图8a的CPU使用率比较中可以看出，一般来说，PCM-H的CPU使用率明显高于PCM-CH。在图 8b 中，PCM-CH 的平均请求速率远高于 PCM-H 从第 30 秒到第 60 秒的平均请求速率。之后，它一直保持在较低水平，直到第250秒。这是因为 PCM-CH 的请求速率急剧上升，从而触发了第一次扩展反应，如图 8c 所示，之后 CPU 使用率仍高于其阈值，并导致后续副本数增加到最大数量。这里，由于 PCM-CH 具有更多副本，因此平均请求速率较低。另一方面，PCM-H 仅使用请求速率。在第一个拓展之后，耗时降低并保持在阈值以下，这不会触发任何进一步的拓展。此外，由于现在它只有13个副本，因此CPU使用率上升并保持在较高水平。此外，由于 Pod 的增加较少，PCM-H 产生的失败请求数仅为 PCM-CH 的一半左右。 Figure 8. Comparison of HPA Performances with different custom metrics. (a) The average CPU usage. (b) The average rate of HTTP requests. (c) The scaling of the replica set. (d) The total number of the failed requests. 有和没有就绪探头的HPA性能比较设置。在一种情况下设置常规HPA，而在另一种情况下则伴随着准备情况调查。 目标。我们的目标是调查Readiness Probe的使用及其对失败请求数和响应时间的影响。 图 9a 显示了 HPA 与使用就绪度探测器 （RP） 和常规 HPA 在失败请求数方面的比较。很明显，在使用RP的情况下，没有失败的请求，因为当额外的 pod 准备就绪时，Kubernetes 服务不会将任何流量路由到它们。只有在它们被认为准备就绪后，它们才能接收和处理传入的请求。因此，我们可以预期，通过使用 RP 可以避免从前面的实验中观察到的失败请求。但是，这是一种权衡，因为一般响应时间明显高于图 9b 中所示的不使用 RP 的情况，因为更多的流量被路由到现有 Pod。 Figure 9. Comparison of HPA performances with and without Readiness Probe. (a) The total number of the failed requests. (b) Response time of the successful requests. 讨论为了总结之前的实验和分析，我们列出了Kubernetes HPA行为的几个关键点。 关于 KRM 和 PCM：KRM 仅报告指标值，并且每个抓取周期只能更改一次，而 PCM 则即使在抓取周期的中间或缺少数据点时，PCM 也能够保持指标值的趋势。直接结果是，与 PCM 相比，KRM 扩展副本集的速度较慢，并且主要扩展到较少数量的副本。这种行为的优点显然是更少的资源消耗。另一方面，当在高负载下时，Pod可能会崩溃或变得不可用。因此，我们建议将 KRM 用于负载更稳定的应用，例如视频处理服务。在这种情况下，来自观看者的请求数量通常很小，因为视频至少需要几分钟到几个小时。相反，PCM 更适合指标频繁变化的应用程序。例如，电子商务网站可能会在销售活动期间在几个小时内经历持续的激增，因此需要快速的系统反应。 关于抓取周期：对PCM抓取周期的调整不会对HPA的性能产生强烈影响。因此，可以设置更长的时间段以减少用于提取指标的资源量。但是，值得注意的是，过长的时间段可能会导致计算指标不精确。关于KRM，抓取周期对HPA的性能有重大影响。较长的时间段可以减少为新 Pod 分配的资源量，但会导致服务质量下降。因此，在考虑了服务类型和群集功能的情况下，应仔细选择抓取期。 关于集群大小：很明显，4个工作线程的集群具有更多的计算能力，这使得它能够比2个工作线程集群更快地执行HPA操作，假设两个集群的工作线程在计算能力方面是相同的。此外，4 工作线程群集的通信能力优于 2 工作线程群集。这会导致两个集群的请求响应时间不同。但是，即使 2 工作线程群集具有相同的计算能力和通信能力，将 Pod 扩展到更宽的群集也更安全，因为当节点崩溃时，与 4 工作线程群集中的四分之一 Pod 相比，一半的 Pod 可能变得不可用。 在具有不同自定义指标的 HPA 上：Prometheus 允许使用自定义指标（如 HTTP 请求速率）来满足特定需求。特别是将多个指标组合在一起也可以提高HPA的有效性，因为任何单个指标的变化都会导致缩放反应。但是，作为缺点，这可能会导致资源浪费。因此，应根据应用程序的类型选择指标或指标组合。例如，游戏应用程序可能具有各种请求大小。在地图上移动字符的请求很小，但数量可能很多。因此，应考虑请求速率，以便可以快速满足每个请求，从而减少“滞后”效应并改善整体游戏体验。另一方面，加载新位置地图的请求很重，但数量很少。在这里，计算要求明显增长，这表明HPA应该根据CPU和内存使用情况进行扩展。简而言之，自定义指标使应用程序能够考虑各种因素，例如请求数量，延迟和带宽，以实现有效的水平自动缩放。 关于Readiness Probe：Kubernetes 的一项强大功能是防止请求被路由到未准备好的 Pod，这将拒绝请求。但是，将许多请求路由到现有 Pod 可能会导致其余请求的响应时间明显延长。因此，在保持传入请求处于活动状态或让它们失败和期望重新请求之间，应根据系统资源和 QoS 要求之间的平衡仔细选择一个请求。 结论Kubernetes 是一个功能强大的容器化应用程序和服务编排平台，可应用于包括云&#x2F;边缘计算和物联网网关在内的重要未来技术。其功能 HPA 可为应用程序提供动态有效的扩展，而无需人工干预。在本文中，我们给出了 Kubernetes 和 HPA 的第一个全面的架构级视图。还彻底解释了每种类型的指标（包括 Kubernetes 资源指标和 Prometheus 自定义指标）是如何收集、计算和提取到 HPA 的。此外，我们还进行了多次实验，涵盖了各种场景，并对Kubernetes HPA的行为进行了清晰的分析。 本文应作为进一步研究和开发Kubernetes和HPA的基础研究。未来，我们的目标是通过更多的HPA场景扩展我们的实验，并为Kubernetes开发更有效的扩展算法。 作者贡献：Conceptualization, T.-T.N., Y.-J.Y. and T.K.; Experiment, T.-T.N. and Y.-J.Y.; Writing—Original Draft Preparation, T.-T.N.; Review &amp; Editing, T.K.; Supervision, D.-H.P. and T.K.; Funding Acquisition, D.-H.P. and S.K. All authors have read and agreed to the published version of the manuscript. 资助：这项工作得到了韩国政府（MSIT）资助的信息通信技术规划与评估研究所（IITP）资助（No.2018-0-00387，开发基于ICT的智能智能福利住房系统，用于预防和控制牲畜疾病）。 致谢：作者对Linh-An Phan在整个研究中的许多宝贵评论表示高度赞赏，并感谢Dinh-Nguyen Nguyen在早期阶段的支持。 利益冲突：作者声明没有利益冲突。 引用 Amazon Web Services. Available online: https://aws.amazon.com (accessed on 23 June 2020). Google Cloud Platform. Available online: https://cloud.google.com (accessed on 23 June 2020). Microsoft Azure. Available online: https://azure.microsoft.com (accessed on 23 June 2020). Pahl, C.; Brogi, A.; Soldani, J.; Jamshidi, P. Cloud Container Technologies: A State-of-the-Art Review. IEEE Trans. Cloud Comput. 2017, 7, 677–692. [CrossRef] He, S.; Guo, L.; Guo, Y.; Wu, C.; Ghanem, M.; Han, R. Elastic application container: A lightweight approach for cloud resource provisioning. In Proceedings of the 2012 IEEE 26th International Conference on Advanced Information Networking and Applications, Fukuoka, Japan, 26–29 March 2012; pp. 15–22. [CrossRef] Dua, R.; Raja, A.R.; Kakadia, D. Virtualization vs containerization to support PaaS. In Proceedings of the2014 IEEE International Conference on Cloud Engineering, Boston, MA, USA, 11–14 March 2014; pp. 610–614. [CrossRef] Pahl, C. Containerization and the PaaS Cloud. IEEE Cloud Comput. 2015, 2, 24–31. [CrossRef] Docker. Available online: https://www.docker.com (accessed on 23 June 2020). Amazon Elastic Container Service. Available online: https://aws.amazon.com/ecs (accessed on 23 June 2020). Red Hat OpenShift Container Platform. Available online: https://www.openshift.com/products/containerplatform (accessed on 23 June 2020). Kubernetes. Available online: www.kubernetes.io (accessed on 23 June 2020). Chang, C.C.; Yang, S.R.; Yeh, E.H.; Lin, P.; Jeng, J.Y. A Kubernetes-based monitoring platform for dynamic cloud resource provisioning. In Proceedings of the GLOBECOM 2017—2017 IEEE Global Communications Conference, Singapore, 4–8 December 2017; pp. 1–6. [CrossRef] Prometheus. Available online: https://prometheus.io (accessed on 23 June 2020). Cloud Native Computing Foundation. Available online: https://www.cncf.io (accessed on 23 June 2020). Thurgood, B.; Lennon, R.G. Cloud computing with Kubernetes cluster elastic scaling. ICPS Proc. 2019, 1–7, doi:10.1145&#x2F;3341325.3341995. [CrossRef] Rattihalli, G.; Govindaraju, M.; Lu, H.; Tiwari, D. Exploring potential for non-disruptive vertical auto scaling and resource estimation in kubernetes. In Proceedings of the IEEE International Conference on Cloud Computing (CLOUD), Milan, Italy, 8–13 July 2019; pp. 33–40. [CrossRef] Song, M.; Zhang, C.; Haihong, E. An auto scaling system for API gateway based on Kubernetes. In Proceedings of the 2018 IEEE 9th International Conference on Software Engineering and Service Science (ICSESS), Beijing, China, 23–25 November 2018; pp. 109–112. [CrossRef] Jin-Gang, Y.; Ya-Rong, Z.; Bo, Y.; Shu, L. Research and application of auto-scaling unified communication server based on Docker. In Proceedings of the 2017 10th International Conference on Intelligent Computation Technology and Automation (ICICTA), Changsha, China, 9–10 October 2017; pp. 152–156. [CrossRef] Townend, P.; Clement, S.; Burdett, D.; Yang, R.; Shaw, J.; Slater, B.; Xu, J. Improving data center efficiency through holistic scheduling in kubernetes. In Proceedings of the 2019 IEEE International Conference on Service-Oriented System Engineering (SOSE), Newark, CA, USA, 4–9 April 2019; pp. 156–166. [CrossRef] 20. Rossi, F. Auto-scaling policies to adapt the application deployment in Kubernetes. CEUR Workshop Proc. 2020, 2575, 30–38. Balla, D.; Simon, C.; Maliosz, M. Adaptive scaling of Kubernetes pods. In Proceedings of the IEEE&#x2F;IFIPNetwork Operations and Management Symposium 2020: Management in the Age of Softwarization andArtificial Intelligence, NOMS 2020, Budapest, Hungary, 20–24 April 2020; pp. 8–12. [CrossRef] Casalicchio, E.; Perciballi, V. Auto-scaling of containers: The impact of relative and absolute metrics.In Proceedings of the 2017 IEEE 2nd International Workshops on Foundations and Applications of Self*Systems (FAS*W), Tucson, AZ, USA, 18–22 September 2017; pp. 207–214. [CrossRef] Casalicchio, E. A study on performance measures for auto-scaling CPU-intensive containerized applications. Clust. Comput. 2019, 22, 995–1006. [CrossRef] Taherizadeh, S.; Grobelnik, M. Key influencing factors of the Kubernetes auto-scaler for computing-intensive microservice-native cloud-based applications. Adv. Eng. Softw. 2020, 140, 102734. [CrossRef] Santos, J.; Wauters, T.; Volckaert, B.; De Turck, F. Towards network-Aware resource provisioning in kubernetes for fog computing applications. In Proceedings of the 2019 IEEE Conference on Network Softwarization (NetSoft), Paris, France, 24–28 June 2019; pp. 351–359. [CrossRef] Santos, J.; Wauters, T.; Volckaert, B.; Turck, F.D. Resource provisioning in fog computing: From theory to practice. Sensors 2019, 19, 1–25. [CrossRef] [PubMed] Zheng, W.S.; Yen, L.H. Auto-scaling in Kubernetes-based Fog Computing platform. In International Computer Symposium; Springer: Singapore, 2018; pp. 338–345. [CrossRef] Heapster. Available online: https://github.com/kubernetes-retired/heapster (accessed on 23 June 2020). InfluxDB. Available online: https://www.influxdata.com (accessed on 23 June 2020). Grafana. Available online: https://grafana.com (accessed on 23 June 2020). Apache JMeter. Available online: https://jmeter.apache.org (accessed on 23 June 2020). Syu, Y.; Wang, C.M. modeling and forecasting http requests-based Cloud workloads using autoregressive artificial Neural Networks. In Proceedings of the 2018 3rd International Conference on Computer and Communication Systems (ICCCS), Nagoya, Japan, 27–30 April 2018; pp. 21–24. [CrossRef] Dickel, H.; Podolskiy, V.; Gerndt, M. Evaluation of autoscaling metrics for (stateful) IoT gateways.In Proceedings of the 2019 IEEE 12th Conference on Service-Oriented Computing and Applications (SOCA), Kaohsiung, Taiwan, 18–21 November 2019; pp. 17–24. [CrossRef] Prometheus Operator. Available online: https://github.com/coreos/prometheus-operator (accessed on 23 June 2020). CAdvisor. Available online: https://github.com/google/cadvisor (accessed on 23 June 2020). Prometheus Adapter. Available online: https://github.com/DirectXMan12/k8s-prometheus-adapter (accessed on 23 June 2020). Gatling. Available online: https://gatling.io/open-source (accessed on 23 June 2020).","categories":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"http://jerryblogs.com/categories/Kubernetes/"},{"name":"论文翻译","slug":"Kubernetes/论文翻译","permalink":"http://jerryblogs.com/categories/Kubernetes/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/"}],"tags":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"http://jerryblogs.com/tags/Kubernetes/"}]},{"title":"限制内核模块自动加载","slug":"linux-core-module-load","date":"2022-05-25T14:06:37.000Z","updated":"2023-04-20T13:39:45.664Z","comments":true,"path":"2022/05/25/linux-core-module-load/","link":"","permalink":"http://jerryblogs.com/2022/05/25/linux-core-module-load/","excerpt":"","text":"概述内核的模块机制允许构建具有广泛软硬件支持的内核，而无需将所有的代码实际加载到任何给定的运行系统中。在一个典型的分配器内核中，所有这些模块的可用性意味着非常多的功能是可用的，但是也可能存在许多可以被利用的bug。在许多情况下，内核的自动模块加载器已被用于将错误代码带入正在运行的系统当中。减少内核暴露于有缺陷的模块的尝试表明，某些类型的加固工作是多么困难。 模块自动加载两种方法可以将模块加载到Linux内核中，无需管理员进行明确的操作。在大多数的现有系统上，它发生在硬件被发现时，无论是通过总线驱动程序（在受支持被发现的总线上）还是通过设备树等外部描述。这个发现将导致一个事件被发送到用户空间像udev这样的已经被配置的守护进程应用和被加载的适当的模块。这种机制由可用的硬件驱动，并且相对难以受攻击者影响。 然而在内核中，潜伏着一种较旧的机制，即request_module()函数的形式。当内核函数确定缺少所需的模块时，它可以调用request_module()函数向用户空间发送请求以加载相关的模块。例如，如果应用程序使用给定的主要和次要编号打开一个char设备，并且这些数字不存在驱动程序，则char设备代码将尝试通过调用来定位驱动程序： 1request_module(&quot;char-major-%d-%d&quot;, MAJOR(dev), MINOR(dev)); 如果驱动模块声明了一个具有匹配编号的别名，他将自动加载到内核中以处理打开请求。 内核中有数百个request_module()调用。有些是非常具体的；如果用户不幸拥有这样一个设备，就会自动加载ide-tape模块。其他的更为普遍；例如，在网络子系统中有许多调用，以找到实现特定网络协议或数据包过滤机制的模块。虽然特定设备的调用已经大部分被udev机制所取代，但是像网络协议这样的模块仍然依赖于request_module()来实现用户透明的自动加载。 自动加载方便系统管理，但是它也可以方便于系统的开发。如果DCCP模块未被加载到内核中，则2月份发布的DCCP协议漏洞是不可利用的。通常情况下，因为DCCP的用户很少。但是自动加载机制允许任何用户通过创建一个DCCP套接字来强制加载该模块。因此，自动加载扩大了内核的攻击面，将非特权用户可能导致加载的任何模块包含在模块中——典型的分发器内核中有很多模块。 收紧系统Djalal Harouni一直在开发一个补丁集，旨在减少自动加载的风险；最新版本是在11月27日发布的。Harouni的工作从grseurity补丁集的加固中获得了灵感，但没有从那里获得代码。在这个版本中（随着时间的推移，它有一些变化），它增加了一个新的sysctl的按钮（&#x2F;proc&#x2F;sys&#x2F;kernel&#x2F;module_autoload_mode），用来限制内核的自动加载机制。如果这个按钮被设置为零（默认值），自动加载就会像在当前内核一样工作。把它设置为1，就可以把自动加载限制在具有特定功能的进程中：具有CAP_SYS_MODULE的进程就可以使任何模块被加载，而具有CAP_NET_ADMIN的进程可以加载任何以netdev-开头的模块。将这个按钮设置为2可以完全禁用自动加载功能。一旦这个值被提高到0以上，在系统的生命周期内就不能再降低了。 补丁集还实现了可以使用prctl()系统调用设置每个进程的标志。该标志（与全局标志具有相同的值）可以限制特定进程及其所有后代的自动加载，而不会改变整个系统中的模块加载行为。 可以肯定地说，这个补丁集不会以其当前形式合并，原因很简单：Linus Torvalds非常不喜欢它。禁用自动加载可能破坏许多系统，这意味着分销商将不愿意启动这个选项，他也不会有太多的用途。“人们在不破坏系统的情况下无法使用的安全选项毫无意义”，他说。讨论有时候会变得激烈，但是Linus并不反对减少内核暴露于自动加载漏洞的想法。这只是找到正确解决方案的问题。 每个进程标志看起来是解决该问题方案的一部分。例如，它可以用于限制在容器内运行的代码的自动加载，同时保持整个系统不变。在容器中创建具有CAP_NET_ADMIN功能的进程来配置该容器的网络并希望容器中运行的大多数代码无法强制加载模块的情况并不少见。 但是Linus说，一个标志永远无法正确控制自动加载发挥作用的所有情况。一些模块可能始终是可加载的，而其他模块可能需要特定的功能。因此他建议保留Harouni的补丁集添加的request_module_cap()函数（仅在存在特定功能时才执行加载）并更广泛使用它。但他确实有一些更改要求。 首先是request_module_cap()不应该在所需的功能不存在的情况下实际阻止模块的加载-至少在开始时不应该。相反，它应该记录一条信息。这将允许对实际需要模块自动加载的地方进行研究，如果幸运的话，将指出可以限制自动加载而不破坏现有系统的地方。他还建议，能力检查过于简单。例如，上面描述的“char-major-”自动加载在进程能够打开具有给定的主要和次要编号的设备节点时发生。在这种情况下，权限测试（打开该特殊文件的能力）已经通过，模块应该无条件加载。因此，可能需要request_module()的其他条件来描述功能不实用的设置。 最后Linus有另一个想法，即最糟糕的错误往往潜伏在维护不善的模块中。例如，上面提到的DCCP模块很少被使用并且无人维护。如果维护良好的模块标有特殊标志，则可以将非特权自动模块限制为仅对这些模块。这将阻止一些更复杂的模块的自动加载。不过这个想法确实提出了一个没人问的问题：当一个模块不在被维护时，谁会很好的维护它以去除“维护良好”的标志。 无论如何，如果Kees Cook提出的计划成立，该标志可能不会立即添加。他建议从启用警告的request_module_cap()方法开始。将为那些可以使用它的人添加每个进程的标志，但限制自动加载的全局按钮不会。最终有可能摆脱非特权模块加载，但这将是未来的目标。短期利益有关如何实际自动加载的更好信息，以及现在想要收紧的管理员提供每个进程的选项。 这段话强调了围绕内核强化工作的基本张力之一。很少有人反对更安全的内核但是一旦加固工作可以破坏现有系统，事情就会变得困难——而且情况通常如此。面向安全的开发人员经常对内核社区抵制对用户可见的影响的加固工作而感到沮丧，而内核开发者对那些会导致错误报告和用户不高兴的改动却没有什么同情心。这些挫折感在这次讨论的开发者都对达成一个所有人都有效的解决方案感兴趣。","categories":[{"name":"Linux","slug":"Linux","permalink":"http://jerryblogs.com/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://jerryblogs.com/tags/Linux/"}]},{"title":"Docker容器和虚拟机的性能评估","slug":"docker-vm-compare","date":"2022-05-25T10:35:17.000Z","updated":"2023-04-20T13:39:18.675Z","comments":true,"path":"2022/05/25/docker-vm-compare/","link":"","permalink":"http://jerryblogs.com/2022/05/25/docker-vm-compare/","excerpt":"","text":"文章信息作者：Amit M Potdara, Narayan D Gb, Shivaraj Kengondc, Mohammed Moin Mulla原文链接：https://reader.elsevier.com/reader/sd/pii/S1877050920311315 摘要服务器虚拟化是IT企业广泛使用的技术革新。虚拟化提供了一个在云上运行不同操作系统的服务的平台。它有利于在单个基本的物理机甚至于形式为hypervisor或容器里构建多个虚拟机。为了承载许多微服务应用，这个新兴的技术提出了一个模型，该模型由较小的单个部署服务执行的不同操作组成。因此，用于低开销的虚拟化技术的需求正在迅速发展。有许多轻量级的虚拟化技术，docker是其中之一，它是一个开源的平台。这项技术允许开发者和系统管理员使用docker引擎构建、创建和运行应用。本论文使用标准基准测试工具如Sysbench、Phoronix和Apache benchmark，这写工具包含CPU性能，内存吞吐量，存储读写性能，负载测试和运行速度测试。 前言在近些年，对云计算的关注正在增加。It行业在Xen、HyperV、VMware vSphere，KVM等等的出现上开发了许多技术，这些技术统称为虚拟化技术。要在同一虚拟机上部署多个应用程序，需要组织和隔离应用程序依赖。由于虚拟化，多个应用程序可以在同一个物理硬件上运行。虚拟化技术的缺点是：虚拟机体积大，由于运行多个虚拟机导致性能不稳定，启动过程需要很长时间才能运行，虚拟机无法解决可管理性，软件更新和持续集成持续交付等难题。这些问题导致了一种称为容器化的新进程的出现，进一步导致了操作系统级别的虚拟化，而虚拟化则将吸收带到了硬件级别。容器使用共享相关库和资源的宿主机操作系统。它更有效，因为它没有guest Os。在宿主机内核上，可以处理特定于应用程序的二进制文件和库，从而使执行速度非常快。容器是在Docker平台的帮助下形成的，该平台结合了应用程序及其依赖。这些应用总是在隔离空间中，在操作系统内核之上运行。Docker这种容器化功能可确保环境支持任何相关应用程序。在这项工作中，为了量化和对比虚拟机管理程序的虚拟机和Docker容器上的应用程序，进行了一系列实验。这些测试有助于我们了解两种主要的虚拟化技术（容器和虚拟机管理程序）的性能影响。本文组织如下：第2节给出了背景相关研究和关于技术和平台的简要说明。第3节介绍了用于了解性能比较的方法。第4节中，介绍了基准测试结果。最后，在第5节中提供了结论和未来的工作。 背景研究Docker容器化是一种技术，它将应用程序、相关依赖项和组织起来以容器的形式构建的系统库。构建和组织的应用程序可以作为容器执行和部署。这个平台被称为Docker，它确保应用可以工作在每个环境。它也自动执行将部署到容器中的应用程序。Docker在容器环境中附加了一个额外的部署引擎层，应用程序在其中执行和虚拟化。为了有效的运行代码，Docker有四个主要部分：Docker容器，Docker Client-Server、Docker镜像和Docker引擎。以下各节将对这些组件进行详细说明。 Docker引擎Docker系统的基本部分是Docker引擎，这是一个客户端-服务器工作模式的应用程序，它安装在主机上，具有以下组件： Docker Daemon：一种长时间运行的程序（Docker命令）有助于创建，构建和运行应用程序； RestApi被用来与docker daemon进行通信； 客户端通过终端发送请求到docker daemon来访问操作。 Docker客户端-服务器Docker技术主要是指客户端-服务器架构。客户端主要与Docker守护进程通信，Docker守护进程充当主机中存在的服务器。守护进程作为运行，构建和分发容器的三个主要进程。Docker容器和守护程序都可以放在一台机器中。 Docker镜像Docker镜像通过两个方法被构建。主要的方式是在一个只读模板的帮助下构建镜像。该模板包含基本镜像，甚至它可以是操作系统比如centos，Ubuntu16.04或者fedroa或者任何其他基本的轻量级的操作系统镜像。通常，基本镜像是每个镜像的基础。每当从头开始构建新镜像时都需要一个基本镜像。这种类型的创建新镜像成为“提交更改”。下一种方法是创建一个Dockerfile，其中包含创建Docker镜像的所有说明。当从终端执行Docker的构建命令时，将使用Dockerfile中声明的所有依赖项创建镜像，此过程称为构建镜像的自动化方法。 Docker容器Docker容器由Docker镜像创建。要以受限方式运行应用程序，应用程序所需的每个套件都将由容器保存。可以依据应用程序或软件的服务要求创建容器镜像。假设一个包含Ubuntu和Nginx服务器的应用程序已经被添加到到Dockerfile中，使用命令docker run，创建包含Nginx服务器的Ubuntu Os镜像的容器并开始运行。 虚拟机和Docker容器之间的比较Docker有时被称为轻量级虚拟机，但是它不是虚拟机。如下表1所述，它们的底层技术在虚拟化技术方面的差异。图2显示了虚拟机和Docker容器的体系结构。 表1 虚拟机 Docker 容器 隔离进程级别 硬件 操作系统 操作系统 Separated 共享 启动时间 长 短 资源使用情况 多 少 预构建的镜像 难以寻找和管理 已可用于主服务器 自定义配置镜像 更大，因为包含整个操作系统 更小，只有主机操作系统上的Docker引擎 流动性 易于迁移到新的主机操作系统 销毁和重建替代迁移 创建时间 更长 秒级 相关工作正在开发的虚拟机的使用在组织中很常见。虚拟机广泛用于执行复杂的任务，例如Hadoop。但是，用户甚至使用虚拟机来启动小型的应用程序，这使得系统效率低下。需要启动一个轻量级的应用程序，它更快，使系统更加高效。Docker容器时提供轻量级虚拟化的技术之一，这激励我们执行后台工作。[3]作者从CPU性能，内存吞吐量，磁盘IO和操作速度角度概述了虚拟机和Docker容器的性能评估。[4]作者专注于再HPC集群中实现Docker容器。在本文的后半部分，作者解释了选择容器模型的不同实现方法以及LNPACK和BLAS的使用。在[5]中，作者讨论了轻量级虚拟化的方法，其中解决了容器和unikernel的问题。此外本文还讨论了方差分析检验的统计评估及使用Tukey方法对所收集的数据进行事后比较。作者还讨论了用于比较单核和容器不同基准测试工具。静态HTTP服务器和键值存储参数用于应用程序性能的实现分析，这些分析部署在云上。Nginx服务器用于HTTP性能，为了测量获取和设置操作，使用redis基准测试。[6]中的作者讨论了使用KVM、Docker和OSv的基准测试应用程序的评估。在[7]中，作者讨论了关于虚拟机和容器化技术的简短调查。他还讨论了DOCKER和Docker性能以及各种参数CPU、内存吞吐量、磁盘IO。在[8]中，作者讨论了Docker架构的基本概念，Docker的组件，Docker镜像，Docker register，docker客户端-服务器架构。讨论了Docker和虚拟机的区别。[9]中，作者解释了基于容器的云技术与一组不同参数的性能比较。本文提供了有关基于OpenStack的云部署的使用情况的信息，并考虑进行比较。用于性能比较的平台时docker，LXC和flockport。[10]中的作者讨论了虚拟机和Docker在各种参数（如CPU、网络、磁盘和两个真正的服务器应用程序redis和Mysql）方面的性能比较。 方法论在本节中，使用基准测试工具执行KVM和Docker的评估。以下用于性能评估的基准测试工具时Sysbench，Phoronix，Apache benchmark。这些基准测试工具可以测量CPU性能、内存吞吐量、存储读取和写入、负载测试和操作速度测量。两台HP服务器用于有关各种参数的性能评估，因为其中一台服务器用作安装在主机操作系统之上的虚拟机，而除了此docker引擎之外，guest Os安装在虚拟机之上，另一台服务器作为裸机，主机操作系统Ubuntu 16.04和docker引擎都安装在其上。所有测试均在配备两个因特尔E5-2620 v3处理器的惠普服务器上执行，频率为2.4GHz，共12个内核和64GB的RAM。Ubuntu16.04 64位Linux内核3.10.0用于执行所有测试。为了保持一致性和统一性，使用相同的操作系统。Ubuntu16.04作为Docker容器的基础镜像。为虚拟机配置了12 vCPU和足够的RAM。图3介绍了使用各种基准测试工具不同虚拟化技术的评估方法。 结果和讨论本节讨论虚拟化技术的性能分析。结果分为四个小节。4.1描述所有CPU测量、内展示在4.2和4.3节的存吞吐量和存储读写测量值。4.4节描述负载测试分析。4.5节描述操作速度测量，包括两个测试：八皇后问题和八个谜题问题。最后再4.6节中，执行了统计t检验分析。 CPU性能计算性能可以通过系统在给定时间执行的操作数或特定任务的完成时间来衡量。结果主要取决于分配给服务器的虚拟CPU内核数。测试CPU性能比较是通过以下工具sysbench、phoronix和apache benchmark。 最大质数运算在Sysbench工具测试找出执行最大素数所需的时间，操作的最大质数位50000，时间为60秒，4个线程操作。从下图观察，与VM相比，docker容器执行操作所需时间要少的多。这是由于虚拟机中存在虚拟机管理程序，因此执行需要更多的时间。 7 ZIP 压缩测试7 zip是一款开源的文档存档器，用于将一组文件压缩到称为压缩包的容器中。LZMA基准测试的两个测试，压缩和解压LZMA方法。此测试测量使用7 zip压缩文件所需的时间。用于压缩测试的文件大小为10GB。根据获取的结果，当使用大量文件执行压缩时，Docker容器的性能要比VM好很多。 内存性能RAM speed&#x2F;SMP（对称多处理）是一种缓存和内存基准测试工具，用于测量虚拟化技术（即 Docker 和虚拟机）的 RAM 速度。图 6.表示虚拟化技术之间的 RAM 速度比较。测试RAM速度时，考虑了以下两个主要参数。INTmark 和 FLOATmark 组件用于 RAM 速度 SMP 基准测试工具，该工具可在读取和写入单个数据块时测量最大可能的缓存和内存性能。INTmem和FLOATmem，它们是合成模拟，但与计算的现实世界紧密平衡。每个子测试都由四个子测试（复制，缩放，添加，三元组）组成，以测量内存性能的不同方面。数据从一个内存位置到另一个内存位置的传输是通过复制命令完成的，即（X &#x3D; Y）。写入前的数据修改乘以一定的常量值是通过scale命令完成的，即（X &#x3D; n * Y）。数据从第一个内存位置读取，然后在调用命令 ADD 时从第二个内存位置读取。然后将结果数据放在第三位（X &#x3D; Y + Z）。三元组是添加和缩放的组合。从第一个内存位置读取数据以进行缩放，然后从第二个位置相加以将其写入第三个位置（X &#x3D; n*Y + Z）。图 6.显示相对于 RAM 速度 SMP 测试的内存性能。 磁盘IO性能测试硬盘性能，使用 IOzone 基准测试工具进行性能分析。为了测试系统的写入和读取等操作，使用了1MB的记录大小和4GB的文件大小。从图 7 中可以推断出，与虚拟机相比，Docker 的性能要好得多。VM 的磁盘写入和读取操作减少了 Docker 容器的一半以上（约 54%）。图 7.显示了虚拟机和 Docker 的磁盘性能。 负载测试对于负载测试性能比较，使用Apache基准测试工具，其中它测量给定系统每秒可以容忍的请求数。执行python程序以使用Apache Benchmarking工具测试负载。图 8.表明 VM 的吞吐量分析比 Docker 的吞吐量分析要少得多。这是因为虚拟机中的网络延迟高于 Docker 中的网络延迟。分析表明，Docker容器在每秒处理请求数方面优于虚拟机。 运行速度测量八位女王的问题将八位女王放在8×8的棋盘上，这样他们都没有互相攻击。该测试测量解决问题所需的时间。Eight queen程序是用python编写的，决定了系统的计算性能。图 9.显示了 Docker 和虚拟机的计算性能。根据执行时间，docker 容器解决问题所需的时间更少，而虚拟机需要更长的时间。八拼图测试：拿4×4板，8块瓷砖和一个空的空间。使用空白空间，排列与最终配置相匹配的图块数量是主要目标。它可以将四个相邻的操作（右，左，下和上）磁贴滑入空白区域 - 该测试测量解决问题所需的时间。Eight拼图程序是用python编写的，决定了系统的计算性能。图 10.显示了 Docker 和虚拟机的计算性能。根据执行时间，docker 容器解决问题所需的时间更少，而虚拟机需要更长的时间。 t-test分析t 检验统计测量用于确定两个组的方法之间是否存在关键区别，这两个组可能在相关特征中连接。在下面的结果演示中，统计推理技术 t-test 已被用于证明 docker 容器的性能显著优于虚拟机。此外，阈值的置信水平α为 0.05。两组数据的概率可以通过取 t 统计量、t 分布值和自由度来确定。在分析部分使用原假设 （H0） 和备择假设 （H1） 约定。在进行检验分析时，考虑原假设为真，这是进一步统计分析的假设。然后，测试的结果可以证明，如果 H0 为真，则假设可能是错误的。需要测试 Docker 容器执行操作所需的时间是否比虚拟机少。样本检验包括 10 个关于查找素数操作的实验。表3显示了为分析部分采集的10个样本。发现执行虚拟机操作所花费的平均时间为 153.5 秒。现在需要检查 Docker 容器的平均时间是否小于虚拟机。t 统计量 （t） 的等式为 。 结论Docker 容器是一种新兴的轻量级虚拟化技术。这项工作评估了两种虚拟化技术，即 Docker 容器和虚拟机。在虚拟机和基于 Docker 容器的主机上，从 CPU 性能、内存吞吐量、磁盘 I&#x2F;O、负载测试和操作速度测量等方面进行性能评估。据观察，Docker容器在每次测试中的表现都优于VM，因为虚拟机中存在QEMU层使其效率低于Docker容器。容器和虚拟机的性能评估是使用Sysbench，Phoronix和apache基准测试等基准测试工具执行的。作为未来的工作，我们计划在Docker中调度容器，同时开发更安全的容器变体，这将减少安全约束。 References[1] Docker. https://docs.docker.com/ 2019 [Online; Accessed 24-03-2019][2] Shivaraj Kengond, DG Narayan and Mohammed Moin Mulla (2018) “Hadoop as a Service in OpenStack” in Emerging Research in Electronics, Computer Science and Technology , pp 223-233.[3] C. G. Kominos, N. Seyvet and K. Vandikas, (2017) “Bare-metal, virtual machines and containers in OpenStack” 20th Conference on Innovations in Clouds, Internet and Networks (ICIN), Paris, pp. 36-43.[4] Higgins J., Holmes V and Venters C. (2015) “Orchestrating Docker Containers in the HPC Environment”. In: Kunkel J., Ludwig T. (eds) High Performance Computing. Lecture Notes in Computer Science, vol 9137.pp 506-513[5] Max Plauth, Lena Feinbube and Andreas Polze, (2017) “A Performance Evaluation of Lightweight Approaches to Virtualization”, CLOUD COMPUTING: The Eighth International Conference on Cloud Computing, GRIDs, and Virtualization.[6] Kyoung-Taek Seo, Hyun-Seo Hwang, Il-Young Moon, Oh-Young Kwon and Byeong-Jun Kim “Performance Comparison Analysis of Linux Container and Virtual Machine for Building Cloud,” Advanced Science and Technology Letters Vol.66, pp.105-111.[7] R. Morabito, J. Kjällman and M. Komu, (2015) “Hypervisors vs. Lightweight Virtualization: A Performance Comparison”, IEEE International Conference on Cloud Engineering, Tempe, AZ, pp. 386-393.[8] Babak Bashari Rad, Harrison John Bhatti and Mohammad Ahmadi (2017) “An Introduction to Docker and Analysis of its Performance” IJCSNS International Journal of Computer Science and Network Security, VOL.17 No.3, pp 228-229.[9] Kozhirbayev, Zhanibek, and Richard O. Sinnott. (2017) “A performance comparison of container-based technologies for the cloud”, Future Generation Computer Systems, pp: 175-182.[10] Felter, Wes, et al. (2015) “An updated performance comparison of virtual machines and linux containers”, IEEE international symposium on performance analysis of systems and software (ISPASS). pp:171-172.[11] Sysbench. https://wiki.gentoo.org/wiki/Sysbench 2019 [Online; Accessed 24-03-2019][12] Phoronix Benchmark tool. https://www.phoronix-test-suite.com/ 2019 [Online; Accessed 24-03-2019][13] Apache Benchmark tool. https://www.tutorialspoint.com/apache_bench/ 2019 [Online; Accessed 24-03-2019]","categories":[{"name":"Docker","slug":"Docker","permalink":"http://jerryblogs.com/categories/Docker/"}],"tags":[{"name":"Docker","slug":"Docker","permalink":"http://jerryblogs.com/tags/Docker/"}]},{"title":"kubernetes中的网络策略：性能评估和安全分析","slug":"k8s-network-policy","date":"2022-05-25T10:31:00.000Z","updated":"2023-04-21T20:56:22.191Z","comments":true,"path":"2022/05/25/k8s-network-policy/","link":"","permalink":"http://jerryblogs.com/2022/05/25/k8s-network-policy/","excerpt":"","text":"文章信息作者：Gerald Budigiri; Christoph Baumann; Jan Tobias Mühlberg; Eddy Truyen; Wouter Joosen原文：https://ieeexplore.ieee.org/document/9482526 摘要具有超高可靠性要求和低延迟要求的5G应用需要在移动网络中采用边缘计算解决方案。根据终端用户和第三方公司的需求，像k8s这样的容器编排框架已经进一步成为动态部署边缘应用的首选标准。不幸的是，复杂的网络和安全问题被强调为阻碍行业成功采用容器技术的挑战。安全挑战因（错误）概念而加剧，即安全的荣期间通信以牺牲性能为代价，但是这样中需求对于5G边缘计算用例来说都至关重要。为了追求低开销的安全方案，本论文研究网络策略，即k8s用于控制租户之间网络隔离的概念。我们评估Calico和Cilium基于ebpf的解决方案的性能开销，分析网络策略的安全性，突出网络策略的安全威胁，并概述相应的先进解决方案。我们的评估表明，网络策略时适合低延迟容器间通信的低开销安全解决方案。 I.前言5G的出现为几个新的超可靠低延迟通信（URLLC）应用和用例打开了大门，如虚拟&#x2F;增强现实（VR &#x2F; AR），车辆到一切（V2X）和远程手术（RS），有望显着增加对计算和通信资源的需求。即使硬件功能最近有所进步，这些应用的严格性能要求仍然无法与实际的设备和网络功能相匹配[1]。为了解决这种不匹配问题，5G提供商采用了边缘计算（边缘计算是指在用户或数据源的物理位置或附近进行的计算，这样可以降低延迟，节省带宽。）和网络功能虚拟化（NFV）等几种技术概念。借助NFV，边缘计算平台虚拟化了网络功能模块，并将云计算能力扩展到终端用户附近的边缘设备，从而在多租户边缘生态系统中提供灵活性和敏捷性。云原生计算趋势满足了这一发展，其中应用程序由使用无状态API相互交互的微服务组成。Kubernetes等灵活的编排框架之上使用轻量级和可以指的容器，而不是更资源密集型和启动缓慢基于VM的解决方案，使得云原生网络功能（CNF）成为5G边缘计算URLLC应用的更好选择。现在微服务架构正在被研究为边缘NFV的可能解决方案。然而，复杂的网络和安全问题被强调为工业中容器采用的挑战。在多租户边缘计算云环境中，潜在的安全漏洞具有更高的严重性，在这些环境中，需要隔离属于不同房的微服务，以便他们只能在必要时进行交互。此外，对于RS和V2X等任务关键性5G应用，通信性能和安全性都不应受到影响。因此，必须为容器间通信找到低开销的安全解决方案。虽然行业和研究界已经做出了许多努力，来提高容器安全性，但其中大多数工作都集中在容器镜像，运行时，内核操作系统和k8s配置上，很少或根本没有考虑低延迟应用程序中的网络安全问题。k8s提供允许限制容器间通信的网络策略。虽然缺乏入侵检测等现代防火墙的高级功能，但不同网络策略仍然提供了合理的网络安全水平。在多租户环境中，它们通过将流量限制为仅允许互相通信的微服务，在租户之间提供可配置的网络隔离。虽然通过k8s网络策略API进行定义，但是策略的实施由自定义的容器网络接口（CNI）插件处理。之前的工作比较了不同CNI插件，并讨论了他们在多租户中的使用。相比之下，本文首次专门研究了k8s网络策略的性能开销和安全影响，并探讨了网络策略在边缘保护URLLC应用程序的适用性。我们做出以下贡献： 性能：我们评估了合适的CNI插件并且表明CNI插件的网络策略产生的性能开销可以忽略不计，该开销仅随策略数量和不同策略配置方式而略有不同。 安全：我们定义一个针对网络策略的攻击模型并且分析相应的威胁和漏洞。此外我们提出了合适的最先进的低延迟解决方案，以应对这些威胁。 II.背景与动机在介绍这些结果之前，我们概述了k8s的网络策略的技术基础，功能和挑战。本节介绍k8s、k8s中的网络策略，并且为什么它们在多租户边缘平台中的重要性。此外，我们就CNI插件和网络策略支持进行比较，并解释Calico中的策略实施。 A.Kubernetes(k8s)k8s是容器编排的事实标准，它支持部署、拓展和管理容器化微服务应用程序并提供网络概念和对大规模互联微服务的支持。单个微服务应用程序在容器中运行，一个或多个紧密耦合的容器在Pod中运行，而pod在节点上运行，通过k8s主节点上运行的API服务进行控制。由于Pod是临时的，在拓展过程中动态启动或终止，因此服务对象用于为pod提供稳定的端点。此类和其他配置对象与控制k8s的编排过程的状态信息一起存储在分布式etcd数据库中。 B.kubernetes 网络策略在多租户平台，保护边缘用户的隐私是重要的。尽管k8s提供集群范围的命名空间用来提供隔离以进行管理和资源配额管理，单词支持不足以避免网络遍历。然而缺乏足够的网络分段是是一个一个经常被引用的高风险漏洞，该漏洞已被Equifax数据泄露等大型网络犯罪所利用。网络策略通过显式声明允许和拒绝的连接，帮助提供限制pod之间（在和&#x2F;或跨命名空间）以及pod与外部网络之间的流量所需的护栏。网络策略规范由podSelector组成，用于指定将受策略约束的pod，以及用于指定策略类型（入口或出口）的策略类型。入口规则指定允许入站的流量到目标Pod，出口规则指定允许的来自目标pod的出站流量。每个规则都由一个NetworkPolicyPeer组成，用于通过无类域间路由（CIDR）表示法选择连接另一端允许流量的pod，该表示法指定IP地址块，命名空间或pod标签选择一个NetworkPolicyPort，它允许显示指定可能与Pod通信的端口或网络协议。网络策略可能是累加的，如果多个策略选择一个Pod，则流量将限制在这些策略的入口&#x2F;出口规则并集所允许的范围内。 CNI插件和网络模式由于没有内置功能来实施网络策略，k8s依靠CNI插件来实施。CNI插件作为附加组件安装，该附加组件可以作为容器运行，也可以依赖开源k8s组件。虽然存在许多CNI插件，但只有Calico和Cilium支持使用ebpf实施网络策略，ebpf是iptables的高性能替代方案。事实上，再将ebpf与iptables进行比较时，我们观察到延迟减轻了0.7到0.8倍，节点间和节点内场景的吞吐量分别提高了3.5倍和1.2倍。测量是在封闭实验室的OpenStack测试平台上使用Calico进行的（参见III-A和III-B部分，了解测试平台的实验设置和规格）。Calico和Cilium都使用流量控制钩子，在Pod的虚拟以太网接口的入口和出口处附加ebpf程序。根据主机模式评估Calico ebpf和Cilium ebpf性能，在该模式下，基准测试在OpenStack VM实例上运行。结果表明，尽管ebpf数据平面有优势，但是CNI插件（可能还有整个K8s架构）的开销仍然很大，特别是对于节点间的通信。结果进一步表明，Calico ebpf在节点间场景中的表现明显优于Cilium ebpf。这是因为Cilium默认以隧道模式运行，这会引发封装标头，而Calico在每个节点上使用Linux内核路由支持来提供纯第三层网络解决方案，从而降低开销。因此，Calico ebpf被用于本文其余性能评估。Calico和主机节点内测量在裸机上重复，其中主机结果在延迟和吞吐量方面分别高出8.7%和10.7%，而OpenStack则为33%和31%。OpenStack上较高的开销表示VM的基于OpenStack的网络配置与Pod基于CNI的网络配置之间可能存在负面干扰。 D.Calico eBPF 策略实现Calico CNI插件将Felix守护程序部署到每个节点，该守护进程将内核路由编程到本地Pod并且使用边界网关协议分发路由信息。Felix通过API服务器从K8s etcd获取网络策略定义，并且将他们转换为BPF程序，这些程序被加载到内核中，并附加到每个Pod的veth接口的tc钩子上。只有来自Pod标签匹配的网络策略规则才会被附加，从而实现可拓展的实现，Linux的conntrack（Linux内核网络栈的一个核心功能，允许内核跟踪所有的逻辑网络连接或流）对此进行了强化（见本目录下另一篇关于conntrack的文章）。Calico还以自定义的资源定义（CRD）的形式实现自己的网络策略模型，该模型提供了拓展的操作范围，例如根据其顺序确定规则的优先级。Felix进一步使用快速数据路径（xDP）在节点上实现数据包过滤，以防止拒绝服务（DoS）攻击。 E.多租户网络平台中的网络策略基于微服务的应用程序可以包含数百个微服务，像netfilter这样的公司使用500多个微服务来运行其电影流系统。在多租户边缘平台中，不同的应用程序可能来自不同的提供商，需要强大网络隔离，因此必须遵循最小权限原则，为微服务之间每个预期的租户件交互显式指定网络策略，该原则旨在减少攻击者的安全攻击接口。同样所有租户内容器通信都需要显式允许策略。因此多租户平台中的策略数量可能会随着租户应用程序的数量和大小急剧增加。因此，对于5G边缘部署，不仅要对k8s的网路性能进行基准测试，还要评估网络策略的性能开销。 III.间接费用考核本节总结了各种条件下节点内和节点间场景下Calico网络策略的网络时延开销的评估结果。 A.基准和架构评估我们使用netperf的TCP流和请求-响应模式分别进行吞吐量和端到端的延迟测量。我们将netperf配置为测试长度为120秒，目标是99%的置信度，即测量的平均值在实际平均值的正负2.5%以内。我们不报告吞吐量和CPU利用率结果，因为他们遵循与延迟相同的模式。 B.实验设置用于运行所有实验的测试平台是私有openStack云（Liberty版本）的隔离部分，如图3所示。我们从公共边缘云中使用OpenStack开始，通常最好在VM中运行容器以保护云提供商的资产。OpenStack云由主从架构组成，具有两台控制器机器，以及可调度虚拟机的droplets，这些droplets具有Inter（R）Xeon（R）CPUE5-2650 2.00GHz 处理器和带有Ubuntu xenial的64GBDIMM DDR3内存，每个droplets都有两个10Gbit网络接口。这些droplets有16个CPU内核，其中2个是为OpenStack云的操作保留的。有一个主节点和两个工作节点组成的k8s集群是使用kubeadm部署的，运行k8s版本1.19.2。所有节点都有4个vCPU和8GBRAM，并且部署在同一个物理droplets上，以消除网络延迟的变化。此外，每个节点的vCPU内核专门固定在属于droplets同一主板插槽的物理内核上。 增加策略数量的效果：我们创建了20个命名空间，每个命名空间包含5个微服务（Pod）。三个Pod用于性能测量；一个本地pod和两个远程Pod（每个场景一个，即节点内和节点间）并且所有的pod都被分配了相同数量的策略。Calico提供了一个policyOrder的功能，用于确定策略实施的优先级，较低的顺序优先。最低顺序是不允许任何Pod间流量的策略，然后是允许与不相关的Pod进行通信的策略，直到达到所需数量的策略。然后将最高顺序分配给允许三个选定Pod之间通信的策略。然后，网络策略的数量从零到2000不等，结果如图四所示（accross 100 Pod）。对于所有已评估数量的网络策略，我们观察到对性能的显著影响。 网络策略：表1涵盖了各种k8s网络策略功能的不同策略配方。虽然对表中编号5、11、13和14的每个配方都进行了单独评估，但是仅对具有相似结构的配方（即2、8、9和12）进行了一次测试。对于其余的配方，即1、3、4、6、7和10，拥有许多的策略是不合理的，因为对于GlobalNetworkPolicy，这些策略中所需的流量隔离功能可以通过命名空间中的一个策略甚至整个集群中的一个策略来实现。评估提供了与图四（100个Pod）中观察到的结果是类似结果。 # Deny traffic # Allow traffic 1 to a pod 8 to a pod 2 limiting traffic to a pod 9 to a pod from all NSs 3 to a NS 10 from a NS 4 from other NSs 11 from pods in another NS 5 from a pod 12 from external clients 6 non-whitelisted from a NS 13 only to a port 7 external egress 14 using multiple selectors C.网络策略的可拓展性从上述所有评估中可以看出，在执行网络策略方面没有发现明显的开销。我们可以将此归因于分散的Calico实现，其中仅在每个pod的veth接口上评估相关策略。即使集群中可能有2000个策略，也只会在那些选择器与通信pod匹配的策略才会在这些pod上进行评估。此外Calico的order功能是在策略创建或修改时而不是数据包时强制执行的。为了评估网络策略的可伸缩性，我们在集群中部署了三个pod，并对其进行了标记，使其与集群中所有策略相匹配。如图四的结果（accross 3 pod）仍未显示显著的策略开销，至少对于节点内流量而言。这表明Calico使用ebpf来实施网络策略是可拓展的。其原因可能是Calico进检查新流中第一个数据包的策略之后，conntrack会自动允许同一个流的后续数据包而不是重新检查每个数据包。 IV.网络策略安全分析网络策略限制跨不同pod的容器之间的入站和出站网络连接，拒绝任何未明确允许的连接。下面我们将讨论在同一k8s集群中托管不同边缘应用程序的微服务边缘平台提供商所感知的容器通信网络策略的安全隐患与挑战。 A.攻击模型我们认为攻击者已破坏其中一个边缘应用程序命名空间的微服务。这可能是由恶意或受损的边缘应用程序提供商、通过破坏在平台上部署服务的软件供应链或利用外部可访问微服务中的漏洞来实现的。假设受感染的应用程序在非特权（非root）容器中运行，则攻击者会受到适用于该容器及其相应Pod的功能和资源限制的限制。攻击者仍可以执行任意代码并访问容器的所有允许的内存区域、文件和网络连接。因此，攻击者可能会连接到集群中其他可访问的微服务，并利用任何存在的漏洞来破坏他们。另一方面，我们要求集群虚拟化基础架构值得信赖且经过充分强化，以便攻击者无法逃离其容器沙箱并获得其托管节点上的root权限。攻击者的目标是通过破坏其他服务（尤其是那些有权访问敏感信息或任务关键性服务以破坏其功能的服务）在集群中传播。次要目标是不被发现。然后，了解任何Pod的网络策略以及相应的可访问子网可以帮助攻击者避免触发警报。从这个意义上说，适用于集群的网络策略可以被视为敏感信息。 B.网络策略的安全影响如果根据最小权限原则定义的策略，仅允许必要的连接，则网络策略是阻止上述攻击者的有效工具。由此产生的网络分段减少了攻击面，保护了可能易受攻击的共同托管的应用程序，并防止任何横向攻击者移动。然而，网络策略并不是灵丹妙药。攻击者仍可以滥用允许的连接，并以相应的微服务为目标，以实现远程执行代码或泄露敏感信息。虽然策略允许阻止与某些端口的连接，但表1中的简单规则不对允许的通信节点之间的流量提供任何细粒度的控制。其他解决方案，如网络监控和异常检测，可用于检测和阻止此类攻击。识别出可疑参与者后，可以使用网络策略将其隔离在集群的隔离部分中。但是新颁布的网络策略仅对新连接强制执行，不会切换现有的连接。 C.网络策略安全漏洞和威胁使用网络策略本身有几个陷阱。下面我们报告了常见的漏洞和潜在攻击以及文献中可用的补救措施。 1）：配置错误：StackRox的一项调查显示，91%的K8s用户遇到过安全问题，其中67%归因于配置错误。此外配置错误仍被认为是云中数据泄露的主要原因。配置网络策略时，存在大量机会，这些配置村在大量薄弱或错误的机会，这些配置无疑会威胁到容器网络隔离。这些包括错误的Pod标签规范、拼写错误等手动错误，甚至在编写后完全忘记强制实施网络策略。如果禁用了网络策略，k8s不会发出警告，而只是接受并静默忽略配置。 最先进的解决方案：开放策略代理（OPA）通过在Pod创建期间通过自定义准入控制器对网络策略强制实施适当的OPA要求，有助于减低错误配置风险，例如网络策略中的拼写错误或疏忽、忘记强制执行甚至错误删除他们。但是，OPA不会验证配置的策略要求是否建立更高级别的安全目标，如网络隔离。 2）弱策略设计：所有安全策略（包括但不限于网络策略）都需要根据最小权限和零信任等原则进行设计和配置。应明确允许容器之间的通信，并将其保持在应用程序完成其工作所需的最低限度。例如，不应无意中向只需要从数据库读取的应用程序授予写入权限。指定网络策略时，应为应用程序容器提供一组权限，每组权限都是最小化为服务所需最小权限集。允许策略应允许最低限度，例如通过NerworkPolicyPort，而拒绝策略应该是最大程度地阻止所有未经授权的流量。但是，编写最低特权策略是一项艰巨地任务，因为k8s中典型应用程序需要服务之间、从外部（入口）到外部端点（出口）的数百个连接。 最先进的解决方案：实施网络策略时规则中，OPA可以将受保护的容器允许入口规则限制为特定值，以避免授予其他容器访问权限。BASTION通过确保容器的连接仅限于自身与组成服务所需的容器之间的相互依赖关系，对容器强制实施最低特权网络访问。 3）租户管理员：虽然边缘基础结构通常是受信任的，但是租户及其管理员可能会提出某些风险。具有足够权限的流氓管理员可能会与攻击者串通，以削弱网络集群策略和隔离，从而悄悄地允许非法网络连接。特别是，常规k8s网络策略绑定到租户空间，并且可以由租户管理员更改。 最先进的解决方案：在多租户环境中，必须遵循控制平面中的身份验证、访问控制和审核地最佳实践，以限制恶意租户和内部攻击的影响。租户管理员更改集群策略的问题在Calico中通过全局策略和优先级的概念得到解决。与配置错误一样，OPA还提供了防止使用非法网络策略启动容器的保护。 4）特权网络：不幸的是，容器通常以不安全的设置运行，授予不必要或无意的权限，从而给节点操作系统和节点上其他容器带来风险。网络特权容器直接在LAN网络上公开，绕过Pod网络接口和网络策略。这样容器可以通过分配给他们的网关IP地址或通过猜测相应的IP地址来访问其他节点，从而损害所需的网络隔离。 最先进的解决方案：[4]中介绍了一个高性能的安全实施网络堆栈，它通过对共享主机网络命名空间的启用了网络特权的容器实施细粒度访问控制来解决此漏洞。此外尽管Calico不提供安全机制来防止主机网络命名空间滥用，但它可以防止特权容器滥用虚拟网关IP地址。Pod安全策略可确保不会生成具有无意功能或访问权限的容器。他们也可以由OPA强制执行。 5）易受攻击的实现：在k8s中，网络策略的实施取决于CNI插件组件。这些组件及其依赖项本身可能包含漏洞，这些漏洞可能会危及策略实施或允许攻击者拦截和重定向网络流量，例如CVE-2019-9946、CVE-2020-10749 和 CVE-2020-13597。 最先进的解决方案：使软件保持最新状态可以减轻漏洞的影响，但不能解决根本问题。形式验证技术和安全的编程语言有助于提高软件的安全性。 6）ebpf漏洞利用：在我们的设置中，Calico&#x2F;node代理将网络策略转换为ebpf程序以进行实施。因此，需要在节点上启用ebpf内核功能才能使用网络策略，这可能会增加节点的攻击面。在Linux4.4之前，所有bpf()命令都要求调用方具有CAP SYS ADMIN功能，然而，目前非特权用户可以通过将有限的ebpf程序附加到他们拥有的套接字来创建和运行这些程序。如 CVE-2020-8835 中所示，此功能不仅被利用来在内核内存中执行越界读取和写入，而且还被利用来实现权限提升 [31]。 最先进的解决方案：通过将内核完全禁用bpf()系统调用的非特权访问，将内核无特权bpf禁用的系统设置为1，可以缓解此类威胁。ebpf内核验证器可以限制从cbpf程序访问哪些内核函数和数据结构。Seccomp-BPF限制了用户空间程序可用的系统调用和参数集。 7）泄露网络策略：如上所述，对于热衷于避免检测的攻击者来说，网络策略本身可能是最有价值的信息。获取它们的最简单的方法是从k8s API服务器或etcd请求他们，但用户平面攻击者通常无法访问这些API。具有监视控制平面流量能力的对手可以获取配置信息，例如网络策略（如果以明文形式发送）。攻击者还需要通过诸如[31]之类的漏洞利用来提供额外的内存读取功能以便从本地Calico&#x2F;node代理或ebpf内核工具泄露策略。无特权攻击者可能会探测网络以推断允许的连接，但这可能会破坏保持隐身的最初目标。 最先进的解决方案：如上所述，泄露策略信息的原始尝试很容易受到最佳实践的阻碍，例如为控制平面启用相互传输层安全加密，身份验证和RBAC。尽管Calico支持TLS来保护Calico组件和控制平面通信，但默认情况下，大多数提供的清单中都未配置安全性，以使Calico部署变得更容易。对于Calico，建议使用TLS与其数据存储进行通信，同时使用客户端整数和JSON Web令牌身份验证模块。出于机密性和完整性的原因，应考虑在可信执行环境（TEE）（如英特尔SGX）中运行任务关键型应用和控制平面组件，以便在每个容器应用之间的节点中创建隔离的安全区环境。英特尔SGX安全区通过加密内存隔离驻留在容器中的应用，从而保护应用内存免受恶意或特权容器以及受损节点操作系统的影响。虽然SGX在执行安全区代码时会产生性能开销，但其应用于控制平面组件以保护网络策略的机密性和完整性不会给用户平面流量增加任何开销。 V.结语边缘计算和容器化是多租户5G云环境中无疑重要的技术。然而，安全要求仍然阻碍其广泛采用，特别是在具有高性能和安全要求的5G URLLC边缘应用中，因为大多数安全解决方案都会损害性能。这使得找到一个低开销的容器安全解决方案变得至关重要。在评估了网络策略（k8s的可配置网络隔离安全解决方案）并且没有观察到明显的性能开销之后，我们证明了他们是此类应用程序的合适的安全解决方案。作为一般关注点，交互微服务应部署在同一节点上以确保最高性能，但是这会增加对不同租户的适当隔离的需求。 Acknowledgment这项研究的部分资金来自KU Leuven研究基金和弗兰德研究计划网络安全。这项研究已获得欧盟H2020 MSCA-ITN行动5GhOSTS的资助，赠款协议号814035。 引用[1] Q.-V. Pham et al., “A survey of multi-access edge computing in 5Gand beyond: Fundamentals, technology integration, and state-of-the-art,”IEEE Access, 2020.[2] H. Hawilo et al., “Exploring microservices as the architecture of choicefor network function virtualization platforms,” IEEE Network, 2019.[3] J. Watada, A. Roy, R. Kadikar, H. Pham, and B. Xu, “Emerging trends,techniques and open issues of containerization,” IEEE Access, 2019.[4] J. Nam, S. Lee, H. Seo, P. Porras, V. Yegneswaran, and S. Shin, “BASTION: A security enforcement network stack for container networks,”in 2020 USENIX Annual Technical Conf. (USENIXATC 20), 2020.[5] S. Sultan, I. Ahmad, and T. Dimitriou, “Container security: Issues,challenges, and the road ahead,” IEEE Access, 2019.[6] M. Souppaya, J. Morello, and K. Scarfone, “Application containersecurity guide (2nd draft),” NIST, Tech. Rep., 2017.[7] E. Reshetova, J. Karhunen, T. Nyman, and N. Asokan, “Security of oslevel virtualization technologies: Tech. report,” arXiv:1407.4245, 2014.[8] A. Grattafiori, “Understanding and hardening linux containers,”Whitepaper, NCC Group, 2016.[9] S. Vaucher, R. Pires, P. Felber, M. Pasin, V. Schiavoni, and C. Fetzer,“SGX-aware container orchestration for heterogeneous clusters,” in 2018IEEE 38th Conf. on Dist. Computing Syst. (ICDCS). IEEE, 2018.[10] S. Arnautov et al., “SCONE: Secure linux containers with intel SGX,”in 12th USENIX OSDI 16, 2016.[11] M. S. I. Shamim, F. A. Bhuiyan, and A. Rahman, “XI commandmentsof Kubernetes security: A systematization of knowledge related toKubernetes security practices,” in 2020 IEEE SecDev. IEEE, 2020.[12] A. Balkan, “Kubernetes network policy recipes,” https://github.com/ahmetb&#x2F;kubernetes-network-policy-recipes&#x2F;, [accessed 2021-01-28].[13] S. Qi et al., “Understanding container network interface plugins: designconsiderations and performance,” in 2020 IEEE Int. Symp. on Local andMetropolitan Area Networks (LANMAN). IEEE, 2020.[14] X. Nguyen, “Network isolation for K8s hard multi-tenancy,” 2020.[Online]. Available: https://aaltodoc.aalto.fi/handle/123456789/46078[15] “Kubernetes namespaces,” https://kubernetes.io/blog/2016/08/kubernetes-namespaces-use-cases-insights&#x2F;, [accessed 2020-11-24].[16] “How to stop the next equifax-style megabreach,” https://www.wired.com&#x2F;story&#x2F;how-to-stop-breaches-equifax&#x2F;, 2017, [accessed 2021-01-14].[17] “About eBPF,” https://docs.projectcalico.org/about/about-ebpf/, [accessed 2020-12-18].[18] “eBPF datapath,” https://docs.cilium.io/en/v1.9/concepts/ebpf/, 2020,[accessed 2020-12-18].[19] CED, “How it’s built,” https://medium.com/the-andela-way/how-itsbuilt-netflix-8f62ab329011/, 2019, [accessed 2020-12-18].[20] “Care and Feeding of Netperf 2.7.X,” https://hewlettpackard.github.io/netperf&#x2F;doc&#x2F;netperf.html&#x2F;, [accessed 2020-10-23].[21] https://github.com/falcosecurity/falco/, 2020, [accessed 2021-01-27].[22] C.-W. Tien et al., “KubAnomaly: Anomaly detection for Docker orchestration platform with neural network approaches,” Eng. Reports, 2019.[23] “Kubernetes adoption, security, and market share trends report,”https://www.stackrox.com/kubernetes-adoption-security-and-marketshare-for-containers/, 2020, [accessed 2020-12-18].[24] F. Truta, “Misconfig. remains #1 cause of data breaches in the cloud,”https://securityboulevard.com/2020/04/misconfiguration-remains-the-1-cause-of-data-breaches-in-the-cloud&#x2F;, [accessed 2021-01-14].[25] M. Ahmed, “How to enforce Kubernetes network security policiesusing OPA,” https://www.magalix.com/blog/how-to-enforce-kubernetesnetwork-security-policies-using-opa/, 2020, [accessed 2020-11-02].[26] D. Monica, “Least priv. cont. orchestration,” https://www.docker.com/ ´blog&#x2F;least-privilege-container-orchestration&#x2F;, [accessed 2020-12-17].[27] “Zero-trust networks in Kubernetes, cloud-native applications,”https://www.stackrox.com/wiki/zero-trust-networks-in-kubernetescloud-native-applications/, 2020, [accessed 2020-12-17].[28] “Pod security policies,” https://kubernetes.io/docs/concepts/policy/podsecurity-policy/#host-namespaces/, 2020, [accessed 2021-01-27].[29] M. Ahmed, “Enforce pod security policies in Kubernetes usingcommit d72943c30b7562b19cc450936892fedf7adbdc98Author: dreamcoin1998 &#x67;&#97;&#111;&#106;&#x75;&#110;&#x62;&#x69;&#x6e;&#57;&#56;&#64;&#103;&#x6d;&#97;&#x69;&#108;&#x2e;&#x63;&#x6f;&#x6d;Date: Wed May 25 18:31:00 2022 +0800 增加kubernetes网络策略：性能评估和安全分析翻译 …skipping…OPA,” https://www.magalix.com/blog/enforce-pod-security-policies-inkubernetes-using-opa/, 2020, [accessed 2021-01-27].[30] “bpf(2) - linux manual page,” https://man7.org/linux/man-pages/man2/bpf.2.html&#x2F;, 2020, [accessed 2020-12-21].[31] P. Manfred, “Kernel privilege escalation via improper eBPF programverification,” https://www.thezdi.com/, 2020, [accessed 2020-12-17].[32] M. Fleming, “A thorough introduction to ebpf,” https://lwn.net/Articles/740157&#x2F;, 2017, [accessed 2020-12-21].[33] “Using eBPF in Kubernetes,” https://kubernetes.io/blog/2017/12/usingebpf-in-kubernetes/, 2017, [accessed 2021-01-28].[34] “Customize the manifests,” https://docs.projectcalico.org/getting-started/kubernetes&#x2F;installation&#x2F;config-options&#x2F;, 2020, [accessed 2020-12-01].[35] “Configure encryption and authentication,” https://docs.projectcalico.org&#x2F;security&#x2F;comms&#x2F;crypto-auth&#x2F;, 2020, [accessed 2020-12-01].[36] A. Gowda and D. Coulter, “Microsoft Docs:Enclave aware containers onAzure ,” https://docs.microsoft.com/en-us/azure/confidential-computing/enclave-aware-containers&#x2F;, 2020, [accessed 2020-10-30].[37] P.-L. Aublin et al., “TaLoS: Secure and transparent TLS terminationinside SGX enclaves,” Imperial College London, Tech. Rep, vol. 5, 2017.","categories":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"http://jerryblogs.com/categories/Kubernetes/"},{"name":"论文翻译","slug":"Kubernetes/论文翻译","permalink":"http://jerryblogs.com/categories/Kubernetes/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/"}],"tags":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"http://jerryblogs.com/tags/Kubernetes/"}]},{"title":"Linux Conntrack为什么会崩溃并避免问题","slug":"linux-conntrack","date":"2022-05-25T10:31:00.000Z","updated":"2023-04-20T13:39:41.461Z","comments":true,"path":"2022/05/25/linux-conntrack/","link":"","permalink":"http://jerryblogs.com/2022/05/25/linux-conntrack/","excerpt":"","text":"文章信息作者：Alex Pollitt原文：https://www.tigera.io/blog/when-linux-conntrack-is-no-longer-your-friend/ 连接跟踪（“conntrack”）是Linux内核网络堆栈的核心功能。它允许内核跟踪所有逻辑网络连接或流，从而识别构成每个流的所有数据包，以便可以一致的处理它们。 Conntrack是一个重要的内核特性，它支持一些关键的主线用例： NAT依赖于连接跟踪信息，因此它可以以相同的方式转换流中的所有数据包。例如当pod访问kubernetes服务时，kube-proxy的负载均衡使用NAT将连接重定向到特定的后端pod。Conntrack记录对于特定连接，发往服务IP的数据包应该全部发送到同一个后端Pod，并且从后端pod返回的数据包未经NAT回到源pod。 有状态的防火墙，例如Calico，依靠连接跟踪信息来精确将“响应”流量列入白名单。这允许您编写一个网络策略，上面写着“允许我的pod连接到任何远程IP”，而无需编写策略来明确允许响应流量。（否则，将不得不提娜佳更不安全的规则“允许任何IP到我的pod的数据包”） 此外conntrack通常会提高性能（减少CPU并减少数据包延迟），因为只有流中第一个数据包需要经过完整网络对战处理才能确定如何处理它。有关此操作的一个示例，请参阅“comparing kube-proxy modes”博客。 可是，conntrack有它自己的限制… 它在哪里崩溃conntrack表有一个可配置的最大大小，如果它被填满，连接通常会开始拒绝或丢弃。对于大多数工作负载，表有足够的空间，这永远不会成为问题。但是在某些情况下，需要对conntrack表进行更多的考虑： 最明显的情况是，服务器处理极高数量的同时活动的连接。例如，如果你的conntrack表配置为128k条目，但是有超过128k的同时连接，肯定会遇到问题。 稍微不那么明显的情况是，服务器每秒处理非常多的连接数。即使连接是短暂的，Linux也会在很短的超时时间内（默认为120秒）继续跟踪链接。例如如果您的conntrack表配置为128k条目，并且您尝试每秒处理1100个连接，那么即使连接时间非常短暂（128k &#x2F; 120s &#x3D; 1092个连接&#x2F;s） 有一些小众的工作负载属于这种类型。此外如果你出在一个敌对的环境中，那么用大量的半开放连接来淹没你的服务器，可以作为一种拒绝服务的攻击。在这里两种情况下，通过增加conntrack表的大小或减少conntrack超时，调整conntrack可能足以满足你的需求，但是如果你把调整弄错了，会导致很多痛苦。对于其他情况，你需要绕过conntrack来处理违规的流量。 一个真实的例子与我们合作的一个大型Saas供应商有一组运行在裸机服务器上的memcached服务器（没有虚拟化或容器化），每个服务器每秒处理5万多个短时连接。这远远超过了标准Linux配置所能应付的范围。他们曾尝试调整conntrack配置以增加表大小并减少超时，但调整很脆弱，增加的RAM使用是一个重大损失（想想GBytes），而且连接是如此的短暂，以至于conntrack没有提供其通常的性能优势（减少CPU或数据包延迟）。相反，他们转向了Calico。Calico的网络策略允许绕过特定流量的conntrack（使用doNotTrack的标志）。为他们提供了所需的性能，以及Calico带来的额外安全优势。 绕过conntrack的取舍是什么？ 不跟踪网络策略通常是对称的。在saas提供商的案例中，他们的工作负载是内部的，因此使用网络策略，他们可以非常狭隘地将允许访问memcached服务的所有工作负载的往来流量列为白名单 不跟踪策略对连接的方向是无视的。所以在memcached服务器被攻击的情况下，理论上可以尝试连接到任何memcached客户端，只要它使用正确的源端口。然而，假设你为你的memcached客户端正确定义了网络策略，那么这些链接尝试仍在客户端被拒绝。 不跟踪网络策略适用于每个数据包，而正常的网络策略只适用于流中的第一个数据包。这可能会增加每个数据包的CPU成本，因为每个数据包都需要被网络策略处理。但是对于短暂的连接，这种额外的处理会被连通性处理的减少所抵消。例如，在saas供应上的案例中，每个连接中的数据包数量非常少，因此将策略应用于每个数据包的额外开销是一个合理的权衡。 进行测试我们测试了单个memcached服务器pod和在远程节点上运行的多个客户端pod，因此我们可以每秒驱动非常高的连接。memcached服务器pod主机有8个内核和一个512k的conntrack表。我们测量了以下之间的性能差异：没有网络策略、Calico正常网络策略和Calico不跟踪网络策略。在第一次测试中，我们将连接数限制在每秒4000个，这样我们就可以关注CPU的差异。没有策略和正常策略在性能上没有可衡量的差异，但不跟踪策略使得CPU使用率降低了约20%。 在第二个测试中，我们推送了我们的客户所能容纳的尽可能多的连接，并测量了memcached服务器每秒能够处理的最大连接数。正如预期那样，无策略和正常策略都在每秒4000多个连接数上达到conntrack表的限制(512k &#x2F; 120s &#x3D; 4396个连接&#x2F;s)。有了不跟踪策略，我们的客户端每秒推送了60000个连接而没有遇到任何问题。我们相信我们可以通过吸引更多的客户来超越这个数字，但是我们觉得这些数字足以说明这篇博客的意义了。 结论Conntrack是一个重要的内核特性。它做的很出色。许多主线用例都依赖它。然而对于某些小众的场景，conntrack的开销超过了它带来的正常好处。在这些场景中，Calico的网络策略可以用来有选择的绕过conntrack，同时仍然执行网络安全。对于所有的其他流量，conntrack仍然是你的朋友。","categories":[{"name":"Linux","slug":"Linux","permalink":"http://jerryblogs.com/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://jerryblogs.com/tags/Linux/"}]},{"title":"docker内的init进程是否能被SIGKILL或SIGTERM杀死","slug":"docker-signal","date":"2021-08-13T15:41:47.000Z","updated":"2023-04-20T13:39:16.232Z","comments":true,"path":"2021/08/13/docker-signal/","link":"","permalink":"http://jerryblogs.com/2021/08/13/docker-signal/","excerpt":"","text":"问题在容器内给容器内的init进程发送SIGTERM信号，init进程会不会被杀死，发送SIGKILL信号呢？在容器外发送信号呢？为什么？ 问题来源：极客时间 相关理论知识信号相关 SIGKILL和SIGTERM信号都是Linux信号，SIGKILL和SIGSTOP属于不能被忽略和handle的内核信号，主要提供给内核进程和root用户进行特权操作。 使用kill命令的时候，缺省状态下，kill 1 向init进程发送SIGTREM信号 Ctrl+C发送的是SIGING信号 编程语言的signal很多编程语言对不同的信号做了handle处理，使用C语言需要自己做处理（除了SIGKILL和SIGSTOP之外，他们有内核提供的默认的handle）。下列是Python和Go语言对部分signal的处理： Python Golang SIGQUIT 执行默认操作，进程退出并将内存中的信息转存到硬盘（核心转储） 以堆栈转储方式退出 SIGCHLD 终止子进程 子进程退出 SIGHUP 在控制终端上检测到挂起或控制进程的终止 程序退出 SIGTERM 终结 程序退出 SIGPIPE 写入到没有读取器的管道，默认忽略 在标准输出或标准错误上写入损坏的管道将导致程序退出，而在其他的文件描述符上写入不会采取任何操作，但是会抛出EPIPE错误 SIGING 引发 KeyboardInterrupt 关于signal信号的处理，可以查看下列文档： Python：https://docs.python.org/zh-cn/3/library/signal.html?highlight=signal#signal.SIGPIPE Golang：https://pkg.go.dev/os/signal 容器中的init进程 容器中的init进程是pid为1的进程 当容器以单进程运行时，init进程为容器内执行的进程 当容器以多进程运行时，会启动一个管理进程，该进程即为容器的init进程 在容器内发送SIGKILL命令，内核将会屏蔽掉该信号（与在跟命名空间中一致），防止其他进程杀掉init进程 在容器外杀掉init进程后，再向容器内添加进程将会导致ENOMEN错误，因为进程已经终止 init进程是容器内所有孤儿进程的父进程 当使用kill命令时，内核做了什么（源码分析）寻找kill系统调用Linux系统调用由原本的sys_XXXX换成了SYSCALL_DEFINE，故在&#x2F;kernel&#x2F;signal.c文件下，寻找SYSCALL_DEFINE，找到SYSCALL_DEFINE2(kill, pid_t, pid, int, sig)，代码如下： 12345678910111213/** * sys_kill - send a signal to a process * @pid: the PID of the process * @sig: signal to be sent */SYSCALL_DEFINE2(kill, pid_t, pid, int, sig)&#123; struct kernel_siginfo info; // info是signal的结构体 prepare_kill_siginfo(sig, &amp;info); return kill_something_info(sig, &amp;info, pid);&#125; 关于SYSCALL_DEFINEx，可以在https://blog.csdn.net/hxmhyp/article/details/22699669文章里面看到。 关于kernel_siginfo，可以看如下代码： 123typedef struct kernel_siginfo &#123; __SIGINFO;&#125; kernel_siginfo_t; 关于__SIGINFO： 123456struct &#123; \\ int si_signo; /* Signal number */ int si_errno；/* An errno value */ int si_code; // 产生信号的原因 union __sifields _sifields;&#125; 简而言之，__SIGINFO就是标识signal的一些信息。 接下来我们来看看prepare_kill_siginfo(sig, &amp;info);方法做了什么： 123456789static inline void prepare_kill_siginfo(int sig, struct kernel_siginfo *info)&#123; clear_siginfo(info); info-&gt;si_signo = sig; // 传入的信号 info-&gt;si_errno = 0; info-&gt;si_code = SI_USER; // 产生自用户空间 info-&gt;si_pid = task_tgid_vnr(current); // 主要是从当前namespace 中找到对应的pid号。 info-&gt;si_uid = from_kuid_munged(current_user_ns(), current_uid()); // 返回0，也就是root用户&#125; 关于info-&gt;si_code = SI_USER; 中的SI_USER，在内核源码\\include\\uapi\\asm-generic\\siginfo.h可以看到Linux定义的一些信号相关的信息。下列贴出部分： 1234567891011121314151617/* * si_code values * Digital reserves positive values for kernel-generated signals. */#define SI_USER 0 /* sent by kill, sigsend, raise 由kill、raise等发送的*/#define SI_KERNEL 0x80 /* sent by the kernel from somewhere 从内核某处发送 */#define SI_QUEUE -1 /* sent by sigqueue */#define SI_TIMER -2 /* sent by timer expiration */#define SI_MESGQ -3 /* sent by real time mesq state change */#define SI_ASYNCIO -4 /* sent by AIO completion */#define SI_SIGIO -5 /* sent by queued SIGIO */#define SI_TKILL -6 /* sent by tkill system call */#define SI_DETHREAD -7 /* sent by execve() killing subsidiary threads */#define SI_ASYNCNL -60 /* sent by glibc async name lookup completion */#define SI_FROMUSER(siptr) ((siptr)-&gt;si_code &lt;= 0)#define SI_FROMKERNEL(siptr) ((siptr)-&gt;si_code &gt; 0) 由此可以看到，这里的标识表示SI_USER是由kill发出的。 接下来可以看task_tgid_vnr(current)；首先是current： 1234567891011121314/* * We don&#x27;t use read_sysreg() as we want the compiler to cache the value where * possible. */static __always_inline struct task_struct *get_current(void)&#123; unsigned long sp_el0; asm (&quot;mrs %0, sp_el0&quot; : &quot;=r&quot; (sp_el0)); return (struct task_struct *)sp_el0;&#125;#define current get_current() // 获取当前运行的task_struct 它是获取当前运行的task_struct，我们都知道，在Linux里面task_struct表示一个进程描述符，存储了进程相关的一些信息。 然后我们看一下task_tgid_vnr方法，一路追踪到底，可以看到： 12345678910111213pid_t __task_pid_nr_ns(struct task_struct *task, enum pid_type type, struct pid_namespace *ns)&#123; pid_t nr = 0; // pid_t 宏定义 int 变量 rcu_read_lock(); // 申请rcu锁 if (!ns) ns = task_active_pid_ns(current); nr = pid_nr_ns(rcu_dereference(*task_pid_ptr(task, type)), ns); rcu_read_unlock(); // 释放rcu锁 return nr;&#125; task_active_pid_ns方法：\\linux\\kernel\\pid.c 1234struct pid_namespace *task_active_pid_ns(struct task_struct *tsk)&#123; return ns_of_pid(task_pid(tsk));&#125; 接着继续ns_of_pid方法：\\include\\linux\\pid.h 1234567static inline struct pid_namespace *ns_of_pid(struct pid *pid)&#123; struct pid_namespace *ns = NULL; if (pid) ns = pid-&gt;numbers[pid-&gt;level].ns; return ns;&#125; 这里返回pid所在的命名空间。 然后是pid_nr_ns：\\include\\linux\\pid.h 123456789101112pid_t pid_nr_ns(struct pid *pid, struct pid_namespace *ns)&#123; struct upid *upid; pid_t nr = 0; if (pid &amp;&amp; ns-&gt;level &lt;= pid-&gt;level) &#123; upid = &amp;pid-&gt;numbers[ns-&gt;level]; if (upid-&gt;ns == ns) nr = upid-&gt;nr; &#125; return nr;&#125; 主要是从当前namespace 中找到对应的pid号。 OK，所以prepare_kill_siginfo方法获取kernel_siginfo相关的信息并进行初始化。 接下来我们来看最关键的kill_something_info： 12345678static int kill_something_info(int sig, struct kernel_siginfo *info, pid_t pid)&#123; int ret; if (pid &gt; 0) return kill_proc_info(sig, info, pid); ...&#125; kill_proc_info: 12345678static int kill_proc_info(int sig, struct kernel_siginfo *info, pid_t pid)&#123; int error; rcu_read_lock(); error = kill_pid_info(sig, info, find_vpid(pid)); rcu_read_unlock(); return error;&#125; 首先是find_vpid: 1234struct pid *find_vpid(int nr)&#123; return find_pid_ns(nr, task_active_pid_ns(current));&#125; task_active_pid_ns刚刚我们已经讲过，所以这里是返回当前进程所在的命名空间。 而find_pid_ns作用是找到其所属的struct pid。 然后我们进入kill_pid_info： 123456789101112131415161718192021int kill_pid_info(int sig, struct kernel_siginfo *info, struct pid *pid)&#123; int error = -ESRCH; struct task_struct *p; for (;;) &#123; rcu_read_lock(); p = pid_task(pid, PIDTYPE_PID); if (p) error = group_send_sig_info(sig, info, p, PIDTYPE_TGID); rcu_read_unlock(); if (likely(!p || error != -ESRCH)) return error; /* * The task was unhashed in between, try again. If it * is dead, pid_task() will return NULL, if we race with * de_thread() it will find the new leader. */ &#125;&#125; pid_task： 123456789101112struct task_struct *pid_task(struct pid *pid, enum pid_type type)&#123; struct task_struct *result = NULL; if (pid) &#123; struct hlist_node *first; first = rcu_dereference_check(hlist_first_rcu(&amp;pid-&gt;tasks[type]), lockdep_tasklist_lock_is_held()); if (first) result = hlist_entry(first, struct task_struct, pid_links[(type)]); &#125; return result;&#125; 根据一致的struct pid找到struct task_struct。 group_send_sig_info： 12345678910111213141516int group_send_sig_info(int sig, struct kernel_siginfo *info, struct task_struct *p, enum pid_type type)&#123; int ret; rcu_read_lock(); ret = check_kill_permission(sig, info, p); // 检查是否是有效信号，检测是否来自用户或内核 // 来自用户需要记录审计日志后返回0，来自内核直接返回0 rcu_read_unlock(); if (!ret &amp;&amp; sig) // 如果是有效信号，调用do_send_sig_info ret = do_send_sig_info(sig, info, p, type); return ret;&#125; 我们来看看check_kill_permission的调用链： 123456789101112131415161718192021222324252627282930313233343536373839/* * Bad permissions for sending the signal * - the caller must hold the RCU read lock */static int check_kill_permission(int sig, struct kernel_siginfo *info, struct task_struct *t)&#123; struct pid *sid; int error; if (!valid_signal(sig)) // return sig &lt;= _NSIG ? 1 : 0; _NSIG == 64 这里检测是否是一个有效的信号 return -EINVAL; // EINVAL 宏定义 22 if (!si_fromuser(info)) // si_fromuser判断是否是来自用户，是返回True return 0; error = audit_signal_info(sig, t); // 记录审计日志，这里始终返回0 if (error) return error; if (!same_thread_group(current, t) &amp;&amp; // current和当前进程信号是否一致 !kill_ok_by_cred(t)) &#123; switch (sig) &#123; case SIGCONT: sid = task_session(t); /* * We don&#x27;t return the error if sid == NULL. The * task was unhashed, the caller must notice this. */ if (!sid || sid == task_session(current)) break; fallthrough; default: return -EPERM; &#125; &#125; return security_task_kill(t, info, sig, NULL); // 返回0&#125; si_fromuser： 1234567891011static inline bool si_fromuser(const struct kernel_siginfo *info)&#123; return info == SEND_SIG_NOINFO || // SEND_SIG_NOINFO为SI_USER，这里info.si_code为SI_USER故为True (!is_si_special(info) &amp;&amp; SI_FROMUSER(info)); // is_si_special函数关键处为info &lt;= SEND_SIG_PRIV; SEND_SIG_PRIV即SI_KERNEL表示由内核发送 // 故is_si_special(info)为True，即!is_si_special(info)为false // SI_FROMUSER为下列宏定义 // #define SI_FROMUSER(siptr) ((siptr)-&gt;si_code &lt;= 0) // 故这里SI_FROMUSER(info)为True // 综上所述，si_fromuser判断sig是否来自用户&#125; 进入audit_signal_info(sig, t)调用： 1234567891011121314151617181920212223242526/** * audit_signal_info - record signal info for shutting down audit subsystem * @sig: signal value * @t: task being signaled * * If the audit subsystem is being terminated, record the task (pid) * and uid that is doing that. */int audit_signal_info(int sig, struct task_struct *t)&#123; kuid_t uid = current_uid(), auid; if (auditd_test_task(t) &amp;&amp; // auditd_test_task判断是否是注册的审计进程，是返回1，否返回0 (sig == SIGTERM || sig == SIGHUP || sig == SIGUSR1 || sig == SIGUSR2)) &#123; // 如果sig是SIGTERM、SIGHUP或用户自定义的SIGUSR1和SIGUSR2 audit_sig_pid = task_tgid_nr(current); auid = audit_get_loginuid(current); if (uid_valid(auid)) audit_sig_uid = auid; else audit_sig_uid = uid; security_task_getsecid(current, &amp;audit_sig_sid); &#125; return audit_signal_info_syscall(t); // 始终返回0&#125; auditd_test_task： 1234567891011121314151617181920/** * auditd_test_task - Check to see if a given task is an audit daemon * @task: the task to check * * Description: * Return 1 if the task is a registered audit daemon, 0 otherwise. */int auditd_test_task(struct task_struct *task)&#123; int rc; struct auditd_connection *ac; rcu_read_lock(); ac = rcu_dereference(auditd_conn); // rc = (ac &amp;&amp; ac-&gt;pid == task_tgid(task) ? 1 : 0); // task_tgid通过task_struct获取pid结构体，而ac是Linux audit守护进程，负责将审计记录写入磁盘，使用ausearch或aureport实用程序查看日志。 rcu_read_unlock(); return rc;&#125; 然后回到group_send_sig_info函数，我们聚焦到do_send_sig_info函数： 12345678910111213int do_send_sig_info(int sig, struct kernel_siginfo *info, struct task_struct *p, enum pid_type type)&#123; unsigned long flags; int ret = -ESRCH; if (lock_task_sighand(p, &amp;flags)) &#123; // 获取信号处理函数锁 ret = send_signal(sig, info, p, type); unlock_task_sighand(p, &amp;flags); &#125; return ret;&#125; 调用send_signal： 1234567891011121314151617181920212223242526272829303132333435363738static int send_signal(int sig, struct kernel_siginfo *info, struct task_struct *t, enum pid_type type)&#123; /* Should SIGKILL or SIGSTOP be received by a pid namespace init? */ // pid namespace init是否需要接收SIGKILL or SIGSTOP bool force = false; if (info == SEND_SIG_NOINFO) &#123; /* Force if sent from an ancestor pid namespace */ // 如果来自祖先用户空间，则强制发送 force = !task_pid_nr_ns(current, task_active_pid_ns(t)); &#125; else if (info == SEND_SIG_PRIV) &#123; /* Don&#x27;t ignore kernel generated signals */ // 来自内核的信号不会被忽略 force = true; &#125; else if (has_si_pid_and_uid(info)) &#123; /* SIGKILL and SIGSTOP is special or has ids */ struct user_namespace *t_user_ns; rcu_read_lock(); t_user_ns = task_cred_xxx(t, user_ns); if (current_user_ns() != t_user_ns) &#123; kuid_t uid = make_kuid(current_user_ns(), info-&gt;si_uid); info-&gt;si_uid = from_kuid_munged(t_user_ns, uid); &#125; rcu_read_unlock(); /* A kernel generated signal? */ force = (info-&gt;si_code == SI_KERNEL); /* From an ancestor pid namespace? */ if (!task_pid_nr_ns(current, task_active_pid_ns(t))) &#123; info-&gt;si_pid = 0; force = true; &#125; &#125; return __send_signal(sig, info, t, type, force);&#125; task_pid_nr_ns我们刚刚已经讲过了，让我们再讲一遍： 123456789101112131415pid_t __task_pid_nr_ns(struct task_struct *task, enum pid_type type, struct pid_namespace *ns)&#123; //参数中的 ns = task_active_pid_ns(t)是返回目标进程的namespace // 这里的task是current，也就是当前进程的task_struct pid_t nr = 0; // pid_t 宏定义 int 变量 rcu_read_lock(); // 申请rcu锁 if (!ns) ns = task_active_pid_ns(current); // 返回pid也就是调用kill的进程所在的命名空间 nr = pid_nr_ns(rcu_dereference(*task_pid_ptr(task, type)), ns); // 从当前namespace 中找到对应的pid号 rcu_read_unlock(); // 释放rcu锁 return nr;&#125; 然后来到pid_nr_ns： 123456789101112pid_t pid_nr_ns(struct pid *pid, struct pid_namespace *ns)&#123; struct upid *upid; pid_t nr = 0; if (pid &amp;&amp; ns-&gt;level &lt;= pid-&gt;level) &#123; upid = &amp;pid-&gt;numbers[ns-&gt;level]; // 获取和目标进程处于同一level的namespace if (upid-&gt;ns == ns) // 当前进程和目标进程是不是同一个命名空间 nr = upid-&gt;nr; &#125; return nr;&#125; 在这里，我们使用kill命令发送signal的进程和目标进程之间的关系存在3种情况： 处于同一个namespace： 那么也就是&amp;ns.level&#x3D;&#x3D;&amp;pid.level，而且&amp;pid-&gt;numbers[ns-&gt;level] &#x3D;&#x3D; ns，最终结果返回当前进程pid 当前进程处于目标进程的父namespace： 则&amp;pid.level &lt; &amp;ns.level，最终返回0 所以由于pid &gt; 0，结果!task_pid_nr_ns(current, task_active_pid_ns(t))情况如下： 处于同一个namespace：False 当前进程处于目标进程的父namespace：True 所以，如果当前进程处于目标进程的祖先namespace，那这个信号一定会被传递给init进程。 1234567891011121314151617181920212223242526stateDiagram [*] --&gt; SYSCALL_DEFINE2 SYSCALL_DEFINE2 --&gt; prepare_kill_siginfo prepare_kill_siginfo --&gt; task_tgid_vnr task_tgid_vnr --&gt; pid_nr_ns prepare_kill_siginfo --&gt; kill_something_info kill_something_info --&gt; kill_proc_info kill_proc_info --&gt; kill_pid_info kill_pid_info --&gt; group_send_sig_info group_send_sig_info --&gt; kill_pid_info group_send_sig_info --&gt; check_kill_permission group_send_sig_info --&gt; do_send_sig_info check_kill_permission --&gt; security_task_kill security_task_kill --&gt; do_send_sig_info do_send_sig_info --&gt; send_signal send_signal --&gt; task_pid_nr_ns send_signal --&gt; has_si_pid_and_uid has_si_pid_and_uid --&gt; current_user_ns has_si_pid_and_uid --&gt; task_pid_nr_ns current_user_ns --&gt; make_kuid make_kuid --&gt; from_kuid_munged task_pid_nr_ns --&gt; task_active_pid_ns send_signal --&gt; __send_signal from_kuid_munged --&gt; __send_signal task_active_pid_ns --&gt; __send_signal __send_signal --&gt; [*] Linux忽略SIGKILL和SIGSTOP发送到init进程123456789101112131415static int __send_signal(int sig, struct kernel_siginfo *info, struct task_struct *t, enum pid_type type, bool force)&#123; struct sigpending *pending; // sig的链表，保存了进程上尚未处理的信号 struct sigqueue *q; // 实时信号可能会排队，使用队列实现 int override_rlimit; int ret = 0, result; assert_spin_locked(&amp;t-&gt;sighand-&gt;siglock); // 自旋锁 result = TRACE_SIGNAL_IGNORED; // 判断该信号SIGKILL或者SIGSTOP是否是发送给init，如果是则忽略 if (!prepare_signal(sig, t, force)) goto ret;&#125; 追踪到prepare_signal函数一直往下： 1234567891011121314151617181920212223242526272829static bool sig_task_ignored(struct task_struct *t, int sig, bool force)&#123; void __user *handler; handler = sig_handler(t, sig); /* SIGKILL and SIGSTOP may not be sent to the global init */ // SIGKILL and SIGSTOP不会被发送到global init进程 // 如果是SIGKILL and SIGSTOP目标进程是init进程 if (unlikely(is_global_init(t) &amp;&amp; sig_kernel_only(sig))) return true; // 这里的flag标志是task_struct的标志，见下面 // SIGNAL_UNKILLABLE为进程刚被fork，但是还没被执行，所以这里是如果进程刚被创建，但是还没被执行，返回true // 如果是默认的handle // 不是SIGKILL或者是SIGSTOP并且可以被发送到进程 // 所以这里标识如果进程刚被fork，发送的信号不是必须被处理的信号，且handle是默认的handle，则不能响应信号 if (unlikely(t-&gt;signal-&gt;flags &amp; SIGNAL_UNKILLABLE) &amp;&amp; handler == SIG_DFL &amp;&amp; !(force &amp;&amp; sig_kernel_only(sig))) return true; /* Only allow kernel generated signals to this kthread */ // PF_KTHREAD是内核线程 if (unlikely((t-&gt;flags &amp; PF_KTHREAD) &amp;&amp; (handler == SIG_KTHREAD_KERNEL) &amp;&amp; !force)) return true; return sig_handler_ignored(handler, sig);&#125; flag信号： 123456// flag可能取值如下：PF_FORKNOEXEC 进程刚创建，但还没执行。PF_SUPERPRIV 超级用户特权。PF_DUMPCORE dumped core。PF_SIGNALED 进程被信号(signal)杀出。PF_EXITING 进程开始关闭 再看sig_handler_ignored： 1234567891011121314static inline bool sig_handler_ignored(void __user *handler, int sig)&#123; /* Is it explicitly or implicitly ignored? */ // 属于可以被忽略的信号 return handler == SIG_IGN || // 默认信号处理程序，也就是说信号是SIGCONT，SIGCHLD，SIGWINCH，SIGURG他们的默认处理信号是忽略 (handler == SIG_DFL &amp;&amp; sig_kernel_ignore(sig));&#125;#define sig_kernel_ignore(sig) siginmask(sig, SIG_KERNEL_IGNORE_MASK)#define SIG_KERNEL_IGNORE_MASK (\\ rt_sigmask(SIGCONT) | rt_sigmask(SIGCHLD) | \\ rt_sigmask(SIGWINCH) | rt_sigmask(SIGURG) ) SIGCONT，SIGCHLD，SIGWINCH，SIGURG： 12345678SIGCHLD：● 子进程终止时；● 子进程接收到SIGSTOP信号停止时；● 子进程处在停止态，接收到SIGCONT信号后被唤醒时。SIGWINCH:窗口大小改变时SIGURG：有&quot;紧急&quot;数据或out-of-band数据到达socket时产生. 总结 信号时SIGKILL 或 SIGSTOP目标进程是init进程，该信号会被忽略即不能响应信号 如果进程刚被fork，发送的信号不是必须被处理的信号，且handle是默认的handle，则不能响应信号 目标线程是内核线程，并且是默认内核线程处理程序，但是不能被发送，则不能响应信号 信号是SIGCONT，SIGCHLD，SIGWINCH，SIGURG他们的默认处理信号是忽略","categories":[{"name":"Docker","slug":"Docker","permalink":"http://jerryblogs.com/categories/Docker/"}],"tags":[{"name":"Docker","slug":"Docker","permalink":"http://jerryblogs.com/tags/Docker/"}]},{"title":"Docker的命名空间","slug":"docker-namespace","date":"2021-08-09T15:36:40.000Z","updated":"2023-04-20T13:39:13.005Z","comments":true,"path":"2021/08/09/docker-namespace/","link":"","permalink":"http://jerryblogs.com/2021/08/09/docker-namespace/","excerpt":"","text":"命名空间Linux内核用命名空间来区分内核资源。不同的进程看到的资源是不同的。 这里的资源包括pid，hostname，userid，文件名，网络访问和进程间通信 在Linux内核3.8版本，内核对容器有足够的支持 内核空间的类型 Linux内核5.6，支持8种命名空间。 mount：mount命名空间控制挂载点 在创建命名空间时，当前的命名空间会复制到新的命名空间。但是已经创建的挂载点不会在明明见之间传播。 创建此类型的标志clone_NEW PID：提供独立于其他命名空间的pid集合，不同pid namespace里面的pid可以重复 PID命名空间内的进程ID是唯一的，从1开始分配 PID 1进程的终止将终止该命名空间以及其后代所有进程 PID namespace的使用需要配置参数CONFIG_PID_NS Network(net)：网络命名空间虚拟化提供隔离的网络相关的系统隔离资源，如网络设备、IPv4、IPv6协议栈，IP路由表、防火墙规则、&#x2F;proc&#x2F;net目录、&#x2F;sys&#x2F;class&#x2F;net目录、&#x2F;proc&#x2F;sys&#x2F;net下的各种文件、端口号等等。特别的，网络命名空间隔离UNix域抽象套接字命名空间。 物理网络设备：当网络命名空间内的进程终止，网络命名空间被释放时，物理网络设备将会回到初始话网络命名空间。 虚拟网络设备：提供了管道式的，抽象化的，可以在网络命名空间之间使用的隧道，在一个物理网络设备到另一个网络命名空间之间使用的桥梁。当一个namespace被释放，它包含的veth设备将会被销毁。 使用network namespace需要内核配置CONFIG_NET_NS参数 User：用来区分安全标识符和属性，特别是UserID、GroupID、根目录和keys。 Cgropu：（Cgroup可以对一组进程做资源控制） IPC：隔离了IPC资源，即SystemV IPC对象，POSIX消息队列。需要CONFIG_IPC_NS内核参数 Time：CONFIG_TIME_NS内核参数。虚拟化CLOCK_MONOTONIC和CLOCK_BOOTTIME。 UTS：虚拟化hostname和NIS域名。CONFIG_UTS_NS PID namespacenamespace init process第一个在namespace中创建的进程，是任何在这个pid namespace中的孤儿进程的父进程。 当init终止，内核通过发送SIGKILL信号终止namespace中所有的子进程，在这种情况下，调用fork将返回ENOMEN错误，因为init进程已经终止。 内核会帮助init进程屏蔽掉任何其他信号，防止其他进程不小心kill掉init进程导致系统挂掉。可以在父PID namespace中发送SIGKILL信号终止namespace中init进程。通过什么方式屏蔽？ PID namespace nestPID namespace可以嵌套：每一个PID namespace都有一个parent，除了根PID namespace。父PID namespace是使用clone()或unshare()函数创建的进程。从Linux 3.7开始，内核限制最深嵌套是32。 子PID namespace中的进程对于父PID namespace中的进程是不可见的。进程可以下降到子命名空间，但是不能进入任何一个父命名空间。 进程的PID命名空间在进程创建时确定，不可更改。 &#x2F;proc 和 PID namespace对一个PID namespace而言，&#x2F;proc目录只包含当前namespace和它所有子孙后代namespace里的进程的信息。 mount namespacemount namespace隔离了不同的挂载点，每个挂载的命名空间实例将看到不同的单目录层次结构。使用clone和unshare传递CLONE_NEWS信号能够创建心的mount namespace。 当一个心得mount namespace创建，将进行如下初始化： 使用clone创建，将子namespace的挂载点列表是父挂载点列表的复制 使用unshare创建，新命名空间挂载点列表是调用者之前命名空间挂载点列表的复制 mount namespace的限制 每一个mount namespace拥有一个user namespace。一个新的mount namespace是另一个namespace的副本，新旧namespace拥有不同的user namespace，那么新的namespace将被赋予更低的特权级别 当创建一个更低特权级别的mount namespace，共享挂载将会减少到从属挂载，这确保更低特权级别执行的映射不会传播到更高特权级别 来自更高特权级别的单个单元的挂载将会被加锁，不会被更低优先级别的mount namespace分开。unshare的CLONE_NEWS操作将原有命名空间当中的mount以及其递归的mount作为单个单元进行传播。 在一个mount namespace作为挂载点且不在其他mount namespace的文件或目录可以被重命名、取消链接和删除（不是挂载点）。 共享子树背景：为了加载一个新的硬盘，使其在所有的mount namespace中可用，需要在所有的mount namespace中执行挂载操作。 共享子树功能在Linux 2.6.15中引入。该功能允许自动的、在命名空间之前传播的受控的挂载和卸载事件。 每个挂载点被标记为如下的传播类型： MS_SHARED：在一组对等节点之间共享事件。 MS_PRIVATE：这个挂载点是私有的，挂载和卸载事件不会传播进来或传播出去。 MS_SLAVE：事件将会在master的共享对等组之间传播，不会传播到其他组。（一组挂载点可以是其他组的slave） MS_UNBINDDABLE：类似于私有的mount，这个mount将不能被绑定挂载。","categories":[{"name":"Docker","slug":"Docker","permalink":"http://jerryblogs.com/categories/Docker/"}],"tags":[{"name":"Docker","slug":"Docker","permalink":"http://jerryblogs.com/tags/Docker/"}]},{"title":"Docker的架构","slug":"docker-framework","date":"2021-08-08T17:03:04.000Z","updated":"2023-04-20T13:39:05.424Z","comments":true,"path":"2021/08/09/docker-framework/","link":"","permalink":"http://jerryblogs.com/2021/08/09/docker-framework/","excerpt":"","text":"Docker的架构docker是client-server架构，client通过socket或restful API与docker daemon之间进行数据交互。两者可以跑在同一个系统上，或者使用远程的Docker daemon。后者负责繁重的构建、运行、分发docker 容器。 docker内部包含三个组件： docker images docker registers 镜像仓库（私有和公有） docker containers Docker image如何工作docker image是层次化的只读模板，它利用union file system将不同的层组成一个image，同时允许覆盖单独文件系统的文件和目录，形成统一的文件系统。当image增加层或者更新层，需要重新构建image。 Docker container work当从docker images 运行一个container，它会在image的顶层添加一个读写层（union file system）以使你的应用可以运行。 当运行一个container，会发生什么？ pull the image：docker检查镜像是否存在，当镜像不存在，会从docker hub中拉取；当镜像存在，即使用其构建。 create a new container：一旦image存在，docker使用其创建一个镜像。 分配一个文件系统并且挂载一个读写层：容器在文件系统之中被创建，并且读写层被添加进image 分配一个网络&#x2F;网桥接口：分配一个网络&#x2F;网桥接口，允许容器与本机交互 设置IP地址：从IP池中寻找并分配一个可用的IP地址 执行指定的进程：运行应用程序 捕获并且提供应用的输出：连接并且记录标准输入，输出和错误 当container结束时，停止并且删除contianer 使用的底层技术namespaceLinuex 内核提供了一层隔离，容器的每个方面运行在namespace，并且对它所处的namespace之外没有访问权限 Control GroupsCGroups允许Docker将可用硬件资源共享给容器，并在必要时设置约束和限制，，比如限制特定容器的可用内存。 Union file systemUnion通过创建层来操作的文件系统。","categories":[{"name":"Docker","slug":"Docker","permalink":"http://jerryblogs.com/categories/Docker/"}],"tags":[{"name":"Docker","slug":"Docker","permalink":"http://jerryblogs.com/tags/Docker/"}]},{"title":"如何阅读docker源码","slug":"docker-learn","date":"2021-08-08T14:00:00.000Z","updated":"2023-04-20T13:39:10.866Z","comments":true,"path":"2021/08/08/docker-learn/","link":"","permalink":"http://jerryblogs.com/2021/08/08/docker-learn/","excerpt":"","text":"来自docker-dev群组：https://groups.google.com/g/docker-dev/c/g0jUDDltp4o learn go figure out how the basic command flags work build docker executables with minor changes 来自StackOverflow：https://stackoverflow.com/questions/22639685/new-to-docker-wonder-how-to-read-dockers-source-code 理解Cgroup和Linux内核的命名空间 阅读 docker’s github source code repository和contributing to docker guide","categories":[{"name":"Docker","slug":"Docker","permalink":"http://jerryblogs.com/categories/Docker/"}],"tags":[{"name":"Docker","slug":"Docker","permalink":"http://jerryblogs.com/tags/Docker/"}]},{"title":"B站面试","slug":"B站面试","date":"2020-10-31T12:36:31.000Z","updated":"2023-04-20T17:37:34.463Z","comments":true,"path":"2020/10/31/B站面试/","link":"","permalink":"http://jerryblogs.com/2020/10/31/B%E7%AB%99%E9%9D%A2%E8%AF%95/","excerpt":"","text":"redis和rebitMQ的区别 消息队列和任务队列的区别 进程线程协程的区别 docker虚拟化的原理 docker和虚拟机的区别 python协程的实现原理 哈希表的实现原理 如何解决哈希冲突 https的实现 证书的校验过程 async和原生的yield有什么不同 不同的进程之间为什么不能互相访问内存 fork一个子进程之后，操作系统会做什么事情","categories":[{"name":"面试经历","slug":"面试经历","permalink":"http://jerryblogs.com/categories/%E9%9D%A2%E8%AF%95%E7%BB%8F%E5%8E%86/"}],"tags":[{"name":"面试经历","slug":"面试经历","permalink":"http://jerryblogs.com/tags/%E9%9D%A2%E8%AF%95%E7%BB%8F%E5%8E%86/"}]},{"title":"Docker的一些问题","slug":"Docker的一些问题","date":"2020-10-31T12:36:31.000Z","updated":"2023-04-20T17:34:48.522Z","comments":true,"path":"2020/10/31/Docker的一些问题/","link":"","permalink":"http://jerryblogs.com/2020/10/31/Docker%E7%9A%84%E4%B8%80%E4%BA%9B%E9%97%AE%E9%A2%98/","excerpt":"","text":"ADD和COPY异同相同点： ADD和COPY都支持将本地目录及文件复制到容器内指定目录和文件 不同点： ADD支持从URL下载链接文件，不过下载的文件如果需要改变权限还需要另外一层RUN进行权限调整（默认是600）。如果源路径为一个tar压缩文件，压缩格式为gzip、bzip2以及xz的情况下，ADD会自动解压这个压缩文件到目标路径。 ADD指令会令镜像构建缓存失效，从而使镜像构建变得缓慢 所有文件复制均用COPY，仅在需要自动解压缩的时候使用ADD docker build的机制将Dockerfile转换为一个唯一的docker镜像 镜像的cache机制一个Dockerfile构建多次，对于Dockerfile中的某一条指令并不会产生多个全新的镜像层。 docker build 的 cache机制 Docker Daemnon通过Dockerfile构建新的镜像，发现新构建的镜像与某已有的镜像重复时，可以放弃构建新的镜像，而是选用已有的镜像作为结果，采取本地已经cache的镜像作为结果。 核心机制：遍历本地所有镜像，发现镜像与即将构建的镜像一致，将找到的镜像作为cache镜像，复用cache镜像作为构建结果。 ADD命令原则上不再使用cache。不过也有解决方案，就是将紧接着的文件计算出一个唯一的hash值，hash值未发生变化，可认为文件内容没有发生变化，可以使用cache机制。 docker进程启动就退出 查看日志和容器主进程退出码 检查容器日志 docker logs &#123;容器号&#125; 如果看到了Exited(127)那就是由于内存超标导致的触发 Out Of Memory ，然后被强制终止，如果是Exited(0)表示容器主进程正常退出。 如果是执行后台服务程序的命令，Docker不是虚拟机，Docker启动的是进程，因此后台服务应该放到前台，可以直接启动应用。 docker使用uwsgi时，如果在uwsgi.ini配置文件中指定了daemonize参数，容器的uwsgi应用的日志将会输出到指定的文件，进程在后台运行，而不是在前台运行，这样就造成通过docker run启动或者compose启动时容器立刻退出的情况","categories":[{"name":"Docker","slug":"Docker","permalink":"http://jerryblogs.com/categories/Docker/"}],"tags":[{"name":"Docker","slug":"Docker","permalink":"http://jerryblogs.com/tags/Docker/"}]},{"title":"Python内存回收","slug":"Python内存回收","date":"2020-10-31T12:36:31.000Z","updated":"2023-04-20T17:32:39.096Z","comments":true,"path":"2020/10/31/Python内存回收/","link":"","permalink":"http://jerryblogs.com/2020/10/31/Python%E5%86%85%E5%AD%98%E5%9B%9E%E6%94%B6/","excerpt":"","text":"在使用可变对象作为参数的时候一定要非常注意，防止意外修改形参值，不要使用可变对象作为默认参数，会变成函数对象的属性，再修改的时候会出现意想不到的效果。 def不删除对象，而是删除对对象的引用 _变量是交互式解释器中最后一次执行语句的返回结果 弱引用不会增加引用计数，强引用会增加引用计数 1open(&quot;test.txt&quot;, &quot;wt&quot;, encoding=&quot;utf-8&quot;).write(&#x27;1, 2, 3&#x27;) 在CPython中，这样写是安全的。因为文件对象的引用数量会在write方法返回后归零，Python在销毁内存中表示文件的对象之前，会立即关闭文件。","categories":[{"name":"Python","slug":"Python","permalink":"http://jerryblogs.com/categories/Python/"}],"tags":[{"name":"Python","slug":"Python","permalink":"http://jerryblogs.com/tags/Python/"}]},{"title":"Python函数参数","slug":"Python函数参数","date":"2020-10-31T12:36:31.000Z","updated":"2023-04-20T17:33:11.435Z","comments":true,"path":"2020/10/31/Python函数参数/","link":"","permalink":"http://jerryblogs.com/2020/10/31/Python%E5%87%BD%E6%95%B0%E5%8F%82%E6%95%B0/","excerpt":"","text":"1234__defaults__ # 保存定位参数和关键字参数的默认值__kwdefaults__ # 保存仅限关键字参数的默认值__code__ # 参数的名称__annotations__ # 注解存储位置 装饰器两个特性 能把被装饰的函数替换成其他函数 装饰器在加载其他函数时立即执行（在被装饰的函数定义后立即执行，通常在导入时） 变量作用域规则如果变量在函数内找不到，会从全局变量中找，如果变量在函数中被赋值，那么假定变量是局部变量 闭包延伸了作用域的函数，包含了在函数定义体中引用、但是不在定义体中定义的非全局变量。 1234567def make_averager():... series = []... def averager(new_value):... series.append(new_value)... total = sum(series)... return total / len(series)... return averager series是一个自由变量，即指未在本地作用域中绑定的变量 series绑定在返回的avg函数的__closure__属性中，其里面的元素对应avg.__code__.co_freevars中的一个名称，这些元素是cell对象，有个cell_contents属性保存真正的值。 闭包是一种函数，会保留定义函数时存在的自由变量的绑定，调用函数时，虽然定义作用域不可用了，但是仍然能够使用这些绑定。 nolocal使用nolocal变量时，会将变量标记为自由变量，即使在函数中赋值，也会变成自由变量 标准库中的装饰器functools.lru_cache 实现了备忘功能，缓存不会无限增长，一段时间内不使用的缓存条目会被扔掉 functools.singledispatch 把Java中的方法重载带入python","categories":[{"name":"Python","slug":"Python","permalink":"http://jerryblogs.com/categories/Python/"}],"tags":[{"name":"Python","slug":"Python","permalink":"http://jerryblogs.com/tags/Python/"}]},{"title":"Python数据模型","slug":"Python数据模型","date":"2020-10-31T12:36:31.000Z","updated":"2023-04-20T17:31:44.369Z","comments":true,"path":"2020/10/31/Python数据模型/","link":"","permalink":"http://jerryblogs.com/2020/10/31/Python%E6%95%B0%E6%8D%AE%E6%A8%A1%E5%9E%8B/","excerpt":"","text":"deck[0]或者deck[1]是通过__getitem__来实现的 迭代通常是隐式的，如果一个集合类型没有实现__contains__方法，那么in会进行一次顺序迭代搜索 abs是一个内置函数，如果输入的是整数或者是浮点数，他返回输入值的绝对值。如果输入值是复数，返回它的模 python有一个内置的repr函数，把一个对象以字符串的形式打印出来以方便辨认，字符串表示形式 __repr__和__str__的区别在于，后者是在函数中被使用，或是在print函数中打印一个对象的时候才被调用，并且返回的字符串对终端用户更加友好，__repr__是更好的选择，如果一个对象没有__str__函数，解释器会调用__repr函数作为替代。 __add__和__mul__这两个方法返回值都是重新创建的向量对象，被操作的两个对象依然原封不动，代码只是读取了他们的值而已 默认情况下，我们调用bool来将自己定义的类转换为布尔值总是被判定为True，除非自定义__bool__，如果没有，就会调用__len__方法，如果返回0，则为False 为什么len不是普通方法 如果x是一个内置的对象实例，那么len（x）调用的速度会非常快，因为CPython会直接从一个C结构体中读取对象的长度，完全不会调用任何方法。","categories":[{"name":"Python","slug":"Python","permalink":"http://jerryblogs.com/categories/Python/"}],"tags":[{"name":"Python","slug":"Python","permalink":"http://jerryblogs.com/tags/Python/"}]},{"title":"Python数据结构","slug":"Python数据结构","date":"2020-10-31T12:36:31.000Z","updated":"2023-04-20T17:33:41.955Z","comments":true,"path":"2020/10/31/Python数据结构/","link":"","permalink":"http://jerryblogs.com/2020/10/31/Python%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/","excerpt":"","text":"[toc] 序列构成的数组python标准库用C实现了丰富的序列类型 容器序列 list、turple和collection.deque，这些序列能够存放不同的数据 存放的包含任意对象类型的引用 扁平序列 str、bytes、bytearray、memoryview和array.array，这些类型只能容纳一种类型 一段连续的内存空间，存放的是值，只能存放字符、字节和数值这种基础类型 按照能否被修改来分类 可变序列 list、bytearray、array.array、colllection.deque和memoryview 不可变序列 tuple、str和bytes 列表推导式和生成器表达式只用列表推导式来创建一个新的列表，如果列表推导式过长，超过两行，那就要考虑是否要用for重写 在Python2里面for关键字之后的赋值操作可能会影响列表中的同名变量，比如 123x = &quot;hhh&quot;a = [x for x in &quot;ABC&quot;]print(x) # &quot;C&quot; 而在Python3里面不会出现这个问题，在Python3里面都会有自己的局部作用域 生成器表达式能够节约内存 元组不仅仅是不可变的列表，还是没有字段名的记录 元组里面携带了包含了所有的元素的总数和他们的位置，在任何的表达式里面我们对元组进行排序，那么这些携带的信息就会丢失 对列表的切片进行赋值 对序列使用+或者*都会创建一个新的对象而不会修改原有对象 不要把可变对象放到元组里面 增量赋值不是一个原子操作，先加再赋值 查看python的字节码并不难 list.sort()不会产生新的列表，而是在原有的列表上进行赋值，而sorted会产生一个新的列表","categories":[{"name":"Python","slug":"Python","permalink":"http://jerryblogs.com/categories/Python/"}],"tags":[{"name":"Python","slug":"Python","permalink":"http://jerryblogs.com/tags/Python/"}]},{"title":"Unix基础知识","slug":"linux-base-kwnolege","date":"2020-10-31T12:36:31.000Z","updated":"2023-04-20T17:14:40.743Z","comments":true,"path":"2020/10/31/linux-base-kwnolege/","link":"","permalink":"http://jerryblogs.com/2020/10/31/linux-base-kwnolege/","excerpt":"","text":"Unix基础知识unix体系结构从严格意义上说，操作系统可以定义为一种软件，这个软件控制硬件资源，为程序运行提供环境，我们通常将这种软件称为内核 kernel，位于整个环境的核心。 系统调用：内核的接口称为系统调用。公用函数库建立在系统调用之上，应用程序既可以使用公用函数库，也可以使用系统调用。shell是一个特殊的应用程序，为运行其他应用程序提供接口。 每个操作系统都在内核里面有一些内建的函数库，这些函数可以用来完成一些系统调用，把应用程序的请求传给和内核，调用相应的内核函数完成所需的处理，将处理结果返回给应用程序。 广义上讲，操作系统包括内核和一些其它软件，比如系统实用程序，应用程序，shell以及公用函数库等等。 登录先键入用户名，然后键入密码。系统口令文件 /etc/passwd 7个字段依次是：登录名，加密口令，uid，gid，注释字段，起始目录，shell程序 文件和目录所有东西的起点是根目录 &#x2F; 文件系统每个目录项都包含一个文件名，同时包含文件的属性信息。比如文件类型、大小、所有者、权限、最后修改时间。 文件名斜线（&#x2F;）和空字符不能出现在文件名中。 文件描述符通常是一个小的非负整数，内核用来标识一个特定进程正在访问的文件。当内核去打开一个现有文件或者是新文件的时候，会返回一个文件描述符，可以用它对文件进行读写。 标准输入、标准输出、标准错误每当运行一个新程序，所有的shell都会打开3个文件描述符，标准输入，标准输出、标准错误。并且自动连接到终端。用“&gt;”可以重定向到文件。 不带缓冲的IO函数open、read、write、lseek以及close提供了不带缓冲的IO，都使用文件描述符。 Linux对文件的IO操作分为带缓存和不带缓存，首先明确以下几点： 不带缓存，实际上是在用户层没有缓存，所以叫做无缓存的IO。对于内核来说，实际上还是进行了缓存。 带不带缓存是相对来说的。如果要将数据写入磁盘中，内核会先将数据写入内核所设的缓冲储存器，将缓冲储存器写满之后，在进行实际上的IO操作。也就是把数据写入磁盘中。 所以，不带缓存不是没有缓存直接写入磁盘，而是在用户层没有缓存 带缓存的IO实际上是在用户层建立一个缓存区，将用户层缓存区写满之后再写入内核缓存区，内核缓存区满了再写入磁盘中。这个用户层缓存，实际上减少了write()函数的调用次数。 所以两种方式的数据流向路径是： 无缓存IO：数据——内核缓存区——磁盘 有缓存IO：数据——流缓存区——内核缓存区——磁盘 标准IO带缓存的目的实际上是为了减少read()和write()的次数。 标准IO提供了三种缓存方式： 全缓存：填满标准IO缓存后才执行IO操作，磁盘上是全缓存。 行缓存：当输入遇到新行符或者缓存满时才由标准IO执行实际IO操作，stdin、stdout实际上是行缓存。 无缓存：相当于read、write。stderr通常是无缓存，他需要尽快输出。 标准IO即buffered IO都是调用unbuffered IO系统调用来实现的。他们并不直接读写磁盘，会集中对数据进行读写，用write()、read()函数，所以使用库函数的效率是高于直接使用系统调用的。 程序和进程程序是存储在磁盘当中某个目录的可执行文件，内核使用exec函数将缓存读入内存并执行程序。 进程控制3个用于进程控制的主要函数：fork、exec、waitpid 线程和线程ID一个进程内的所有线程共享同一地址空间，文件描述符，栈以及与进程相关的属性，因为他们能访问同一存储区，所以线程在访问共享数据时需要采取同步措施以避免不一致性。 出错处理当UNIX系统出错时，通常会返回一个负值，而且在整型变量error通常被设置为具有特定信息的值。例如，open函数如果成功执行则返回一个非负文件描述符，出错则返回-1。 用户标识用户ID用户ID用来标识唯一的用户。 用户ID为0的用户用来标识root或者超级用户。 组ID组文件通常是&#x2F;etc&#x2F;group 信号用于通知进程发生了某种情况，进程有三种处理信号的方式： 忽略信号。 按系统默认方式处理。对于除数0，系统默认方式是终止该进程。 提供一个函数，信号发生的时候调用该函数，捕捉该信号。 终端键盘上产生的信号：中断键，执行终止。退出键。它们用于终端当前运行的进程。调用kill函数，像一个进程发送信号，不过需要是那个进程的所有者或者是超级用户。 时间值进程时间：也被 称为CPU时间，用度量进程使用的CPU资源。 系统为一个进程维护了3个进程的时间值： 时钟时间：墙上时钟时间，进程运行的时间总量，其值与系统运行的进程数有关。 用户CPU时间：执行用户指令所用的时间量。 系统CPU时间：为该进程执行内核程序所用的时间量。","categories":[{"name":"Linux","slug":"Linux","permalink":"http://jerryblogs.com/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://jerryblogs.com/tags/Linux/"}]},{"title":"Linux笔记","slug":"linux-note","date":"2020-10-31T12:36:31.000Z","updated":"2023-04-20T17:11:53.400Z","comments":true,"path":"2020/10/31/linux-note/","link":"","permalink":"http://jerryblogs.com/2020/10/31/linux-note/","excerpt":"","text":"&#x2F;tmp目录自动清理在&#x2F;tmp文件夹下的文件是会被清理、删除的。主要是作业里面会自动调动tmpwatch来删除那些一段时间内没有访问的文件。 tmpwatch是一个命令或者说一个包 修改tmpwatch配置在 /usr/sbin/tmpwatch 以上是在Redhat里面 以下是Ubuntu 在Ubuntu14.04之前，&#x2F;tmp的清理工作由upstart脚本完成，&#x2F;etc&#x2F;init&#x2F;mounted-tmp.conf，这个脚本每次在&#x2F;tmp挂载的时候运行，也就是每次系统启动的时候。清理逻辑是如果&#x2F;tmp文件少于所配置的变量$TMPTIME天，将会被删除。$TMPTIME是一个定义在&#x2F;etc&#x2F;default&#x2F;rcS文件里面的变量。 Ubuntu 14.04 由tmpreader脚本运行删除，日常被/etc/cron.daily脚本调用，可以在/etc/default/rcS中和/etc/tmpreader.conf中配置。 Ubuntu 16.10 123456789在```/etc/tmpfiles.d/```文件夹下面由一个配置文件来控制是否删除 /tmp 。格式大致如下：```shell#/etc/tmpfiles.d/tmp.confd /tmp 1777 root root 20d 通过这样的设置，即使不重新引导系统，文件清理仍可运行。","categories":[{"name":"Linux","slug":"Linux","permalink":"http://jerryblogs.com/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://jerryblogs.com/tags/Linux/"}]},{"title":"Linux调度器","slug":"linux-scheduler","date":"2020-10-31T12:36:31.000Z","updated":"2023-04-20T17:11:59.410Z","comments":true,"path":"2020/10/31/linux-scheduler/","link":"","permalink":"http://jerryblogs.com/2020/10/31/linux-scheduler/","excerpt":"","text":"调度器介绍一个好的调度算法应该考虑以下几个方面： 公平：保证每个进程得到合理的CPU时间 高效：使CPU保持忙碌状态，即总是有进程在CPU上执行 响应时间：使交互应用的响应时间尽可能短 周转时间：使批处理用户等待输出的时间尽可能短 吞吐量：使单位时间内处理的进程数量尽可能多 负载均衡：在多核多处理器系统中提供更高的性能 在整个调度系统中存在两种调度算法，分别是针对实时进程和普通进程，所以在整个Linux内核中，实时进程和普通进程是并存的，但他们使用的调度算法并不相同，普通进程使用的使CFS调度算法（红黑树调度） 进程在Linux当中进程主要分两种，分别是实时进程和普通进程 实时进程：对系统的响应时间要求很高，他们需要很短的响应时间，并且这个时间的变化非常小典型的实时进程有音乐播放器，视频播放器等。 普通进程：包括交互进程和非交互进程，交互进程如文本编辑器，他会不断地休眠，又不断通过鼠标键盘进行唤醒，而非交互进程就如后台维护进程，他们对IO，响应时间没有很高的要求，比如编译器。 他们在Linux内核运行时是共存的，实时进程的优先级是099，实时进程优先级不会在运行期间内改变（静态优先级），而普通进程的优先级为100139，普通进程的优先级会在内核运行期间进行相应的改变（动态优先级） 调度策略 SCHED_NORMAL:普通进程使用的调度策略，现在此调度策略使用的是CFS调度器 SCHED_FIFO:实时进程使用的调度策略，此调度策略的进程一旦使用CPU则一直运行，直到有比其更高优先级的实时进程进入队列，或者自动放弃CPU，适用于时间要求比较高，但每次运行时间比较短的进程。 SCHED_RR:试试进程使用的时间片轮转法策略，实时进程的时间片用完后，调度器将其放到队尾，这样每个实时进程都可以执行一段时间。适用于每次运行时间比较长的实时进程 调度处于TASK_RUNNING状态的进程会进入调度器进行选择，其他状态的调度器不会进入调度器进行调度，调度的时机如下： 调用cond_resched()时 显示调用schedule()时 从系统调用或异常终端返回用户空间时 从中断上下文返回用户空间时 当内核抢占（默认开启）时，会多出几个调度时机： 在系统调用或异常中断上下文中调用preempt_enabled()时（多次调用preempt_enabled()时，系统只会在最后一次调用时调度） 在中断上下文中，从中断处理函数返回到可抢占的上下文时（这里是中断下半部，中断上半部实际上会关中断，而新的中断只会被登记，由于上半部处理很快，上半部处理完成后才会执行新的中断信号，这样就形成了中断可重入，但是即使是中断下半部，也是不能够被调度的） 在系统启动调度器初始化时会初始化一个调度定时器，调度定时器每隔一定时间执行一个中断，在中断会对当前运行进程进行更新，如果进程需要被调度，在调度定时器中断中会设置一个调度标志位，之后从定时器中断返回，因为上面中断上下文返回时是有调度时机的，如果定时器中断返回时正好是返回到用户态空间，且调度标志位又置位了，这时候就会做进程切换。在内核源码的汇编代码中所有的中断返回处理都必须去判断调度标志位是否设置，如设置则执行schedule()进行调度。每次调度时，会优先在实时进程运行队列中查看是否有可运行的实时进程，如果没有，再去普通进程运行队列找下一个可运行的普通进程，如果也没有，则调度器会使用idle进程进行运行。 在中断期间的时候（无论是上半部还是下半部），调度是被禁止的，中断之后才被重新允许调度。对于异常，系统并不会禁止调度，也就是在异常上下文中，系统是有可能发生调度的 数据结构普通进程使用的调度算法为CFS调度算法，它是以红黑树为基础的调度算法，相比于实时进程的调度算法复杂很多，而实时进程在组织架构上与普通进程没有太大差别，也较为简单 组织形式 每个CPU包含一个运行队列结构（struct rq），每个运行队列又包含有其自己的实时进程运行队列（struct rt_rq）、普通进程运行队列（struct cfs_rq）。也就是说每个CPU都有它自己的实时进程运行队列和普通进程运行队列。 组调度（struct task_group）如果有两个进程分别属于两个用户，而进程的优先级不同，会导致两个用户所占用的CPU时间不同，这样显然是不公平的（如果优先级差距很大，低优先级进程所属用户使用CPU的时间就很小），所以内核引入组调度。如果基于用户分组，即使进程优先级不同，这两个用户使用的CPU时间都为50%。如果task_group1中的运行时间还没有用完，会调度task_group1中下一个被调度进程；相反，如果task_group1的运行时间使用结束，则调用上一层的下一个被调度进程。需要注意的是，一个组调度中可能会有一部分是实时进程，一部分是普通进程，这也导致这种组要能够满足即能在实时调度中进行调度，又可以在CFS调度中进行调度。 分组方式： 用户ID：按照进程的UserID进行分组，在对应&#x2F;sys&#x2F;kernel&#x2F;uid&#x2F;目录下会生成一个cpu.share文件，可以通过配置文件来配置用户所占CPU时间的比例 cgroup(control group)：生成组用于限制其所有进程，比如我生成一个组（生成后此组为空，里面没有进程），设置其CPU使用率为10%，并把一个进程丢到这个组中，那么这个进程最多只能使用10%的CPU，如果将多个进程丢到这个组，这个组所有进程评分10% 这里的进程组概念和fork调用产生的父子进程组概念不一样，文章所使用的进程组概念全为组调度中进程组的概念。 调度实体(struct sched_entity) load：权重，是通过优先级转换而成，是vruntime计算的关键 on_rq：表明是否处于红黑树运行队列当中，需要明确的一个观点是，CFS运行队列里面包含有一个红黑树，但这个红黑树并不是CFS运行队列的全部，因为红黑树仅仅是用于选出下一个调度程序的算法。很简单的一个例子，普通程序运行时，其并不在红黑树中，但是还是处于CFS运行队列中，其on_rq为真。只有准备退出，即将睡眠等待和转为实时进程的进程其CFS运行队列的on_rq为假 vruntime：虚拟运行时间，调度的关键，其计算公式：一次调度间隔的虚拟运行时间 &#x3D; 实际运行时间 * （NICE_o_LOAD &#x2F; 权重）。可以看出跟实时运行时间和权重有关，红黑树以此作为排序的标准优先级越高的进程在运行时其vruntime增长的越慢，其可运行时间相对就长，而且也越有可能处于红黑树的最左节点，调度器每次都选择最左边的节点为下一个调度进程。注意其值为单调递增，在每个调度器的时钟中断时当前进程的虚拟运行时间都会累加，单纯的说就是进程们都在比谁的vruntime最小，最小的将被调度。 cfs_rq：此调度实体所处的CFS运行队列 my_q：如果此调度实体代表的时一个进程组，那么此调度实体就包含有一个自己的CFS运行队列，其CFS运行队列中存放的是此进程组中的进程，这些进程就不会再其他CFS运行队列的红黑树选择，如果进程组B还有个子进程组C原理都一样，就是一个层次结构。 调度类，里面包含的是调度处理函数struct sched_class在内核中不同的调度算法他们的操作不同，为了方便修改、替换调度算法，使用了调度类，每个调度算法只需要实现自己的调度类就可以了，CFS算法有他的调度类，SCHED_FIFO也有他自己的调度类，当一个进程创建时，用什么调度算法就将task_struct -&gt; sched_class 指向其相应的调度类，调度器每次调度处理器时，就通过当前进程的调度类函数进行操作，大大提高了可移植性和易修改性。 CFS运行队列（struct cfs_rq）我们现在知道，在系统中至少有一个CFS运行队列，其就是根CFS运行队列，而其他的进程组和进程都包含在此运行队列中，不同的是进程组又有它自己的CFS运行队列，其运行队列中包含的是此进程组中的所有进程。当调度器从根CFS运行队列中选择了一个进程组进行调度时，进程组就会从自己的CFS队列中选择一个调度实体进行调度（这个调度实体可能为进程，也可能为一个子进程组），就这样一直深入，只要确定其代表着一个CFS运行队列，并且包含一个红黑树进行选择调度进程即可。 load：其保存的是进程组中所有进程的权值总和，子进程计算vruntime时有时候需要用到它 CPU运行队列（struct rq）每个CPU都有自己的rq结构，其用于描述在此CPU上所运行的所有进程，其包括一个实时进程队列和一个根CFS运行队列，在调度时，调度器首先会去实时进程队列找是否有实时进程需要运行，如果没有才会去CFS运行队列找是否有进程需要运行。","categories":[{"name":"Linux","slug":"Linux","permalink":"http://jerryblogs.com/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://jerryblogs.com/tags/Linux/"}]},{"title":"汇编语言","slug":"os-compilation","date":"2020-10-31T12:36:31.000Z","updated":"2023-04-20T18:08:09.912Z","comments":true,"path":"2020/10/31/os-compilation/","link":"","permalink":"http://jerryblogs.com/2020/10/31/os-compilation/","excerpt":"","text":"什么是汇编语言？CPU只负责计算，本身不具备智能。你输入一条指令，他就运行一次，然后停下来，等待下一次指令。这些指令都是二进制的，称为操作码（opcode），比如加法指令就是00000011.编译器的作用就是，把高级语言写好的程序，翻译成一条条的操作码。因为二进制的语言是不可读的，所以，产生了汇编语言。 汇编语言是二进制指令的文本形式，与二进制指令是一一对应的关系。比如，加法指令00000011写成汇编语言就是ADD。只要还原成二进制，汇编语言就可以被CPU直接执行，所以它是最底层的低级语言。 汇编语言的来历： 最早，编写程序就是手写指令，然后通过各种开关输入计算机。后来，发明了纸带打孔机，将二进制指令自动输入计算机。为了解决可读性问题，将二进制改为八进制，然而八进制可读性也不行，最后变成用文字表达，内存地址也不再直接引用，而是用标签表示。 将文字指令翻译成二进制步骤称为assembling，完成这个步骤的程序就叫做assembler、。他处理的文本就叫做assembly code。标准化以后，称为assembly language，缩写为asm，中文译为汇编语言。 每一种CPU的机器指令都是不一样的，因此，对应的汇编语言也不一样。 寄存器： CPU本身只负责计算，不负责存储数据。数据一般都是存储在内存中，CPU要用的时候就会去内存读写数据。但是CPU的运算速度要远高于内存的读写速度，为了提高效率，CPU都自带一级缓存和二级缓存。基本上，CPU缓存可以看作是读写较快的内存。但是，CPU缓存还是不够快，另外，数据在缓存中的地址是不固定的，CPU每次读写都要寻址，会拖慢速度。因此，CPU还自带了寄存器，用来存储最常用的数据。也就是说，读写最频繁的数据（循环变量）会被存储在寄存器中，CPU优先读写寄存器，再由寄存器跟内存交换数据。 寄存器不依靠地址区分数据，而是依靠名称。每一个寄存器都有自己的名称，我们告诉CPU去具体的哪一个寄存器拿数据，这样的速度是最快的。 为什么寄存器比内存快？计算机的存储层次之中，寄存器最快，内存次之，最后是硬盘。为什么寄存器比内存快？距离不同（不是主要因素），内存离CPU比较远设计不同，内存的设计相对简单，就是一个电容加上一个晶体管，而寄存器要多几个电子元件。另外，通电以后，寄存器的晶体管一直有电，而内存的晶体管只有用到的才有电，没用到的就没电，这样有利于省电。这些设计上的因素决定了寄存器读取速度比内存要快。工作方式不同：寄存器的工作方式：找到相关的位，并读取这些位。内存的工作方式：找到数据的指针（指针可能放在寄存器内，所以这一步就包括了寄存器的所有工作了），将指针送往内存管理单元（MMU），由MMU将虚拟的内存地址翻译成实际的物理地址，将物理地址送往内存的控制器，由内存的控制器找出该地址应该插在哪一根内存插槽上，确定数据在哪一个内存块上，从该块读取数据，最后，数据先送回内存控制器，再送回CPU，然后开始使用。 寄存器的种类： 早期的 x86 CPU 只有8个寄存器，而且每个都有不同的用途。现在的寄存器已经有100多个了，都变成通用寄存器，不特别指定用途了，但是早期寄存器的名字都被保存了下来。八个寄存器分别是：EAX、EBX、ECX、EDX、EDI、ESI、EBP、ESP前七个都是通用的，ESP寄存器有特定用途，保存当前Stack的地址。 常常看到的32位CPU、64位CPU这样的名称，其实就是寄存器的大小。32位CPU寄存器的大小就是4个字节。 内存模型：Heap（堆） 寄存器只能放少量数据，大多数时候，CPU要指挥寄存器，直接跟内存交换数据。 程序运行的时候，操作系统会给它分配一段内存，用来存储程序和运行时产生的数据。这段内存有起始地址和结束地址，比如从0x1000到0x8000，起始地址是比较小的那个地址，结束地址是比较大的那个地址。程序运行过程中，对于动态的内存占用请求（比如新建对象、或者使用macllo命令），系统就会从预先分配好的那段内存之中，划出一部分给用户，具体规则是从起始地址开始划分。举例来说，用户要求得到10个字节内存，那么起始地址0x100开始分配，一直分配到地址0x100A。 这些因为用户主动请求而划分出来的内存区域，叫做堆。它由起始地址开始，从低位向高位增长。Heap的一个重要特点就是不会自动消失，必须手动释放，或者由垃圾回收机制回收。","categories":[{"name":"操作系统","slug":"操作系统","permalink":"http://jerryblogs.com/categories/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/"}],"tags":[{"name":"操作系统","slug":"操作系统","permalink":"http://jerryblogs.com/tags/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/"}]},{"title":"IO多路复用","slug":"os-epoll","date":"2020-10-31T12:36:31.000Z","updated":"2023-04-20T17:30:04.296Z","comments":true,"path":"2020/10/31/os-epoll/","link":"","permalink":"http://jerryblogs.com/2020/10/31/os-epoll/","excerpt":"","text":"什么是IO多路复用IO多路复用就是通过一种机制，一个进程可以监视多个文件描述符，一旦某个描述符就绪，能够通知程序进行相应的读写操作。 但是select、poll、epoll本质上都是同步IO，都需要在读写事件就绪后自己负责进行读写，这个读写的过程是阻塞的，而异步IO无需自己负责进行读写，异步IO的实现会负责把数据从内核拷贝到内核空间。 epoll不同于select和poll的忙轮询和无差别轮询，epoll会把每个流发生了怎样的IO事件通知我们。 epoll的相关操作： epoll_create()创建一个epoll对象，一般epollfd &#x3D; epoll_create() epoll_ctl ，往epoll对象中增加&#x2F;删除某个流的某一个事件 epoll_wait()等待直到注册事件的发生 当一个非阻塞流的读写发生缓冲区满或缓冲区非空，write&#x2F;read会返回-1，并设置error&#x3D;EAGAIN。而epoll只关心缓冲区非满和缓冲区非空事件 select、poll、epoll之间的区别select在内核中，select实现的是用轮询方法，每次检测都会使用FD_SET中的所有句柄，在高并发的情况下select是会被频繁调用的，这样的方法显然没有效率。 时间复杂度O（n）仅仅知道有IO事件发生，却并不知道是哪几个流，只能轮询所有流，找出能读出数据或者写入数据的流，对他们进行操作。同时处理的流越多，无差别轮询时间越长。 poll时间复杂度O（n） 本质上和select没有区别，它将用户传入的数组拷贝到内核空间，然后查询每个fd对应的设备状态，但是没有最大连接数的限制，原因是它是基于链表来存储的。 epoll可以理解为event poll不同于忙轮询和无差别轮询，会把那个流发生怎样的IO事件通知我们。实际上是事件驱动，每个事件关联上fd。 epoll的三大关键要素：mmap、红黑树、链表 本质上都是同步IO，因为它们都需要在读写事件时自己负责进行读写，这个读写过程是阻塞的。 epoll是通过内核与用户空间mmap同一块内存实现的，mmap将用户空间的一块地址和内核空间的一块地址映射到同一块相同的物理地址，使得这块物理内存地址相对于内核和用户都是可见的，减少用户态和内核态之间的数据交换，内核可以直接看到epoll监听的句柄，效率高。 红黑树将存储epoll所监听的套接字，当添加或储存一个套接字的时候都在红黑树上面去处理，红黑树本身插入和删除性能都比较好，时间复杂度O(logN) 通过epoll_ctl函数把添加进来的事件都会被放在红黑树的某个节点内，事件会与相应的设备驱动程序建立回调关系，当相应的事件发生后，就会调用这个回调函数，该回调函数在内核中被称为ep_epoll_callback，这个回调函数把这个事件添加到了rdllist这个双向链表中，一旦有事件发生，epoll就会将该事件添加到双向链表中。当我们调用epoll_wait时，epoll_wait只需要检查双向链表中是否存在注册的事件，效率非常可观，这里需要将发生了的事件复制到用户态内存中即可。 Python中的asynciopython的time.sleep()与asyncio.sleep()不兼容，会在休眠时间内停止所有运行的内容。 使用await和return创建协程函数，调用协程函数必须先用await获取结果 yield在async def中使用不常见，会创建一个异步生成器。可以使用async for进行迭代，暂时不要使用yield 不可以使用yield from await只能在async中使用 使用队列queuq.Queue以保证线程安全 使用async调度协程 默认情况下，异步IO事件循环在单个CPU内核上运行，通常情况下单核CPU运行一个单线程事件循环绰绰有余、 事件循环是可以插入的 asyncio的Event Loop维护一个事件列表，一个事件：回调函数的映射关系，一个计时器，计时器：回调的映射关系。轮询事件列表的事件状态，当某事件发生时，执行回调函数，并唤醒协程 当一个计时器事件注册时，会创建一个future对象，为这个future对象创建一个回调函数，当时间到达时，执行这个回调函数，唤醒协程并且将事件的结果返回给事件，需要注意的是内部的协程暂停了，恢复时需要从最外层的协程开始执行。 asyncio这个库做了什么，没做什么 控制流的暂停与恢复是通过python内部的generator来实现的 协程链，把不同协程链接连接在一起的机制，是通过python内置的支持，即async&#x2F;await，或者说是生成器的yield from Event Loop，这个是asyncio实现的，决定了我们能对什么事件进行异步操作，目前只支持定时器和网络IO操作 协程链的控制与恢复，即内部协程暂停了，恢复时需要从最外层的协程开始恢复 异步请求使用requests与asyncio不兼容，使用aiohttp模块： Python的requests模块与异步IO不兼容，requesst是在urllib3之上构建的，后者使用了http和socket模块 默认情况下，socket操作处于阻塞状态，像requests.get()不是一个可等待对象，而aiohttp是一个可等待的协程。","categories":[{"name":"操作系统","slug":"操作系统","permalink":"http://jerryblogs.com/categories/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/"}],"tags":[{"name":"操作系统","slug":"操作系统","permalink":"http://jerryblogs.com/tags/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/"}]},{"title":"基础概念","slug":"os-what","date":"2020-10-31T12:36:31.000Z","updated":"2023-04-20T17:28:12.671Z","comments":true,"path":"2020/10/31/os-what/","link":"","permalink":"http://jerryblogs.com/2020/10/31/os-what/","excerpt":"","text":"BIOS——Basic Input&#x2F;Output System 基本输入输出系统在IBM PC兼容系统上，是一种业界标准的固件接口。[1]。BIOS这个字眼是在1975年第一次在CP&#x2F;M操作系统中出现。BIOS是个人计算机启动时加载的第一个软件（实际上自Intel Haswell平台以后，UEFI并不是系统引导时第一个被加载的软件）。 UEFI——Unified Extensible Firmware Interface，统一可拓展固件接口是一种个人计算机系统规格，用来定义操作系统与系统固件之间的软件界面，作为BIOS的替代方案[1]。可扩展固件接口负责加电自检（POST）、联系操作系统以及提供连接操作系统与硬件的接口。固件（英语：firmware），是一种嵌入在硬件设备中的软件。通常它是位于特殊应用集成电路（ASIC）或可编程逻辑器件（PLD）之中的闪存或EEPROM或PROM里，有的可以让用户更新。可以应用在非常广泛的电子产品中，从遥控器、计算器到计算机中的键盘、硬盘，甚至工业机器人中都可见到它的身影。 为什么要有操作系统？用户几乎不可能使用裸机计算机硬件只能识别0、1二值机器码机器码直观性差。容易出错，难于交流因此，通常在计算机硬件上会覆盖一层软件，以便用户使用计算机硬件 CPU是计算机硬件的核心，是计算机系统的心脏操作系统则是计算机软件的核心，是计算机系统的大脑操作系统是整个计算机的控制中心，是计算机中首要的、最重要的、最复杂的系统软件 什么是操作系统？操作系统是管理计算机硬件的程序在计算机用户和计算机硬件之间起媒介作用的一种程序系统部件的抽象观点：硬件之上的第一层软件，是对硬件的首次扩充，又是其他软件运行的基础。计算机系统部件：硬件——提供基本的运算资源操作系统——在各种应用程序和用户之间控制与协调对硬件的使用应用程序——定义用户解决问题的资源使用方式（编译、数据库、视频程序、事务程序等）用户——人、机、其他计算机 操作系统目标：执行用户程序并使用户问题更易解决使计算机系统更易使用。以一种有效率的方式使用硬件 操作系统的其他定义：资源分配者——管理和分配资源控制程序——控制用户程序的运行和I&#x2F;O设备的操作内核——在全时运行的一个程序（其他的是应用），开机时最先打开的是操作系统，也是关机时最后关闭的 操作系统的作用：计算机硬件、软件资源的管理者用户使用计算机硬件、软件的接口 对用户——接待员、对系统——管家婆 操作系统（operating system）操作系统发展：os从无到有、从简单到复杂、完善os随着计算机技术的发展而发展为满足不同的需求，出现了多种类型的os 操作系统的发展过程：发展动力——需求推动发展 • 提高资源利用角度——为了提高计算机资源利用率和系统性能，从单道到多道、从集中到分布、从专用到泛在等 • 用户角度——方便用户、人机交互 • 技术角度——物理器件发展，CPU的位宽度（指令和数据）、快速补充、光器件等，以及计算机体系结构的不断发展：单处理机、多处理机、多核、计算机网络 Os的发展和计算机硬件技术、体系结构相关 第一代（1946年-1955年）：真空管时代，无操作系统第二代（1955年-1965年）：晶体管时代、批处理系统第三代（1965年-1985年）：集体电路时代，多道程序设计第四代（1980年-至今）：大规模和超大规模集成电路时代，分时系统。现代计算机正在向着巨型、微型、并行、分布、网络化和智能化几个方面发展。 操作系统类型的发展：无操作系统——简单批处理系统——多道程序批处理系统——分时系统——实时系统——嵌入式系统——并行系统——分布式系统 1.手工操作 1946~50年代（真空管）——集中计算（计算中心）——计算机资源昂贵——用户独占全机 ENIAC计算机运算速度：1000次每秒，数万个真空管，占地100平方米没有程序设计语言（甚至没有汇编），更谈不上操作系统 工作方式：人工操作方式，用户是计算机专业人员；工作方式： 编程语言：机器语言； I&#x2F;O纸带或卡片 缺点：用户独占全机，独占操作系统的全部硬件资源，设备利用率很低CPU等待用户：人工装入&#x2F;卸取纸带或卡片 主要矛盾：人机矛盾——人工操作方式与机器利用率的矛盾CPU与I&#x2F;O速度之间不匹配的矛盾提高效率的途径：批处理脱机I&#x2F;O 脱机I&#x2F;O方式：I&#x2F;O工作在外围机&#x2F;卫星机的控制下完成，或者说是在脱离主机的情况下进行。使用磁带作为输入输出的中介，这种具体的输入&#x2F;输出不需要在主计算机上进行的方式称“脱机输入&#x2F;输出” 2.单道批处理系统（50年代末~60年代中）晶体管把一批作业以脱机输入方式输入到磁带&#x2F;磁鼓利用磁带或磁鼓把任务分类编成作业顺序执行每批作业由专门监督程序自动依次处理 运行特征：顺序性磁带上的各道作业是顺序地进入内存，各作业的完成顺序与他们进入内存的顺序相同单道性：内存中仅有一道程序运行自动性 优点：减少了CPU的空闲时间，提高了主机CPU和I&#x2F;O设备的利用效率，提高了吞吐量缺点：CPU和I&#x2F;O设备使用忙闲不均 2.多道程序批处理系统多道程序设计——60年代中~70年代中（集成电路）多道——内存中同时存放几个作业，使之都处于执行的开始点和结束点之间的多个作业共享CPU、内存、外设等资源目的——利用多道批出理提高资源的利用率 60年代通道和中断技术的出现解决了输入输出等待计算的问题 通道——是一种专用部件，负责外部设备与内存之间信息的传输中断——指主机或接到外界的信号（来自CPU外部或内部）时，立即中止原来的工作，转去处理这一外来事件，处理完后，主机又回到与原来工作点继续工作 在内存中同时有多个作业，CPU在其中切换只要操作系统总是存在可执行的作业，CPU就永远不会因无事可干而闲着多道通过组织作业使得CPU总在执行其中一个作业，从而提高了CPU的利用率 运行特征：多道性——内存中同时驻留多道程序并发执行，从而有效地提高了资源利用率和系统吞吐量无序性——作业的完成顺序与它进入内存的顺序之间无严格的对应关系调度性——作业调度、进程调度 优点——资源利用率高:CPU，内存，I&#x2F;O设备 系统吞吐量大缺点——无交互能力，用户响应时间长 作业平均周转时间长 多道程序对os特点的要求：存储管理——系统必须要为若干作业分配空间CPU调度——系统必须要在就需作业中挑选资源竞争和共享设备分配系统提供I&#x2F;O程序文件管理 • 分时系统 在多道的基础上的新需求：多任务处理(多用户)——交互服务 70年代中期至今分时是指多个用户分享同一台计算机，分时共享硬件和软件资源 实现方式：1.多用户分时——单个用户使用计算机的效率很低，因此允许多个应用程序同时在内存中，分别服务于不同的用户。有用户输入时由CPU执行，处理完一次输入后程序暂停，等待下一次用户输入—时走时停2.前台和后台程序分时：后台程序不占用终端输入输出，不与用户交互—现在的图形用户界面（GUI），除当前交互的程序之外，其他程序均作为后台 通常按时间片分配：各个程序在CPU执行的轮换时间 主机——即使接受，即使处理 作业直接进入内存每个作业一次只运行很短的时间分时技术：把CPU的响应时间分成若干个大小相等（或不等）的时间单位，称为时间片（如100ms），每个终端用户获得CPU（获得一个时间片）后开始运行，当时间片到，该应用程序暂停运行，等待下一次运行 分时系统的特点人机交互性好、共享主机：多个用户同时使用 分时系统的特点：多路性——众多联机用户可以使用同一台计算机独占性——各终端用户感觉到自己独占了计算机交互性——用户与计算机之间可进行“会话”及时性——用户的请求能在很短的时间内获得响应 在分时系统的基础上，操作系统的发展开始分化，如实时系统，通用（桌面）系统，网络系统、分布式系统等","categories":[{"name":"操作系统","slug":"操作系统","permalink":"http://jerryblogs.com/categories/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/"}],"tags":[{"name":"操作系统","slug":"操作系统","permalink":"http://jerryblogs.com/tags/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/"}]},{"title":"Unix环境高级编程-守护线程","slug":"unix-daemon","date":"2020-10-31T12:36:31.000Z","updated":"2023-04-20T18:07:24.712Z","comments":true,"path":"2020/10/31/unix-daemon/","link":"","permalink":"http://jerryblogs.com/2020/10/31/unix-daemon/","excerpt":"","text":"概念是生存期长的一种进程，常常在系统引导装入时启动，仅在系统关闭时终止，没有控制终端，所以在后台运行 守护进程的特征系统进程依赖于操作系统实现，父进程ID为0的各进程通常是内核进程，作为系统装入过程的一部分而启动（init进程是个例外，由内核在引导装入时启动用户层次的命令）内核进程是特殊的，通常存在于系统的整个生命周期中，一超级用户权限运行，无控制终端，无命令行。 Linux使用一个名为kthreadd的内核进程来创建其他的内核进程，所以kthreadd为其他内核进程的父进程。对于需要在内核上下文执行工作但却不被用户层上下文调用的每一个内核组件，通常有自己的内核守护进程。 kswapd守护进程也叫内核换页守护进程。支持虚拟内存子系统在经过一段时间后将脏页面慢慢写回磁盘来回收这些页面。 flush守护进程在可用内存达到设置的最小阈值时将脏页面冲洗至磁盘。也定期将脏页面冲洗回磁盘来减少在系统出现故障时发生的数据丢失。多个冲洗守护进程可以同时存在，每个写回的设备都有一个冲洗守护进程。 sync_supers守护进程定期将文件系统元数据冲洗至磁盘 jbd守护进程帮助实现了ext4文件系统中的日志功能 rpcbind守护进程提供将远程调用（RPC）程序号映射为网络端口号的服务。rsyslogd守护进程将系统消息记入日志的任何程序使用 inetd守护进程侦听网络系统接口，以取得来自网络的对各种网络服务进程的请求 cron守护进程在定期安排的日期和时间执行命令 大多数进程都以超级用户权限运行，所有的守护进程都没有控制终端，其控制终端设置为问号，内核守护进程以无控制终端方式启动。用户级守护进程是调用了setsid。用户级守护进程的父进程是init进程。 守护进程如何让处理出错记录需要有一个集中的守护进程出错记录设施 syslog设施得到了广泛的应用，大多数守护进程都是使用这一设施，有以下3种产生日志消息的方法： 内核例程调用log函数。任何一个进程都可以打开并读取&#x2F;dev&#x2F;klog设备来读取这些消息。大多数用户守护进程调用syslog函数来产生日志消息，这些消息被发送至&#x2F;dev&#x2F;log 无论一个用户进程是在此主机上，还是在通过TCP&#x2F;IP网络连接到其他主机上，都可以将日志消息发向UDP端口514。 通常syslogd守护进程读取所有3种格式的日志消息。此守护进程在启动时读一个配置文件，文件名一般为&#x2F;etc&#x2F;syslog.conf，决定了不同种类的消息应送向何处 单实例守护进程为了正常运作，某些进程可能会实现为，在任意时刻只运行该守护进程的一个副本。（对于cron进程来说，如果有多个实例运行，那么每个副本都可能试图开始某个预定的操作，于是造成该操作的重复执行，可能导致出错）文件和记录锁机制为一种方法提供了基础，该方法保证一个守护进程只有一个副本在运行。如果每一个守护进程创建一个有固定名字的文件，并在该文件的整体上加一把写锁，那么只允许创建一把这样的写锁，在此之后创建的写锁的尝试都会失败。 文件和记录锁提供了一种方便的互斥机制，如果守护基础在一个文件的整体上得到一把写锁，那么在该守护进程终止时，这把锁将被自动删除。简化了复原所需的处理，去除了对以前的守护进程实例需要进程清理的有关操作。 守护进程的惯例 若使用锁文件，那么该文件通常储存在&#x2F;var&#x2F;run目录下。 若守护进程支持配置选项，那么该配置通常存放在&#x2F;etc目录下 守护进程可用命令行启动，但他们通常是由系统初始化脚本之一（&#x2F;etc&#x2F;rc*或者是&#x2F;etc&#x2F;init.d&#x2F;*）启动的 若一个守护进程有一个配置文件，那么当该守护进程启动时会读取该文件，在此之后一般不会再查看它。当某个管理员更改了配置文件，那么该守护进程可能需要被停止，然后再启动，以使该配置文件更改生效。为避免麻烦，有些守护进程通常将捕捉SIGHUP信号，当他们接收到该信号时，重新读配置文件。 客户进程-服务器进程模型在服务进程中调用fork然后exec另一个程序来向客户进程提供服务是很常见的，这些服务进程通常管理着多个文件描述符：通信端点、配置文件、日志文件和类似文件。通常情况下，让子进程中的这些文件描述符保持打开状态并无大碍，他们可能不会被在子进程中执行的程序中所使用，尤其是那些与服务器端无关的程序最坏情况下，保持打开状态会导致安全问题，如更改服务器端或欺骗客户端程序使其认为正在与服务器端通信，从而获取未授权的信息。 解决此问题的一个方法是对所有被执行程序不需要的文件描述符执行时关闭close-on-exec标志。","categories":[{"name":"Linux","slug":"Linux","permalink":"http://jerryblogs.com/categories/Linux/"},{"name":"Unix环境高级编程","slug":"Linux/Unix环境高级编程","permalink":"http://jerryblogs.com/categories/Linux/Unix%E7%8E%AF%E5%A2%83%E9%AB%98%E7%BA%A7%E7%BC%96%E7%A8%8B/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://jerryblogs.com/tags/Linux/"},{"name":"Unix","slug":"Unix","permalink":"http://jerryblogs.com/tags/Unix/"}]},{"title":"Unix环境高级编程-文件和目录","slug":"unix-file-dir","date":"2020-10-31T12:36:31.000Z","updated":"2023-04-20T18:07:35.210Z","comments":true,"path":"2020/10/31/unix-file-dir/","link":"","permalink":"http://jerryblogs.com/2020/10/31/unix-file-dir/","excerpt":"","text":"文件和目录描述文件系统的其他特征和文件的性质 函数 stat、fstat、fstatat和lstat一旦给出pathname，stat函数将返回与此命名文件有关的信息结构。 fstat函数获得已在描述符fd上打开该文件的有关信息。 lstat返回该符号链接的有关信息，而不是由该链接引用的文件的信息 fstatat函数为一个相对于当前打开目录的路径名返回文件统计信息，当AT_SYMLINK_NOFOLLOW标志被设置在flag参数时，fstatat不回去跟随符号链接，而是去返回符号链接本身的值。在默认情况下，返回的是符号链接所指向的实际文件的信息。 第二个参数buf是一个指针，指向一个特定的结构体stat 文件类型UNIX系统大多数文件包含以下几种类型： 普通文件：文本或二进制数据等。二进制可执行文件必须遵循一种标准化的格式，使内核能够理解其格式。 目录文件：这种文件包含了其他文件的名字和指向这些与文件有关信息的指针。对一个目录有读权限的进程可以读该目录的内容，但是只有内核才能直接写目录文件。 块特殊文件：提供对设备带缓冲的访问，每次以固定长度进行。 字符特殊文件。提供对设备不带缓冲的访问。系统中的所有设备要么是字符特殊文件，要么是块特殊文件。 FIFO：用于进程间通信，又是也命名为管道。 套接字：用于进程之间的网络通信。 符号链接：指向另一个文件。 文件类型信息包含在stat结构的st_mode成员中。 POSIX.1允许将进程间通信对象（如信号量或消息队列）说明为文件 设置用户ID和组ID 第一个框在登陆时取自口令文件中的登录项。 第二个框决定访问权限。 第三个框在执行一个程序时包含了有效用户ID和有效组ID的副本。 通常，有效用户ID等于实际用户ID，有效组ID等于实际组ID。 设置用户ID位（S_ISUID）：在文件模式字（st_mode）中指定，含义是，当执行此文件时，将进程有效用户ID设置为文件所有者的ID。 设置用户组位（S_ISGID）：在文件模式字中指定，含义是，当执行此文件是，将进程有效用户ID设置为文件所有者的ID。 所以，当文件所有者是超级用户，而且设置了该文件的设置用户ID位，当该文件由一个进程执行时，该进程具有超级用户权限。例如passwd命令允许用户改变其口令，通常是将用户的新口令写入&#x2F;etc&#x2F;passwd或者&#x2F;etc&#x2F;shadow中，只有超级用户才拥有对此文件的写权限，这时候需要设置用户ID功能。 文件访问权限 chmod命令用u表示用户，用g表示组，用o表示其他 权限的访问规则是： 用名字打开任意类型的文件时，对该名字中包含的每一个目录，包括可能隐含的当前工作目录都应该具有执行权限。 目录的读权限和执行权限的意义不同。读权限允许我们读目录，获得所有文件名的列表，但是没有执行权限我们无法进入这个目录。 读权限决定我们是否能够打开现有文件进行读操作。 写权限决定我们是否能够打开现有文件进行写操作。 为了在open函数中对一个文件指定O_TRUNC标志，必须具有写权限。 为了在一个目录中创建一个新文件，必须对该目录具有写权限和执行权限。 为了删除目录下的一个文件，必须对该目录下具有写权限和执行权限。对文件本身不需要读写权限。 如果用7和exec函数中任何一个执行某文件，必须对该文件具有执行权限，文件必须是一个普通文件。 进程每次打开、创建或删除一个文件时，内核进行文件访问权限测试。 新文件和目录的所有权新文件的用户ID设置为进程的有效用户ID，组ID允许实现下列选项之一： 新文件组ID可以是进程的有效组ID 新文件的组ID可以是所在目录的组ID 函数 access 和 faccessat当用open函数打开一个文件，内核以进程的有效用户ID和有效用户组ID进行权限测试。但当一个进程可能通过设置用户ID和设置用户组ID以超级用户权限运行时，仍可能想验证其实际用户能否访问一个给定的文件。access和faccessat用来进行访问权限测试。 函数umask为文件模式创建屏蔽字 函数 chmod、fchmod和fchmodatchmod函数在指定的文件上进行操作 fchmod对已打开的文件进行操作 fchmodat： pathname参数为绝对路径，忽略fd fd的取值为AT_FDCWD而pathname为相对路径 为了改变一个文件的权限位，进程的有效用户ID必须等于文件的所有者ID，或者该文件必须具有超级用户权限。 chmod只是更新i节点最近一次被修改的时间，而按照系统默认方式，ls -l只是列出最后修改文件内容的时间。 黏着位S_ISVTX 交换区：Linux为了提高读写效率与速度，会将文件在内存中进行缓存，这部分缓存就是Cache Memory。即使程序运行结束，Cache Memory也不会自动释放。这会导致Linxu系统在频繁读写文件后，物理内存不够用，这时候需要将物理内存释放出来，这些释放的空间被临时保存在Swap空间中，等到程序需要运行时，才将文件从Swap分区中恢复到内存中，系统总是在物理内存不够的时候，才进行Swap交换。 当文件设置了S_ISVTX时，程序第一次被运行，终止时，程序正文副本依旧被保存在交换区中。比如说文本编译程序和C语言编译器。 现如今使用了虚拟内存技术不再需要设置黏着位。 如果对一个目录设置了黏着位，只有对该目录具有写权限的用户且满足下列条件之一才能删除或重命名该文件： 拥有此文件 拥有此目录 是超级用户 在目录&#x2F;tmp和&#x2F;var&#x2F;tmp设置了黏着位，但是用户并不能随意删除别人的文件。 函数chown、fchown、fchownat和lchown改变用户ID和组ID 文件中的空洞空洞是由所设置的偏移量超过文件尾端，并写入了某些操作所造成的。 文件会由于使用了若干块磁盘块以存放指向实际数据块的各个指针 文件截断使用truncate命令进行截断 如果设置的length长度比原来大，会创建一个文件空洞 文件系统我们可以把一个磁盘分区，每个分区可以包含一个文件系统 每个i节点都有一个链接计数，其值是指向i节点的目录项数，当链接计数减少至0时，才可删除该文件。解除对一个文件的计数并不总意味着释放该文件占用的数据块的原因。删除一个目录项的函数是unlink不是delete。 nlink_t这种链接类型被称为硬链接 符号链接的实际内容是包含了该符号链接所指向文件的名字 i节点中包含文件类型、文件权限访问位、文件长度、指向文件数据块的指针等。 只有两项重要数据存放在目录项：文件名和i节点编号。 目录项中i节点编号指向同一文件系统中的相应的i节点，所以一个目录项不能指向另一个文件系统中的i节点。所以命令ln不能跨越文件系统。 文件重命名实际上是构造一个指向现有i节点的新目录项，并删除老目录项。链接计数不会改变，文件内容无需移动。mv命令通常的操作方式。 任何一个叶目录（不包含任何目录的目录）的链接计数总是2，一个是包含此目录的目录项，一个是该目录中的.项。在父目录中的每个子目录都使得该父目录链接计数增加1。 函数 link、linkat、unlink、unlinkat和remove创建一个指向现有文件的链接时link或linkat unlink用于删除一个现有的目录项 当链接计数达到0可以删除这个文件，当删除另一个文件时如果有进程打开了该文件，其内容也不能删除。 内核首先检查打开文件的进程个数，如果计数达到0，内核再去检查其链接计数，如果链接计数也是0，那么就删除这个文件。 函数rename和renameat符号链接符号链接与硬链接不同，硬链接直接指向i节点 mkdir rmdir如果调用rmdir使目录链接计数为0，并且也没有进程打开此目录，则释放此目录占用的空间。如果链接计数为0时仍有进程打开此目录，则在函数返回前删除最后一个目录链接及.和..项。且在此目录中不能再创建新文件。 读目录有访问权限的都可以读目录，但是只有内核能够写目录。一个目录的写权限位和执行权限位决定能否创建文件及删除文件，但是并不表示能够写目录本身。 chdir、fchdir、getcwdchdir用来改变工作目录（&#x2F;etc&#x2F;passwd中第六个字段） shell当前工作目录并不会随着程序调用而改变，shell应该直接调用chdir函数，cd命令内建在shell当中 设备特殊文件每个文件系统所在的存储设备都由其主次设备号表示，主设备号标识设备驱动程序，有时为与其通信的外设板，次设备号标识特定的子设备。","categories":[{"name":"Linux","slug":"Linux","permalink":"http://jerryblogs.com/categories/Linux/"},{"name":"Unix环境高级编程","slug":"Linux/Unix环境高级编程","permalink":"http://jerryblogs.com/categories/Linux/Unix%E7%8E%AF%E5%A2%83%E9%AB%98%E7%BA%A7%E7%BC%96%E7%A8%8B/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://jerryblogs.com/tags/Linux/"},{"name":"Unix","slug":"Unix","permalink":"http://jerryblogs.com/tags/Unix/"}]},{"title":"Unix环境高级编程-信号","slug":"unix-signal","date":"2020-10-31T12:36:31.000Z","updated":"2023-04-20T18:07:48.104Z","comments":true,"path":"2020/10/31/unix-signal/","link":"","permalink":"http://jerryblogs.com/2020/10/31/unix-signal/","excerpt":"","text":"信号信号是软件中断 信号概念很多条件产生信号： 当用户按某些键时，引发终端产生的信号（delete或者是Ctrl+C）产生中断信号SIGINT 硬件异常产生信号；除数为0、无效的内存引用等。由硬件检测到，并通知内核，然后内核为该条件发生时正在运行的进程产生适当的信号。 进程调用kill函数可将任意信号发送给另一个进程或进程组 用户可用kill命令将信号发送给其他进程 当检测到某种软件条件已经发生，应当立即通知有关其他进程页产生信号。","categories":[{"name":"Linux","slug":"Linux","permalink":"http://jerryblogs.com/categories/Linux/"},{"name":"Unix环境高级编程","slug":"Linux/Unix环境高级编程","permalink":"http://jerryblogs.com/categories/Linux/Unix%E7%8E%AF%E5%A2%83%E9%AB%98%E7%BA%A7%E7%BC%96%E7%A8%8B/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://jerryblogs.com/tags/Linux/"},{"name":"Unix","slug":"Unix","permalink":"http://jerryblogs.com/tags/Unix/"}]},{"title":"Unix环境高级编程-进程","slug":"unix-process","date":"2020-10-31T12:36:31.000Z","updated":"2023-04-20T18:07:44.956Z","comments":true,"path":"2020/10/31/unix-process/","link":"","permalink":"http://jerryblogs.com/2020/10/31/unix-process/","excerpt":"","text":"进程标识ID为0的进程通常是调度进程，常常被称为交换进程，是内核的一部分，并不执行磁盘中的程序，常被称作系统进程。 ID为1的进程是init进程，在自举结束时由内核调用。init进程通常读取与进程有关的初始化文件（&#x2F;etc&#x2F;init.d等）并将系统引导到一个状态。init进程不会终止，他是一个用户进程，但是以超级用户特权运行。 ID为2的进程是页守护进程，负责虚拟存储器系统的分页操作。 虚拟存储器CPU通过发出虚拟地址，虚拟地址再通过MMU翻译成物理地址，最后获得数据 将一块连续的内存，直接使用不符合实际，将一定大小的地址分成一页，通过页来访问数据，把每一页的首地址作为入口，即PTE来检索页，用页表来管理页。 函数fork一个现有的进程可以调用fork创建一个新进程，fork函数被调用一次，但返回两次，在子进程返回0，在父进程返回子进程的ID，因为没有其他的函数可以获取所有子进程的ID，而父进程总是可以调用getppid获取父进程的ID。 子进程是父进程的副本，子进程获取父进程的进程空间、堆和栈的副本，但他们之间并不共享这些存储空间部份，只共享正文段。 正文段C程序的存储空间布局正文段 CPU执行的机器指令部分，通常是可共享的，即使是频繁执行的程序也只需要有一个副本，通常是只读的。例如，有几个用户都在使用文本编辑器，在内核中只需要该指令的一个副本即可。 初始化数据段 包含了程序中明确的赋值的变量，例如C函数中任何函数之外的声明。 未初始化数据段 bss段，在程序开始之前，内核将此段中的数据初始化为0或空指针。 栈 自动变量以及每次函数调用所需保存的信息都放在此段中，比如每次调用返回地址以及调用者的环境信息都存放在栈中；然后最近被调用的函数在栈上为其自动和临时变量分配空间。 堆 通常在堆中进行动态存储分配。 在fork之后通常跟随着exec，很多实现并不执行一个父进程数据段、栈和堆的完全副本，而是使用了写时复制技术。这些区域父子进程可以共享，但是如果任一方修改了其中的数据，只为修改的部份制作一个副本，通常是虚拟存储中的一“页”。 当标准输出、输入连接到终端设备时，它是行缓冲的，否则是全缓冲的。 文件共享fork的一个特性是所有打开的文件描述符都被复制到子进程中。父进程和子进程共享一个文件表项。 子进程继承父进程下列属性： 文件表项 实际用户ID、实际组ID、有效用户ID、有效组ID 附属组ID 进程组ID 会话ID 控制终端 设置用户ID标志和设置组ID标志 当前工作目录 根目录 文件模式和创建屏蔽字 信号屏蔽和安排 对任意打开文件描述符的执行时关闭标志 环境 连接的共享存储段 存储映像 资源限制 区别如下： fork的返回值不同 进程ID不同 父进程的ID不同 子进程不继承父进程的文件锁 子进程的未处理闹钟被清除 子进程未处理的信号集设置为空集 fork失败的两个原因： 系统中有太多的进程 实际用户ID的进程总数超过了限制 fork的两种用法： 网络服务中父进程在收到请求后fork一个子进程处理这个请求，父进程继续等待下一个请求。 一个进程要执行不同的程序，在shell中是常见情况，子进程在fork返回后立即调用exec 函数 exit不管进程以何种形式终止，都会执行同一段代码，即关闭所有打开的文件描述符，释放所使用的存储器退出状态：main函数返回值，或者是传递参数给exit、_exit、_Exit的参数 终止状态：调用_exit或者_Exit最后返回给父进程的值 孤儿进程：父进程在子进程之前终止。这种情况下，该子进程会被init进程收养，保证所有进程都有一个父进程。内核中是这样处理的：在一个进程终止时，内核会检查所有活动的进程，然后判断它是否是要终止进程的子进程，如果是，则该进程的父进程ID更改为1。 僵死进程：子进程在父进程之前终止，父进程可以用wait或者waitpid函数获取它的内核为子进程保存的信息。至少包括进程ID、终止状态以及该进程使用的CPU时间总量。一个已经终止，但是父进程还未对其进行善后处理的进程为僵死进程。 孤儿进程并不会有什么危害，因为孤儿进程会被init进程接管，init进程会定时的调用wait函数去获取孤儿进程的终止状态，内核随机释放所占用的数据结构。产生僵尸进程的元凶是父进程，父进程没有及时处理已经终止的子进程，导致进程ID和空间持续占用，这时候只需要调用kill函数发送SIGTERM或者是SIGKILL信号杀死父进程，僵尸进程也就变成了子进程，子进程会被init函数接管，最终被销毁，总的来说，两种方式： 信号机制，子进程终止时父进程会收到一个SIGCHLD信号，在父进程中进行处理 fork两次，使子进程变成孤儿进程 init函数不会产生僵死进程，因为只要有一个子进程终止，init就会调用一个wait函数取得其终止状态。一个init的子进程可能指init函数产生的子进程，也可能指孤儿进程。 函数wait和waitpid当一个进程正常或异常终止时，内核就向其父进程发送一个SIGCHLD信号。 调用wait或者是waitpid的进程发生了什么： 如果其所有子进程都还在运行，则阻塞 如果一个进程已终止，父进程取得他的终止状态立即返回 如果没有自己进程则立即出错返回 两个函数区别如下： wait使其调用者阻塞，但是waitpid有一个选项可是其调用者不阻塞 waitpid可以控制它所等待的进程 如果等待的进程是一个僵死进程，则wait立即返回，如果其有多个子进程，在第一个子进程终止时返回。 core文件介绍在一个进程崩溃时，一般会在指定目录下生成一个core文件。core文件仅仅是一个内存映像（同时加上调试信息），主要用来调试。 竞争条件当多个进程都企图对共享数据进行处理，而最后的结果又取决于进程运行的顺序时，我们认为发生了竞争条件。 函数exec使用fork函数创建一个新的子进程后，子进程往往要调用另一种exec函数执行另一个程序。当进程调用exec后，该进程执行的程序完全替换为新程序，新程序从main函数开始执行。 进程会计进程调度调度策略和调度优先级是由内核确定的，进程可以选择nice值降低对CPU的占用，特权进程允许提高权限 进程组进程组是一个或多个进程的集合，通常他们是在同一种作业结合起来的。同一进程组的各进程接受来自同一个登陆终端的各种信号，每个进程组有唯一一个进程组ID 每个进程组有一个组长进程，组长进程的ID等于其进程ID 该进程组长可以创建一个进程组、创建该组中的进程、然后终止。只要某进程组中有一个进程存在，该进程组就存在。这与其组长进程组是否终止无关 一个进程只能为他自己或它的子进程设置进程组ID，在他的子进程调用了exec后就不能再更改子进程的进程组ID 会话会话是一个或多个进程组的集合 通常是由shell的管道降级各进程编成一组","categories":[{"name":"Linux","slug":"Linux","permalink":"http://jerryblogs.com/categories/Linux/"},{"name":"Unix环境高级编程","slug":"Linux/Unix环境高级编程","permalink":"http://jerryblogs.com/categories/Linux/Unix%E7%8E%AF%E5%A2%83%E9%AB%98%E7%BA%A7%E7%BC%96%E7%A8%8B/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://jerryblogs.com/tags/Linux/"},{"name":"Unix","slug":"Unix","permalink":"http://jerryblogs.com/tags/Unix/"}]},{"title":"Unix环境高级编程-线程","slug":"unix-thread","date":"2020-10-31T12:36:31.000Z","updated":"2023-04-20T18:07:50.845Z","comments":true,"path":"2020/10/31/unix-thread/","link":"","permalink":"http://jerryblogs.com/2020/10/31/unix-thread/","excerpt":"","text":"线程线程概念每个线程都包含有表示执行环境所必需的信息，其中包括进程中标识线程的线程ID、一组寄存器值、栈、调度优先级和策略、信号屏蔽字、error变量以及线程私有数据。一个进程的所有信息对线程都是共享的，包括可执行程序的代码、程序的全局内存和堆内存、栈以及文件描述符。 线程同步互斥量本质上是一把锁，在访问共享资源之前对互斥量进行加锁操作，访问完成后释放互斥量。 避免死锁死锁系统中若干线程形成环路，该环路中每个线程都在等待相邻线程正在占用的资源。 避免方式： 通过控制互斥量加锁的顺序来避免死锁的发生（假设需要对两个互斥量AB加锁所有线程总是在对互斥量B加锁之前锁住互斥量A，那么这两个互斥量就不会产生死锁） 先释放占有的锁再重试 读写锁（共享互斥锁）读写锁允许更高的并行性。 读写锁有三种状态： 读模式下加锁状态 写模式下加锁状态 不加锁状态 一次只有一个线程可以占用写模式的读写锁，但是可以有多个线程占用读模式的读写锁。 当读写锁在写加锁状态时，所有对这个锁加锁的线程都会被阻塞。当读写锁在读加锁状态时，所以视图以读模式对它进行加锁的线程都可以得到访问权，但是任何以写模式进行加锁的线程都会被阻塞，直到所有的线程释放他们的读锁为止。当读写锁处于以读模式锁住的状态，这时有一个线程试图以写模式进行加锁时，读写锁通常会阻塞随后的读模式请求。这样可以避免读模式锁的长期占用。","categories":[{"name":"Linux","slug":"Linux","permalink":"http://jerryblogs.com/categories/Linux/"},{"name":"Unix环境高级编程","slug":"Linux/Unix环境高级编程","permalink":"http://jerryblogs.com/categories/Linux/Unix%E7%8E%AF%E5%A2%83%E9%AB%98%E7%BA%A7%E7%BC%96%E7%A8%8B/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://jerryblogs.com/tags/Linux/"},{"name":"Unix","slug":"Unix","permalink":"http://jerryblogs.com/tags/Unix/"}]},{"title":"Unix环境高级编程-文件IO","slug":"unix-fileio","date":"2020-10-31T12:36:31.000Z","updated":"2023-04-20T18:07:41.335Z","comments":true,"path":"2020/10/31/unix-fileio/","link":"","permalink":"http://jerryblogs.com/2020/10/31/unix-fileio/","excerpt":"","text":"文件IOopen read write lseek close 文件描述符文件描述符是一个非负整数。当打开或者创建一个文件时，内核向进程返回一个文件描述符。当读写一个文件时，使用open或者create返回的 文件描述符变化范围是0~OPEN_MAX-1，早期的Unix文件系统实现采用上限值是19，即允许每个进程最多打开20个文件，但现在很多系统将其上限增加至63. 在Linux里面，每个进程最大打开数为1024，修改这个设置可以使用如下方法： 临时性修改 1ulimit -n 30665 永久性修改需要更改配置文件 123vim /etc/security/limits.conf* soft nofile 30665 # 应用软件级别的限制* hard nofile 30665 # 操作系统级别的限制 函数open和opennat由open和opennat所返回的文件描述符一定是最小的为用的文件描述符数值。 fd参数把open和opennat分隔开，有三种可能性： path指定绝对路径名，这时候fd参数被忽略 path参数指定相对路径名，fd指出了相对路径名在文件系统中的开始地址 path指定相对路径名，fd参数具有特殊值AT_FDCWD，路径名在当前工作目录中获取，opennat在操作上与open类似 总而言之，opennat希望让线程可以使用相对路径打开目录中的文件，而不再只能打开当前工作目录，其次可以避免time-of-check-to-time-of-use错误（TOCTTOU错误）。 文件名和路径截断在Linux当中过长的文件名会出错。 在Linux当中，获取文件名的长度可以使用这个命令 1234getconf PATH_MAX /usr1024getconf NAME_MAX /usr255 由上面可知，Linux当中默认文件名最大长度是255个字节，而路径最大长度是1024个字节。 函数creat以只写方式打开所创建文件，如果要创建一个临时文件，并要写该文件，必须先调用creat、close然后调用open，现在可以直接用open方式实现。 函数close关闭一个文件会释放该进程加在该文件上的记录锁，当一个进程终止时，内核自动关闭他所打开的所有文件，很多程序利用这一点并不是显式的调用close关闭文件。 函数lseek当前文件偏移量用以度量一个文件从当前开始的字节数。按系统默认的情况，每打开一个文件，偏移量被设置为0。 如果lseek返回-1，表示文件描述符指向的是一个管道、FIFO或者网络套接字，lseek将errno设置为ESPIPE。 lseek仅将当前的偏移量记录在内核中，并不进行任何的IO操作，该偏移量用于下一个IO操作。 如果偏移量大于文件长度，是允许的，下一次写将加长文件长度，并在文件中形成一个空洞，没写过的字节都被读成0。空洞不需要分配磁盘块。 函数read函数writeIO的效率大多数文件系统为改善性能都采取某种预读技术，当检测到正进行顺序读取的时候，系统就试图写入比应用所要求的更多的数据，并假想应用会很快读取。 文件共享内核使用3种数据结构表示打开文件，它们之间的关系决定了在文件共享方面一个进程对另一个进程可能产生的影响。 每个进程在进程表中都有一个记录项，记录项中包含一张打开文件描述符表（进程表项）包含： 文件描述符标志 指向一个文件表项的指针 内核为所有打开文件维持一张文件表，包含： 文件状态标志 当前文件偏移量 指向该文件v节点表项的指针 每个打开文件都有一个v节点结构，v节点包含文件类型和对此文件进行各种操作的函数的指针。还包含i节点（包含文件所有者，文件长度，指向文件实际数据块在磁盘上所在位置的指针等）。 使用write方式导致当前文件偏移量发生改变与使用lseek方式导致当前文件偏移量发生改变的方式不同，lseek不会进行IO操作。 文件描述符标志和文件状态标志在作用范围的区别： 前者只用于一个进程的一个描述符 后者应用于指向该给定文件表项的任何进程中的所有描述符 原子操作追加到一个进程A和B两个进程都对同一个文件进行追加操作，A和B都以打开了该文件，未使用O_APPEND标志。这时候进程A调用了lseek将当前偏移量改变为1500，进程切换，B也用lseek将当前偏移量改变为1500，随即调用write从1500开始进行写操作，写完进程切换，A也调用write从1500开始进行写操作，这时候A的写就会覆盖B的写。 解决方法，将定位到文件尾端和写操作变成一个原子操作 函数pread和pwritepread相当于调用lseek后调用pread，但是又有区别 调用praed时无法中断其定位和读操作 不更新当前文件偏移量 创建一个文件open函数的O_CREATE选项和O_EXCL选项 函数的dup和dup2用来复制一个现有的文件描述符 dup返回的一定是当前可用文件描述符的最小数值，对于dup2可以用fd2指定新描述符的值。如果fd2已经打开，先将其关闭，如果等于fd，则返回fd2，不关闭它。 12345678dup(fd);# 等效于fcntl(fd, F_DUPFD, 0);而调用dup2(fd, fd2);等效于close(fd2);fcntl(fd, F_DUPFD, fd2); 后一种情况下，dup2并不完全等于close加上fcntl。区别如下： dup2是一种原子操作，close和fcntl包含两个函数调用。 有不同的errno 函数sync、fsync和fdatasyncUnix系统实现在内核中设有缓冲区高速缓存，或者页高速缓存，大多数磁盘IO都通过缓冲区进行。当我们向文件写入数据的时候，内核通常先将数据复制到缓冲区中，然后排入队列，晚些时候再写入磁盘。这种方式被称为延迟写。 当内核需要重用缓冲区来存放其他磁盘块中的数据时，会把所有延迟写数据写入磁盘。为了保证磁盘上实际文件系统与缓冲区中内容的一致性，UNIX系统中提供了sync、fsync和fdatasync三个函数。 sync将所有修改过的块缓冲区排入写队列，然后就返回，并不等待实际写磁盘操作结束。称为updata的系统守护进程周期性的调用sync函数，保证定期冲洗内核的块缓冲区。 fsync函数只对由文件描述符fd指定的一个文件起作用，等待磁盘操作结束后才返回。 fsync可用于数据库这样需要确保修改过的块立即写到磁盘上的应用。 fdatasync之影响文件的数据部分，而fsync还会同步更新文件的属性。 函数fcntl可以改变已经打开文件的属性 在修改文件描述符或者是文件状态标志值时需要注意，要先获取现在的标志值，然后按期望修改它，最后设置新的标志值。 通常write只是将数据排入队列，而实际的写磁盘操作则可能在以后的某个时刻进行。而数据库系统则需要使用O_SYNC 函数ioctl&#x2F;dev&#x2F;fd目录项是名为0、1、2等的文件，打开文件&#x2F;dev&#x2F;fd&#x2F;n等效于复制文件描述符n 所以 1fd = open(&quot;/dev/fd/0&quot;, mode) 等效于 1fd = dup(0) Linux中的&#x2F;dev&#x2F;fd是个例外。它把文件描述符映射成指向底层物理文件的符号链接。例如打开&#x2F;dev&#x2F;fd&#x2F;0时，事实上正在打开与标准输入关联的文件，因此返回新文件描述符的模式与&#x2F;dev&#x2F;fd文件描述符的模式并不相关 在Unix上调用creat用&#x2F;dev&#x2F;fd&#x2F;1作为路径名与open时调用O_CREAT作为第二个参数作用相同。但是Linux实现需要很小心，因为Linux实现使用指向实际文件的符号链接，在&#x2F;dev&#x2F;fd文件上使用creat会导致底层文件被截断。 很有意思的问题123./a.out &gt; outfile 2&gt;&amp;1./a.out 2&gt;&amp;1 &gt; outfile# 两者有什么不同 第一种情况：标准输出1首先重定向到outfile，然后调用dup2(1, 2)将标准错误重定向到1，所以这时候2和1都指向同一个文件表项。 第二种情况：标准输出一开始指向终端，这时候先调用dup2(1, 2)使标准错误也指向终端，然后又把标准输出重定向到outfile。 如果查看一个进程打开了哪些文件先用ps命令查看进程pid，进入&#x2F;proc&#x2F;pid号&#x2F;fd文件夹，查看文件描述符指向的文件。","categories":[{"name":"Linux","slug":"Linux","permalink":"http://jerryblogs.com/categories/Linux/"},{"name":"Unix环境高级编程","slug":"Linux/Unix环境高级编程","permalink":"http://jerryblogs.com/categories/Linux/Unix%E7%8E%AF%E5%A2%83%E9%AB%98%E7%BA%A7%E7%BC%96%E7%A8%8B/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://jerryblogs.com/tags/Linux/"},{"name":"Unix","slug":"Unix","permalink":"http://jerryblogs.com/tags/Unix/"}]},{"title":"字节面试","slug":"字节面试","date":"2020-10-31T12:36:31.000Z","updated":"2023-04-20T17:37:11.196Z","comments":true,"path":"2020/10/31/字节面试/","link":"","permalink":"http://jerryblogs.com/2020/10/31/%E5%AD%97%E8%8A%82%E9%9D%A2%E8%AF%95/","excerpt":"","text":"TCP中TIME_WAIT状态如何产生首先调用close()发起主动关闭的一方，在发送最后一个ACK后才会进入time_wait状态，该发送方会保持2MSL事件之后回到初始状态，MSL是数据包在网络空间的最大生存时间。 2MSL &#x3D; 去向ACK最大生存时间 + 来向FIN最大生存时间 产生的原因实现TCP全双工连接的可靠释放如果主动关闭的一方不维护这样一个TIME_WAIT状态，那么在FIN到达的时候会用RST包响应对方，被认为是有错误发生服务端不会断开连接。 使旧的数据包因为过期而消失TCP协议栈无法区分前后两条TCP连接的不同，会把它视为同一个连接，如果不维护TIME_WAIT状态可能产生数据错乱而导致各种无法预知的现象。 如何避免产生time_wait状态服务器设置SO_REUSEADDR套接字选项来通知内核，如果端口忙，但TCP链接处于time_wait状态，可以重用端口。 TCP三次握手为什么需要三次握手防止已经失效的连接请求报文段突然又传送到了服务端，因而产生错误。主要为了防止server端一直等待，浪费资源。 ping涉及到的协议 通过DNS协议将域名转换为地址 通过ARP协议将IP地址转换为MAC地址 发送ICMP回显请求给目标主机并等待回显应答 TCP粘包问题保护消息边界和流保护消息边界：传输协议把数据当作一条独立的消息在网上传播，接收端一次只能接收发送端发出的一个数据包。 面向流则是指无消息保护边界的，如果发送端连续发送数据，接收端有可能在一次接受动作中会接受两个或更多的数据包。 当缓冲区足够大时，有可能会一次接收到两个甚至更多的数据包，这样就需要接收端额外拆包，增加了工作量。 TCP粘包是指发送方发送的若干包数据到接收方接受时粘成一包，从接收缓冲区看，后一包数据的头紧接着前一包数据的尾。 什么时候需要考虑粘包问题 如果利用tcp每次发送数据就与对方建立连接，然后发送完一段数据就关闭连接，这样就不会出现粘包问题 如果发送数据无结构，如文件传输，这样发送方只管发送，接收方只管接受存储就OK，也不用考虑粘包。如果双方建立连接，需要在连接后一段时间内发送不同结构的数据，需要分包，一般可能会在头加入一个数据长度之类的包，以确保接受。、 粘包出现的原因 发送端需要等到缓冲区满才发送出去，造成粘包，也就是说为了提高传输效率，发送方往往要收集到足够多的数据后才发送一包数据 接收方不及时接收缓冲区的包，造成多个包接受。接收方引起的粘包问题是因为接收方进程不及时接收数据，从而导致粘包现象（一次取走多个数据包）。 如何应对粘包接收方创建一预处理线程，对接收到的数据进行预处理，将粘连的包分开。 Linux杀死进程12345kill -l pid // 以用户注销的方式结束进程，也试图杀死所留下的子进程，但不是总能够成功kill -TERM PPID 给父进程发送一个TERM信号试图杀死它和它的子进程killall httpd // 杀死同一进程组内所有进程，允许指定要终止的进程的名称，而非pidkill -HUP PID // 让和缓的执行进程关闭，然后立即重启。kill -SIGNAL PID // 向进程发送信号 -9 杀死 -1 挂起等 查看当前系统的负载，说一下平均负载的三个参数uptime w ```均能够查看load average，分别表示过去1分钟、5分钟、15分钟内运行进程队列中的平均进程数量12345678910111213系统平均负载被定义为特定时间间隔内运行队列中的平均进程数，如果一个进程满足下列条件就会位于运行队列中：- 没有在等待I/O操作的结果- 没有主动进入等待状态- 没有被停止一般来说只要每个CPU的当前活动进程数不大于3，那么系统的性能就是良好的，如果你每个CPU的任务数大于5，那么就表示这台机器的性能有严重问题。# 如何查看监听端口```shellnetstat -antlp 进程和线程的区别 进程是操作系统资源分配的基本单位，线程是操作系统调度的基本单位。一个进程可以拥有多个线程。 线程更加轻量级。创建一个线程比创建一个进程要快10~100倍。而且进程上下文切换比线程上下文切换消耗来得大。 进程有独立的地址空间，启动一个进程，系统会分配地址空间，建立数据表来维护代码段、堆栈段和数据段，非常昂贵。而线程之间不需要系统分配独立的地址空间，比较轻量级，在CPU上下文切换时比较小，线程之间可以共享进程的地址空间、全局变量、打开文件、子进程、信号和信号处理程序等。 多进程更加健壮，而多线程只要有一个线程死掉整个进程也会死掉。 阻塞IO和非阻塞IO阻塞IO进程发起IO系统调用后，进程被阻塞，转到内核空间处理，整个IO处理完毕后返回进程，操作成功则进程获取到数据。 特点： 进程阻塞挂起不消耗CPU资源，及时相应每个操作 实现难度低、开发应用较容易 适用于并发量小的网络应用开发 不适用于并发量大的应用，需要为每个请求分配一个处理进程以及时响应，系统开销大 非阻塞IO进程发起IO系统调用后，如果内核缓冲区没有数据，需要到IO设备中读取，进程返回一个错误而不会被阻塞，进程发起IO系统调用后，如果内核缓冲区有数据，内核就会把数据返回给进程。 异步IO当进程发起一个IO操作，进程返回，但也不能结果，内核把整个IO处理完毕之后，会通知进程结果。如果IO操作成功则进程直接获取到数据。 IO复用模型多个进程的IO可以注册到一个复用器上，然后用一个进程调用该select，select会监听所有注册进来的IO； 如果select没有监听的IO在内的内核缓冲区都没有可读数据，select调用进程会被阻塞；而当任意IO在内核缓冲区中有可读数据时，select调用就会被返回； 而后select调用进程可以自己或通知另外的进程（注册进程）来再次发起读取IO，读取内核中准备好的数据。 多个进程注册后，只有一个select调用进程被阻塞 典型应用： select、poll、epoll三种方案，nginx都可以选择使用这三种方案 特点： 专一进程解决多个进程IO’的问题，性能好； 实现、开发难度较大 适用于高并发服务应用开发：一个进程（线程）响应多个请求； 信号驱动IO模型当进程发起一个IO操作，会向内核注册一个信号处理函数，然后进程返回不阻塞；当内核数据就绪时就会发送一个信号给进程，进程便在信号处理函数中调用IO读取数据。 服务器常见的并发模型单线程循环多线程&#x2F;多进程主要特点是每个网络请求由一个进程&#x2F;线程处理，线程内部使用阻塞式系统调用，在实际场景中适用预先分配的进程&#x2F;线程&#x2F;池，以减少频繁创建销毁线程的开销，往往获得更好的性能。 往往由一个单独的线程处理accept连接，其余线程处理具体的网络请求（收包、处理、发包）；还可以多个进程单独listen，accept网络连接 优点： 实现相对简单 利用多核CPU资源 缺点 线程内部还是阻塞的，如果一个线程在handle的业务逻辑中sleep了，这个线程也就挂了 单线程IO复用Linux高并发服务器中常用epoll作为IO复用机制，线程将需要处理的socket读写事件都注册到epoll中，当有网络IO发生时，epoll_wait返回，线程检查并处理到来的是socket上的请求。 优点： 实现简单 减少锁开销 减少线程切换开销 缺点： 只能使用单核cpu，handle时间过长就会导致整个服务挂死 适用场景：高IO、低计算、handle处理时间短 典型应用：redis 多线程&#x2F;多进程IO复用每个子进程都监听服务，并且都使用epoll机制来处理进程的网络请求，子进程accept()后将创建已连接描述符，然后通过已连接描述符来与客户端通信。 优点：支撑较高并发 缺点：异步编程不直观、容易出错 适用场景：支撑高并发 典型应用：Nginx 协程在应用层用户态模拟线程，在用户态管理协程的调度与切换 优点： 减少上下文切换开销 编程友好、同步的方式写出异步代码 缺点： 多个协程运行在一个线程上，一个协程阻塞将导致整个线程阻塞 HTTP协议由哪些部分组成HTTP协议包含：通用头域、请求消息、响应消息和主体消息 HTTP协议包含：http协议的请求和http协议的响应 http协议的请求又包含以下内容： 请求方法-URL-协议&#x2F;版本号 请求头 请求正文 http协议的相应包含以下内容： 状态行 响应头 响应正文 通用头域Cache-Control头域指定请求和响应遵循的缓存机制，在请求消息或响应消息中设置Cache-Control并不会修改另一个消息处理过程中的缓存处理过程。 Date头域表示消息发送的时间，时间的描述格式由rfc822定义。描述的时间表示世界标准时，换算成本地时间，需要知道用户所在的地区。 Pragma头域用来包含实现特定的指令 请求消息响应消息实体消息JSON和Protobuf什么是ProtobufGoogle开发的一种协议，允许对结构化数据进行序列化和反序列化。 与JSON有什么不同可以互相交换使用。JSON是一种消息格式，源自于JavaScript子集，以文本格式交换，几乎支持所有的编程语言。 Protobuf不仅仅是一种消息格式，还是一组用于定于和交换这些消息的规则和工具。拥有比JSON更多的数据类型。 根据实验，比JSON更快 为什么选择JSON，JSON的优势是什么Protobuf缺少详细的文档，社区不够健壮。 Python 元类元类就是类的类，就是type 如何创建一个元类123456&gt;&gt;&gt; Person = type(&quot;Person&quot;, (), &#123;&quot;age&quot;: 10&#125;)&gt;&gt;&gt; Person.age10&gt;&gt;&gt; Man = type(&quot;Man&quot;, (Person,), &#123;&quot;get_age&quot;: lambda self, self.age&#125;)&gt;&gt;&gt; Man().get_age()10 类的创建过程 当Python见到class关键字时，首先会解析class…中的内容。例如解析基类信息，最重要的是找到元类信息 元类找到后，Python需要准备namespace（也可以认为是type中的dict参数），如果元类实现了__prepare__函数，则会调用它来得到默认的namespace 之后是调用exec来执行类的body，包括属性和方法的定义，最后这些定义会保存在namespace 上述步骤结束后，就得到了创建类所需要的所有信息，这时Python会调用元类的构造函数来创建真正的类","categories":[{"name":"面试经历","slug":"面试经历","permalink":"http://jerryblogs.com/categories/%E9%9D%A2%E8%AF%95%E7%BB%8F%E5%8E%86/"}],"tags":[{"name":"面试经历","slug":"面试经历","permalink":"http://jerryblogs.com/tags/%E9%9D%A2%E8%AF%95%E7%BB%8F%E5%8E%86/"}]},{"title":"红黑树","slug":"红黑树","date":"2020-10-31T12:36:31.000Z","updated":"2023-04-20T17:39:02.303Z","comments":true,"path":"2020/10/31/红黑树/","link":"","permalink":"http://jerryblogs.com/2020/10/31/%E7%BA%A2%E9%BB%91%E6%A0%91/","excerpt":"","text":"红黑树的定义与性质红黑树是一种含有红黑节点并能自平衡的二叉查找树 性质如下： 每个节点要么是黑色，要么是红色 根节点是黑色 每个叶子节点是黑色（这里的叶子节点是指为空的（NIL或NULL）的叶子节点） 每个红色节点的两个子节点一定都是黑色 任意一节点到每个叶子节点的路径都包含数量相同的黑节点 如果一个节点存在黑色子节点，那么该节点肯定有两个子节点 简单的红黑树如下： 我们把正在处理的节点叫做当前节点，它的父亲叫做父节点，它的父亲的另外一个子节点叫做兄弟节点，父亲的父亲叫作祖父节点。 红黑树能够平衡最主要靠三种操作：左旋、右旋和变色： 左旋：以某个节点作为支点（旋转节点），其右子节点变为旋转节点的父节点，右子节点的左子节点变为旋转子节点的右子节点，左节点保持不变 右旋：以某个结点作为支点（旋转结点），其左子节点变为旋转节点的父节点，左子节点的右节点变为旋转节点的左子节点，右节点保持不变 变色：节点的颜色由红变黑或由黑变红 左旋只影响旋转节点和右子树的结构，把右子树的节点往左子树挪了 右旋只影响旋转节点和左子树的结构，把左子树的节点往右子树挪了 旋转操作是局部的，由此可以看出红黑树保持平衡的一些端倪：当一边子树的节点变少了，那么另外向一边子树“借”一些节点，当一边子树节点多了，那么向另外一边子树“租”一些节点。 不能乱挪，还得靠变色 红黑树总是通过旋转和变色达到自平衡 红黑树查找红黑树是一颗平衡二叉树，并且不会破环树的平衡，所以与查找二叉平衡树无异： 从根节点开始，把根节点设置为当前节点 若当前节点为空，返回null 若当前节点不为空，用当前节点的key跟查找key做比较 若当前节点key等于查找key，那么就是查找目标，返回当前节点 若当前节点key大于查找key，把当前节点的左子节点设置为当前节点，重复步骤2 若当前节点key小于查找key，把当前节点右子节点设置为当前节点，重复步骤2 红黑树的查找最坏时间复杂度为O(2logN)* 红黑树的插入操作包括两部分工作，一查找插入的位置，二插入后自平衡，查找插入的父节点很简单，跟查找区别不大： 从根节点开始查找； 若根节点为空那么插入节点作为根节点，结束 若根节点不为空，那么把根节点作为当前节点 若当前节点为null，返回当前节点的父节点，返回当前节点的父节点，结束 若当前节点key等于查找key，那么该节点就是插入节点，更新插入节点的值，结束 若当前节点key大于查找key，把当前节点左子节点设置为当前节点，重复步骤4 若当前节点key小于查找key，把当前节点右子节点设置为当前节点，重复步骤4 这里的key可以理解为键值对的key 插入的节点着色为红色，因为不会违背性质5，从一个节点到该节点的子孙节点的所有路径上包含相同数目的黑节点，因为如果插入的节点是黑色节点，那么必须做自平衡操作。 情景处理 情景一：红黑树为空树直接把插入节点作为根节点就行，但注意，红黑树性质2，根节点是褐色，还需要把插入节点设为黑色 处理：把插入节点作为根节点，并把节点设置为黑色 情景二：插入节点的key已存在插入节点的key已存在，既然红黑树总保持平衡那么把插入节点设置为将要替代节点的颜色，再把节点的值更新完成就插入 处理： 设为当前节点的颜色 更新为当前节点的值为插入节点的值 情景三：插入节点的父节点为黑节点由于插入的节点是红色的，当插入的节点为黑色，并不会影响红黑树的平衡，直接插入即可 情景四：插入节点的父节点为红节点如果插入的父节点为红节点，那么该父节点不可能为根节点，所以插入节点总是存在祖父节点。 情景4.1 叔叔节点存在并且为红节点祖父节点一定为红节点，因为不可能存在两个相连的红节点 此时该插入子树的红黑层数情况是：黑红红，显然最简单的处理方式是把其改为红黑红 处理： 将P和S设置为黑色 将PP设置为红色 将PP设置为当前插入节点 如果PP的父节点是黑色，那么无需再做处理，如果PP的父节点是红色，根据性质4，此时红黑树已经不平衡了，还需要把PP当作新的插入节点，继续做插入操作自平衡处理，知道平衡为止。 如果刚好为根节点，根据性质2，必须把PP重新设置为黑色，那么此时红黑树结构为黑黑红。换句话说，根节点到叶子节点的路径中，黑色节点增加了（本来是黑红，即只有根节点是黑色的，插入之后变成了黑黑红），这也是唯一一种会增加红黑树黑色节点层数的插入场景。 红黑树的生长是自底向上的，而普通的二叉查找树是自顶向下的 情景4.2 叔叔节点不存在或为黑节点，并且插入节点的父亲节点是祖父节点的左子节点情景4.2.1 插入节点是其父节点的左子节点处理： 将P设置为黑色 将红色 对PP进行右旋 情景4.2.2 插入节点是其父节点的右子节点处理： 对P进行左旋 把P设置为插入节点，得到情景4.2.1 进行4.2.1的处理 情景4.3 叔叔节点不存在或为黑节点，并且插入节点的父亲节点是祖父节点的右子节点处理： 对P进行右旋 将P设置为插入节点，得到4.3.1 进行4.3.1的处理 红黑树删除红黑树删除也包括两种操作，一是查找目标节点；二删除后自平衡。 查找目标节点显然可以复用查找操作，当不存在目标节点时，忽略本次操作；当存在目标节点时，删除后就得做自平衡处理了。删除了节点后我们还需要找节点来替代删除节点的位置，不然子树跟父辈节点断开了，除非删除节点刚好没有子节点，那么就不需要替代 二叉树删除节点替代节点有三种情形： 若删除节点无子节点，直接删除 若删除节点只有一个子节点，用子节点替换删除节点 若删除节点有两个子节点，用后记节点（大于删除节点的最小节点）替换删除节点 情景三的后继节点是大于删除节点的最小节点，也是删除节点的右子树中最左节点。 删除节点被替代后，在不考虑节点键值的情况下，对于树来讲，可以认为删除的是替代节点 基于此，上面所说的三种二叉树删除操作最后都会互相转换最终都是转换成情景1 情景2：删除节点用唯一的子节点替换，子节点替换为删除节点后，可以认为删除的是子节点，若子节点又有两个子节点，那么相当于转换为情景三，一直自顶向下转换，总是能转换为情景1 情景3：删除节点用后继节点（后继节点肯定不存在左节点，否否则无需转换），如果后继节点有右节点那么相当于转换为情景2，如果后继节点有右子节点，那么转换为情景2，否则转换为情景1 二叉树节点情景关系如图： 综上所述：删除操作删除的节点可以看作删除替代节点，而替代节点最后总是在树末 红黑树删除的所有操作： 约定以下： 图中字母并不代表节点key大小，P代替节点，P表示代替节点的父节点，S表示代替节点的兄弟节点，SL表示兄弟节点的左子节点，SR表示兄弟节点的右子节点，灰色节点表示它可以是红色也可以是黑色 R是即将被替换到删除节点位置的替代节点，在删除前，他还在原来所在的位置参与二叉树的平衡，平衡后再替换到删除节点的位置，才算删除完成 删除情景1：替换节点是红色节点我们把替换节点换到了删除节点的位置时，由于替换节点时为红色，删除了也不会影响红黑树的平衡，只要把替换节点的颜色设为删除的节点的颜色即可重新平衡 处理：颜色变为删除节点的颜色 删除情景2：替换节点是黑节点当替换节点是黑色节点，删除后可能会违反性质5，所以我们必须进行自平衡处理。还必须考虑替换节点是其父节点的左子节点还是右子节点，来做不同的旋转，以使树重新平衡 情景2.1.1 替换节点的兄弟节点是红节点处理： 将S设置为黑色 将P设置为红色 对P进行左旋，得到情景2.1.2.3 进行情景2.1.2.3的处理 此时R仍然是替代节点，它的新兄弟节点SL和兄弟节点的子节点都是黑色 情景2.1.2 替换节点的兄弟节点是黑节点父节点和子节点的颜色未确定，所以又分为几种情况： 删除情景2.1.2.1：替换节点的兄弟节点的右子节点是红色节点，左子节点任意颜色处理： 将S颜色设为P的颜色 将P设为黑色 将SR设置黑色 对P进行左旋 如果考虑第一次替换的情况SL，根据红黑树的性质4，SL一定是红色，如果考虑到自底向上的情况，每颗子树都保持了平衡状态，整棵树是平衡的。 删除情景2.1.2.2 替换节点的兄弟节点的右子节点是黑节点，左子节点为红节点处理： 将S设置为红色 将SL设置为黑色 对S进行右旋，得到情景2.1.2.1 删除情景2.1.2.3 替换节点的兄弟节点的子节点都为黑节点把兄弟节点设为红色，再把父节点当作替代节点，自底向上处理，去找父节点的兄弟节点去借红节点。 处理： 将S设为红色 把P作为新的替换节点 重新进行节点删除处理 删除情景2：替换节点是其父节点的右子节点与2.1相反 删除情景2.2.1 替换节点的兄弟节点是红节点处理： 将S设置为红色 将P设置为红色 对P进行右旋，得到情景2.2.2.3 进行情景2.2.2.3的处理 删除情景2.2.2：替换节点的兄弟节点是黑节点删除情景2.2.2.1：替换节点的兄弟节点的左子节点是红节点，右节点任意颜色处理： 将S设为P的颜色 将P设置为黑色 将SL设置为黑色 对P进行右旋 删除情景2.2.2.2 替换节点的兄弟节点的左子节点是黑节点，右子节点是红节点处理 将S设为红色 将P设为黑色 将S进行左旋，得到情景2.2.2.1 进行情景2.2.2.1的处理 删除情景2.2.2.3：替换节点的兄弟节点的子节点都为黑节点处理： 将S设置为红色 把P作为新的替换节点 重新进行删除节点情景处理","categories":[{"name":"数据结构","slug":"数据结构","permalink":"http://jerryblogs.com/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"}],"tags":[{"name":"数据结构","slug":"数据结构","permalink":"http://jerryblogs.com/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"}]}],"categories":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"http://jerryblogs.com/categories/Kubernetes/"},{"name":"运维开发","slug":"Kubernetes/运维开发","permalink":"http://jerryblogs.com/categories/Kubernetes/%E8%BF%90%E7%BB%B4%E5%BC%80%E5%8F%91/"},{"name":"Docker","slug":"Docker","permalink":"http://jerryblogs.com/categories/Docker/"},{"name":"Golang","slug":"Golang","permalink":"http://jerryblogs.com/categories/Golang/"},{"name":"源码分析","slug":"Kubernetes/源码分析","permalink":"http://jerryblogs.com/categories/Kubernetes/%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"},{"name":"Chatgpt","slug":"Chatgpt","permalink":"http://jerryblogs.com/categories/Chatgpt/"},{"name":"Go","slug":"Go","permalink":"http://jerryblogs.com/categories/Go/"},{"name":"Go-Zero","slug":"Go/Go-Zero","permalink":"http://jerryblogs.com/categories/Go/Go-Zero/"},{"name":"运维开发","slug":"运维开发","permalink":"http://jerryblogs.com/categories/%E8%BF%90%E7%BB%B4%E5%BC%80%E5%8F%91/"},{"name":"Python","slug":"Python","permalink":"http://jerryblogs.com/categories/Python/"},{"name":"Python PEP 合集","slug":"Python/Python-PEP-合集","permalink":"http://jerryblogs.com/categories/Python/Python-PEP-%E5%90%88%E9%9B%86/"},{"name":"论文翻译","slug":"Kubernetes/论文翻译","permalink":"http://jerryblogs.com/categories/Kubernetes/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/"},{"name":"Linux","slug":"Linux","permalink":"http://jerryblogs.com/categories/Linux/"},{"name":"面试经历","slug":"面试经历","permalink":"http://jerryblogs.com/categories/%E9%9D%A2%E8%AF%95%E7%BB%8F%E5%8E%86/"},{"name":"操作系统","slug":"操作系统","permalink":"http://jerryblogs.com/categories/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/"},{"name":"Unix环境高级编程","slug":"Linux/Unix环境高级编程","permalink":"http://jerryblogs.com/categories/Linux/Unix%E7%8E%AF%E5%A2%83%E9%AB%98%E7%BA%A7%E7%BC%96%E7%A8%8B/"},{"name":"数据结构","slug":"数据结构","permalink":"http://jerryblogs.com/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"}],"tags":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"http://jerryblogs.com/tags/Kubernetes/"},{"name":"Docker","slug":"Docker","permalink":"http://jerryblogs.com/tags/Docker/"},{"name":"Golang","slug":"Golang","permalink":"http://jerryblogs.com/tags/Golang/"},{"name":"Chatgpt","slug":"Chatgpt","permalink":"http://jerryblogs.com/tags/Chatgpt/"},{"name":"Go","slug":"Go","permalink":"http://jerryblogs.com/tags/Go/"},{"name":"运维开发","slug":"运维开发","permalink":"http://jerryblogs.com/tags/%E8%BF%90%E7%BB%B4%E5%BC%80%E5%8F%91/"},{"name":"Python","slug":"Python","permalink":"http://jerryblogs.com/tags/Python/"},{"name":"Linux","slug":"Linux","permalink":"http://jerryblogs.com/tags/Linux/"},{"name":"面试经历","slug":"面试经历","permalink":"http://jerryblogs.com/tags/%E9%9D%A2%E8%AF%95%E7%BB%8F%E5%8E%86/"},{"name":"操作系统","slug":"操作系统","permalink":"http://jerryblogs.com/tags/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/"},{"name":"Unix","slug":"Unix","permalink":"http://jerryblogs.com/tags/Unix/"},{"name":"数据结构","slug":"数据结构","permalink":"http://jerryblogs.com/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"}]}